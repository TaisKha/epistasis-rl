{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "287c74f4-4bc0-423f-904a-72190e9c8962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtaisikus\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/tskhakharova/.local/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:47: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tskhakharova/epistasis-rl/wandb/run-20220807_215555-2t9cs9ge</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/taisikus/epistasis/runs/2t9cs9ge\" target=\"_blank\">worthy-frost-114</a></strong> to <a href=\"https://wandb.ai/taisikus/epistasis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_episode_counted!!!\n",
      "Episode 1: reward=6.96450, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 2: reward=6.96320, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 3: reward=7.21770, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 4: reward=7.18555, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 5: reward=6.36041, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 6: reward=6.54809, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 7: reward=6.78683, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 8: reward=7.26772, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 9: reward=7.03299, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 10: reward=6.74881, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 11: reward=6.38455, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 12: reward=6.82997, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 13: reward=6.97102, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 14: reward=7.20652, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 15: reward=6.90768, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 16: reward=6.88002, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 17: reward=7.12382, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 18: reward=8.11397, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 19: reward=6.89370, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 20: reward=6.43170, steps=100, speed=0.0 f/s, elapsed=0:00:30\n",
      "one_episode_counted!!!\n",
      "Episode 21: reward=6.55948, steps=100, speed=2.1 f/s, elapsed=0:01:17\n",
      "one_episode_counted!!!\n",
      "Episode 22: reward=7.67650, steps=100, speed=2.1 f/s, elapsed=0:02:06\n",
      "one_episode_counted!!!\n",
      "Episode 23: reward=7.64760, steps=100, speed=2.1 f/s, elapsed=0:02:54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Engine run is terminating due to exception: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 558>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    597\u001b[0m engine \u001b[38;5;241m=\u001b[39m Engine(process_batch)\n\u001b[1;32m    598\u001b[0m setup_ignite(engine, params, exp_source, NAME, net)\n\u001b[0;32m--> 600\u001b[0m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/ignite/engine/engine.py:718\u001b[0m, in \u001b[0;36mEngine.run\u001b[0;34m(self, data, max_epochs, max_iters, epoch_length)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_length should be provided if data is None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdataloader \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m--> 718\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/ignite/engine/engine.py:797\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine run is terminating due to exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 797\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/ignite/engine/engine.py:467\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/ignite/engine/engine.py:767\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_engine()\n\u001b[0;32m--> 767\u001b[0m time_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once_on_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;66;03m# time is available for handlers but must be update after fire\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtimes[Events\u001b[38;5;241m.\u001b[39mEPOCH_COMPLETED\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m time_taken\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/ignite/engine/engine.py:859\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39miteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_STARTED)\n\u001b[0;32m--> 859\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_COMPLETED)\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_terminate \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_terminate_single_epoch:\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m    584\u001b[0m loss_v \u001b[38;5;241m=\u001b[39m calc_loss_dqn(\n\u001b[1;32m    585\u001b[0m     batch, net, tgt_net\u001b[38;5;241m.\u001b[39mtarget_model,\n\u001b[1;32m    586\u001b[0m     gamma\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mgamma, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    587\u001b[0m loss_v\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 588\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m epsilon_tracker\u001b[38;5;241m.\u001b[39mframe(engine\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39miteration)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39miteration \u001b[38;5;241m%\u001b[39m params\u001b[38;5;241m.\u001b[39mtarget_net_sync \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/torch/optim/_functional.py:105\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    103\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_correction2\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    109\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[1;32m    110\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import ptan\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from typing import Iterable, Tuple, List\n",
    "import warnings\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# import ptan_ignite\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage\n",
    "# from ignite.contrib.handlers import tensorboard_logger as tb_logger\n",
    "\n",
    "\n",
    "import enum\n",
    "import time\n",
    "from typing import Optional\n",
    "from ignite.engine import Engine, State\n",
    "from ignite.engine import Events, EventEnum\n",
    "from ignite.handlers.timing import Timer\n",
    "from ignite.contrib.handlers.base_logger import BaseOutputHandler\n",
    "from ignite.contrib.handlers.wandb_logger import WandBLogger\n",
    "from typing import Any, Callable, List, Optional, Union\n",
    "\n",
    "EPISODE_LENGTH = 6000\n",
    "\n",
    "class OutputHandler(BaseOutputHandler):\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tag: str,\n",
    "        metric_names: Optional[List[str]] = None,\n",
    "        output_transform: Optional[Callable] = None,\n",
    "        global_step_transform: Optional[Callable] = None,\n",
    "        sync: Optional[bool] = None,\n",
    "        state_attributes: Optional[List[str]] = None,\n",
    "    ):\n",
    "        super().__init__(tag, metric_names, output_transform, global_step_transform, state_attributes)\n",
    "        self.sync = sync\n",
    "\n",
    "    def __call__(self, engine: Engine, logger: WandBLogger, event_name: Union[str, Events]) -> None:\n",
    "\n",
    "        if not isinstance(logger, WandBLogger):\n",
    "            raise RuntimeError(f\"Handler '{self.__class__.__name__}' works only with WandBLogger.\")\n",
    "\n",
    "        global_step = self.global_step_transform(engine, event_name)  # type: ignore[misc]\n",
    "        if not isinstance(global_step, int):\n",
    "            raise TypeError(\n",
    "                f\"global_step must be int, got {type(global_step)}.\"\n",
    "                \" Please check the output of global_step_transform.\"\n",
    "            )\n",
    "\n",
    "        metrics = self._setup_output_metrics_state_attrs(engine, log_text=True, key_tuple=False)\n",
    "        logger.log(metrics, sync=self.sync)\n",
    "\n",
    "\n",
    "class EpisodeEvents(EventEnum):\n",
    "    EPISODE_COMPLETED = \"episode_completed\"\n",
    "    BOUND_REWARD_REACHED = \"bound_reward_reached\"\n",
    "    BEST_REWARD_REACHED = \"best_reward_reached\"\n",
    "\n",
    "\n",
    "class EndOfEpisodeHandler:\n",
    "    def __init__(self, exp_source: ptan.experience.ExperienceSource, alpha: float = 0.98,\n",
    "                 bound_avg_reward: Optional[float] = None,\n",
    "                 subsample_end_of_episode: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Construct end-of-episode event handler\n",
    "        :param exp_source: experience source to use\n",
    "        :param alpha: smoothing alpha param\n",
    "        :param bound_avg_reward: optional boundary for average reward\n",
    "        :param subsample_end_of_episode: if given, end of episode event will be subsampled by this amount\n",
    "        \"\"\"\n",
    "        self._exp_source = exp_source\n",
    "        self._alpha = alpha\n",
    "        self._bound_avg_reward = bound_avg_reward\n",
    "        self._best_avg_reward = None\n",
    "        self._subsample_end_of_episode = subsample_end_of_episode\n",
    "\n",
    "    def attach(self, engine: Engine):\n",
    "        engine.add_event_handler(Events.ITERATION_COMPLETED, self)\n",
    "        engine.register_events(*EpisodeEvents)\n",
    "        State.event_to_attr[EpisodeEvents.EPISODE_COMPLETED] = \"episode\"\n",
    "        State.event_to_attr[EpisodeEvents.BOUND_REWARD_REACHED] = \"episode\"\n",
    "        State.event_to_attr[EpisodeEvents.BEST_REWARD_REACHED] = \"episode\"\n",
    "\n",
    "    def __call__(self, engine: Engine):\n",
    "        for reward, steps in self._exp_source.pop_rewards_steps():\n",
    "            engine.state.episode = getattr(engine.state, \"episode\", 0) + 1\n",
    "            engine.state.episode_reward = reward\n",
    "            engine.state.episode_steps = steps\n",
    "            engine.state.metrics['reward'] = reward\n",
    "            engine.state.metrics['steps'] = steps\n",
    "            self._update_smoothed_metrics(engine, reward, steps)\n",
    "            if self._subsample_end_of_episode is None or engine.state.episode % self._subsample_end_of_episode == 0:\n",
    "                engine.fire_event(EpisodeEvents.EPISODE_COMPLETED)\n",
    "            if self._bound_avg_reward is not None and engine.state.metrics['avg_reward'] >= self._bound_avg_reward:\n",
    "                engine.fire_event(EpisodeEvents.BOUND_REWARD_REACHED)\n",
    "            if self._best_avg_reward is None:\n",
    "                self._best_avg_reward = engine.state.metrics['avg_reward']\n",
    "            elif self._best_avg_reward < engine.state.metrics['avg_reward']:\n",
    "                engine.fire_event(EpisodeEvents.BEST_REWARD_REACHED)\n",
    "                self._best_avg_reward = engine.state.metrics['avg_reward']\n",
    "\n",
    "    def _update_smoothed_metrics(self, engine: Engine, reward: float, steps: int):\n",
    "        for attr_name, val in zip(('avg_reward', 'avg_steps'), (reward, steps)):\n",
    "            if attr_name not in engine.state.metrics:\n",
    "                engine.state.metrics[attr_name] = val\n",
    "            else:\n",
    "                engine.state.metrics[attr_name] *= self._alpha\n",
    "                engine.state.metrics[attr_name] += (1-self._alpha) * val\n",
    "\n",
    "\n",
    "class EpisodeFPSHandler:\n",
    "    FPS_METRIC = 'fps'\n",
    "    AVG_FPS_METRIC = 'avg_fps'\n",
    "    TIME_PASSED_METRIC = 'time_passed'\n",
    "\n",
    "    def __init__(self, fps_mul: float = 1.0, fps_smooth_alpha: float = 0.98):\n",
    "        self._timer = Timer(average=True)\n",
    "        self._fps_mul = fps_mul\n",
    "        self._started_ts = time.time()\n",
    "        self._fps_smooth_alpha = fps_smooth_alpha\n",
    "\n",
    "    def attach(self, engine: Engine, manual_step: bool = False):\n",
    "        self._timer.attach(engine, step=None if manual_step else Events.ITERATION_COMPLETED)\n",
    "        engine.add_event_handler(EpisodeEvents.EPISODE_COMPLETED, self)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        If manual_step=True on attach(), this method should be used every time we've communicated with environment\n",
    "        to get proper FPS\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._timer.step()\n",
    "\n",
    "    def __call__(self, engine: Engine):\n",
    "        t_val = self._timer.value()\n",
    "        if engine.state.iteration > 1:\n",
    "            fps = self._fps_mul / t_val\n",
    "            avg_fps = engine.state.metrics.get(self.AVG_FPS_METRIC)\n",
    "            if avg_fps is None:\n",
    "                avg_fps = fps\n",
    "            else:\n",
    "                avg_fps *= self._fps_smooth_alpha\n",
    "                avg_fps += (1-self._fps_smooth_alpha) * fps\n",
    "            engine.state.metrics[self.AVG_FPS_METRIC] = avg_fps\n",
    "            engine.state.metrics[self.FPS_METRIC] = fps\n",
    "        engine.state.metrics[self.TIME_PASSED_METRIC] = time.time() - self._started_ts\n",
    "        self._timer.reset()\n",
    "\n",
    "\n",
    "class PeriodEvents(EventEnum):\n",
    "    ITERS_10_COMPLETED = \"iterations_10_completed\"\n",
    "    ITERS_100_COMPLETED = \"iterations_100_completed\"\n",
    "    ITERS_1000_COMPLETED = \"iterations_1000_completed\"\n",
    "    ITERS_10000_COMPLETED = \"iterations_10000_completed\"\n",
    "    ITERS_100000_COMPLETED = \"iterations_100000_completed\"\n",
    "\n",
    "\n",
    "class PeriodicEvents:\n",
    "    \"\"\"\n",
    "    The same as CustomPeriodicEvent from ignite.contrib, but use true amount of iterations,\n",
    "    which is good for TensorBoard\n",
    "    \"\"\"\n",
    "\n",
    "    INTERVAL_TO_EVENT = {\n",
    "        10: PeriodEvents.ITERS_10_COMPLETED,\n",
    "        100: PeriodEvents.ITERS_100_COMPLETED,\n",
    "        1000: PeriodEvents.ITERS_1000_COMPLETED,\n",
    "        10000: PeriodEvents.ITERS_10000_COMPLETED,\n",
    "        100000: PeriodEvents.ITERS_100000_COMPLETED,\n",
    "    }\n",
    "\n",
    "    def attach(self, engine: Engine):\n",
    "        engine.add_event_handler(Events.ITERATION_COMPLETED, self)\n",
    "        engine.register_events(*PeriodEvents)\n",
    "        for e in PeriodEvents:\n",
    "            State.event_to_attr[e] = \"iteration\"\n",
    "\n",
    "    def __call__(self, engine: Engine):\n",
    "        for period, event in self.INTERVAL_TO_EVENT.items():\n",
    "            if engine.state.iteration % period == 0:\n",
    "                engine.fire_event(event)\n",
    "\n",
    "\n",
    "\n",
    "def unpack_batch(batch: List[ptan.experience.ExperienceFirstLast]):\n",
    "    states, actions, rewards, dones, last_states = [],[],[],[],[]\n",
    "    for exp in batch:\n",
    "        state = np.array(exp.state)\n",
    "        states.append(state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        dones.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            lstate = state  # the result will be masked anyway\n",
    "        else:\n",
    "            lstate = np.array(exp.last_state)\n",
    "        last_states.append(lstate)\n",
    "    return np.array(states, copy=False), np.array(actions), \\\n",
    "           np.array(rewards, dtype=np.float32), \\\n",
    "           np.array(dones, dtype=np.uint8), \\\n",
    "           np.array(last_states, copy=False)\n",
    "\n",
    "                \n",
    "def calc_loss_dqn(batch, net, tgt_net, gamma, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = \\\n",
    "        unpack_batch(batch)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "    state_action_vals = torch.sum(actions_v * net(states_v), dim=1, dtype=torch.float32)\n",
    "#     actions_v = actions_v.unsqueeze(-1)\n",
    "    \n",
    "#     state_action_vals = net(states_v).gather(1, actions_v)\n",
    "#     state_action_vals = state_action_vals.squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_state_vals = tgt_net(next_states_v).max(1)[0]\n",
    "        next_state_vals[done_mask] = 0.0\n",
    "\n",
    "    bellman_vals = next_state_vals.detach() * gamma + rewards_v\n",
    "    return nn.MSELoss()(state_action_vals, bellman_vals)\n",
    "\n",
    "class EpistasisEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.SAMPLE_SIZE = 300 #t1 = t2 = SAMPLE_SIZE\n",
    "        self.reset()\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.N_SNPS,), dtype=np.uint8)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=\n",
    "                        (3, 2*self.SAMPLE_SIZE, self.N_SNPS), dtype=np.uint8)\n",
    "        \n",
    "        \n",
    "    def establish_phen_gen(self, file):\n",
    "        with open(file) as f:\n",
    "            data = json.load(f)\n",
    "            genotype = np.array(data[\"genotype\"])\n",
    "            self.phenotype = np.array(data[\"phenotype\"])\n",
    "            self.genotype = genotype.T\n",
    "            num_phenotypes = max(self.phenotype)+1\n",
    "            self.disease_snps = data[\"disease_snps\"]\n",
    "            self.phen_gen = [[] for _ in range(num_phenotypes)]\n",
    "            for i in range(len(self.genotype)):\n",
    "                self.phen_gen[self.phenotype[i]].append(i)  \n",
    "            return  self.genotype.shape[0], self.genotype.shape[1]\n",
    "        \n",
    "    def normalize_reward(self, current_reward):\n",
    "        maximum_env_reward = self._count_reward(self.disease_snps)\n",
    "        minimal_reward = 0.5\n",
    "        normalized_reward = (current_reward - minimal_reward) / (maximum_env_reward - minimal_reward)\n",
    "        if normalized_reward > 1:\n",
    "            print(\"normalized reward > 1: \\n normalized reward = \", normalized_reward, \"\\n current reward = \", current_reward, \"\\n maximum_env_reward = \", maximum_env_reward )\n",
    "            normalized_reward = 0.1\n",
    "        return normalized_reward\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        snp_ids = self._take_action(action)\n",
    "        # print(f\"{snp_ids=}, {self.disease_snps=}\")\n",
    "        reward = self._count_reward(snp_ids)\n",
    "        # print(f\"{reward=}\", end=' ')\n",
    "        reward = self.normalize_reward(reward)\n",
    "        # print(f\"{reward=}\")\n",
    "        self.current_step += 1\n",
    "        if self.current_step == EPISODE_LENGTH:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False  \n",
    "        # done = self.current_step == 1\n",
    "        obs = None if done else self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def _count_reward(self, snp_ids):\n",
    "        \n",
    "        all_existing_seq = defaultdict(lambda: {'control' : 0, 'case' : 0})\n",
    "        for i, idv in enumerate(self.obs):\n",
    "            snp_to_cmp = tuple(idv[snp_id] for snp_id in snp_ids) #tuple of SNP that \n",
    "            if self.obs_phenotypes[i] == 0:\n",
    "                all_existing_seq[snp_to_cmp]['control'] += 1\n",
    "            else:\n",
    "                all_existing_seq[snp_to_cmp]['case'] += 1\n",
    "\n",
    "        #count reward      \n",
    "        TP = 0 #HR case\n",
    "        FP = 0 #HR control\n",
    "        TN = 0 #LR control\n",
    "        FN = 0 #LR case\n",
    "\n",
    "        for case_control_count in all_existing_seq.values():\n",
    "          # if seq is in LR group\n",
    "            if case_control_count['case'] <= case_control_count['control']: #вопрос <= или <\n",
    "                FN += case_control_count['case']\n",
    "                TN += case_control_count['control']\n",
    "            else:\n",
    "          # if seq is in HR group\n",
    "                TP += case_control_count['case']\n",
    "                FP += case_control_count['control']\n",
    "        R = (FP + TN) / (TP + FN)\n",
    "        delta = FP / (TP+0.001)\n",
    "        gamma = (TP + FP + TN + FN) / (TP+0.001)\n",
    "        CCR = 0.5 * (TP / (TP + FN) + TN / (FP + TN))\n",
    "        U = (R - delta)**2 / ((1 + delta) * (gamma - delta - 1 + 0.001))\n",
    "        koef = 1\n",
    "        if len(snp_ids) > len(self.disease_snps):\n",
    "                print(\"len(snp_ids) > len(self.disease_snps)\")\n",
    "                koef = 1 / len(snp_ids)\n",
    "\n",
    "        return koef*(CCR + U)\n",
    "\n",
    "  \n",
    "    def reset(self):\n",
    "        pops = [\"ASW\", \"CEU\", \"CEU+TSI\", \"CHD\", \"GIH\", \"JPT+CHB\", \"LWK\", \"MEX\", \"MKK\", \"TSI\"]\n",
    "        sim_idx = np.random.randint(2500)\n",
    "        corp_idx = np.random.randint(1, 23)\n",
    "        pop_idx = np.random.choice(pops)\n",
    "        \n",
    "        # filename = f\"/home/tskhakharova/epistasis-rl/epigen/sim/{sim_idx}_{corp_idx}_{pop_idx}.json\"\n",
    "        filename = f\"/home/tskhakharova/epistasis-rl/epigen/sim/5_7_CEU.json\"\n",
    "        if not os.path.exists(filename):\n",
    "            os.system(f\"cd /home/tskhakharova/epistasis-rl/epigen/ && python3 simulate_data.py --sim-ids {sim_idx} --corpus-id {corp_idx} --pop {pop_idx} --inds 5000 --snps 100 --model models/ext_model.ini\")\n",
    "\n",
    "        self.N_IDV, self.N_SNPS = self.establish_phen_gen(filename)\n",
    "        \n",
    "        self.obs_phenotypes = None\n",
    "        one_hot_obs = self._next_observation()\n",
    "        self.current_step = 0\n",
    "        \n",
    "        return one_hot_obs\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        chosen_snp_ids = []\n",
    "        for i, choice in enumerate(action):\n",
    "            if choice == 1:\n",
    "                chosen_snp_ids.append(i)\n",
    "        return chosen_snp_ids    \n",
    "    \n",
    "    def _next_observation(self):\n",
    "        id_0 = np.random.choice(self.phen_gen[0], self.SAMPLE_SIZE)\n",
    "        id_1 = np.random.choice(self.phen_gen[1], self.SAMPLE_SIZE)\n",
    "        sample_ids = np.array(list(zip(id_0,id_1))).flatten()\n",
    "        self.obs = np.array([self.genotype[idv] for idv in sample_ids])\n",
    "        self.obs_phenotypes = [self.phenotype[idv] for idv in sample_ids]\n",
    "        \n",
    "        #one_hot\n",
    "        one_hot_obs = F.one_hot(torch.tensor(self.obs), 3)\n",
    "        one_hot_obs = one_hot_obs.movedim(2, 0)\n",
    "\n",
    "        return one_hot_obs\n",
    "    \n",
    "class EpsilonTracker:\n",
    "    def __init__(self, selector: ptan.actions.EpsilonGreedyActionSelector,\n",
    "                 params: SimpleNamespace):\n",
    "        self.selector = selector\n",
    "        self.params = params\n",
    "        self.frame(0)\n",
    "\n",
    "    def frame(self, frame_idx: int):\n",
    "        eps = self.params.epsilon_start - \\\n",
    "              frame_idx / self.params.epsilon_frames\n",
    "        self.selector.epsilon = max(self.params.epsilon_final, eps)\n",
    "\n",
    "\n",
    "def batch_generator(buffer: ptan.experience.ExperienceReplayBuffer,\n",
    "                    initial: int, batch_size: int):\n",
    "    buffer.populate(initial)\n",
    "    while True:\n",
    "        buffer.populate(1)\n",
    "        yield buffer.sample(batch_size)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_values_of_states(states, net, device=\"cpu\"):\n",
    "    mean_vals = []\n",
    "    for batch in np.array_split(states, 64):\n",
    "        states_v = torch.tensor(batch).to(device)\n",
    "        action_values_v = net(states_v)\n",
    "        best_action_values_v = action_values_v.max(1)[0]\n",
    "        mean_vals.append(best_action_values_v.mean().item())\n",
    "    return np.mean(mean_vals)\n",
    "\n",
    "\n",
    "def setup_ignite(engine: Engine, params: SimpleNamespace,\n",
    "                 exp_source, run_name: str, net,\n",
    "                 extra_metrics: Iterable[str] = ()):\n",
    "    # get rid of missing metrics warning\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "    handler = EndOfEpisodeHandler(\n",
    "        exp_source, bound_avg_reward=params.stop_reward)\n",
    "    handler.attach(engine)\n",
    "    EpisodeFPSHandler().attach(engine)\n",
    "\n",
    "    # @engine.on(EpisodeEvents.EPISODE_COMPLETED)\n",
    "    # def episode_completed(trainer: Engine):\n",
    "    #     passed = trainer.state.metrics.get('time_passed', 0)\n",
    "    #     print(\"Episode %d: reward=%.5f, steps=%s, \"\n",
    "    #           \"speed=%.1f f/s, elapsed=%s\" % (\n",
    "    #         trainer.state.episode, trainer.state.episode_reward,\n",
    "    #         trainer.state.episode_steps,\n",
    "    #         trainer.state.metrics.get('avg_fps', 0),\n",
    "    #         timedelta(seconds=int(passed))))\n",
    "\n",
    "    @engine.on(EpisodeEvents.BOUND_REWARD_REACHED)\n",
    "    def game_solved(trainer: Engine):\n",
    "        passed = trainer.state.metrics['time_passed']\n",
    "        print(\"Game solved in %s, after %d episodes \"\n",
    "              \"and %d iterations!\" % (\n",
    "            timedelta(seconds=int(passed)),\n",
    "            trainer.state.episode, trainer.state.iteration))\n",
    "        trainer.should_terminate = True\n",
    "\n",
    "    # now = datetime.now().isoformat(timespec='minutes').replace(':', '')\n",
    "    # wandb.tensorboard.patch(root_logdir=\"./logs/debug\")\n",
    "    # logdir = f\"runs/{now}-{params.run_name}-{run_name}\"\n",
    "    # tb = tb_logger.TensorboardLogger(log_dir=logdir)\n",
    "\n",
    "    wandb_logger = WandBLogger(\n",
    "        project=\"epistasis\",\n",
    "        entity=\"taisikus\",\n",
    "    )\n",
    "    \n",
    "    run_avg = RunningAverage(output_transform=lambda v: v['loss'])\n",
    "    run_avg.attach(engine, \"avg_loss\")\n",
    "\n",
    "    # metrics = ['reward', 'avg_reward']\n",
    "    metrics = ['reward', 'steps', 'avg_reward']\n",
    "    event = EpisodeEvents.EPISODE_COMPLETED\n",
    "    handler = OutputHandler(tag=\"episodes\", metric_names=metrics)\n",
    "    wandb_logger.attach(engine, log_handler=handler, event_name=event)\n",
    "    \n",
    "    # write to tensorboard every 100 iterations\n",
    "    PeriodicEvents().attach(engine)\n",
    "    metrics = ['avg_loss', 'avg_fps']\n",
    "    metrics.extend(extra_metrics)\n",
    "    handler = OutputHandler(tag=\"train\", metric_names=metrics, output_transform=lambda a: a)\n",
    "    event = PeriodEvents.ITERS_100_COMPLETED\n",
    "    wandb_logger.attach(engine, log_handler=handler, event_name=event)\n",
    "    \n",
    "    wandb_logger.watch(net)\n",
    "    \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # print(input_shape)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 64, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # fx = x.float() / 2\n",
    "        fx = x.float() / 2\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "    \n",
    "class EpsilonGreedyActionSelector(ptan.actions.ActionSelector):\n",
    "    def __init__(self, epsilon=0.05, selector=None):\n",
    "        self.epsilon = epsilon\n",
    "        self.selector = selector if selector is not None else ptan.actions.ArgmaxActionSelector()\n",
    "\n",
    "    def __call__(self, scores):\n",
    "        assert isinstance(scores, np.ndarray)\n",
    "        actions = []\n",
    "        for batch in scores:\n",
    "            num_selected_snps = 2\n",
    "            snps_idx = []\n",
    "            if np.random.random() < self.epsilon:\n",
    "                snps_idx = np.random.choice(len(batch), num_selected_snps, replace=False)\n",
    "            else:    \n",
    "                for i in range(num_selected_snps):\n",
    "                    largest_score_idx = np.argmax(batch)\n",
    "                    snps_idx.append(largest_score_idx)\n",
    "                    batch[largest_score_idx] = -1\n",
    "            action = np.zeros(len(batch))        \n",
    "            for snp in snps_idx:\n",
    "                action[snp] = 1\n",
    "            actions.append(action)\n",
    "            \n",
    "        return np.array(actions)    \n",
    "        # batch_size, n_actions = scores.shape\n",
    "        # actions = self.selector(scores)\n",
    "        # mask = np.random.random(size=batch_size) < self.epsilon\n",
    "        # rand_actions = np.random.choice(n_actions, sum(mask))\n",
    "        # actions[mask] = rand_actions\n",
    "        \n",
    "\n",
    "params = SimpleNamespace(**{\n",
    "        'env_name': \"EpistasisEnv\",\n",
    "        'stop_reward': 10000,\n",
    "        'run_name': 'dqn-basic',\n",
    "        'replay_size': 10 ** 6,\n",
    "        'replay_initial': 25000,\n",
    "        'target_net_sync': 10000,\n",
    "        'epsilon_frames': 500000,\n",
    "        'epsilon_start': 1.0,\n",
    "        'epsilon_final': 0.1,\n",
    "        'learning_rate': 0.00025,\n",
    "        'gamma': 0.99,\n",
    "        'batch_size': 32\n",
    "    })\n",
    "\n",
    "# # для prelimitary\n",
    "# params = SimpleNamespace(**{\n",
    "#         'env_name': \"EpistasisEnv\",\n",
    "#         'stop_reward': 1.0,\n",
    "#         'run_name': 'dqn-basic-5000',\n",
    "#         'replay_size': 5000,\n",
    "#         'replay_initial': 100,\n",
    "#         'target_net_sync': 50,\n",
    "#         'epsilon_frames': 4500,\n",
    "#         'epsilon_start': 1.0,\n",
    "#         'epsilon_final': 0.1,\n",
    "#         'learning_rate': 0.001,\n",
    "#         'gamma': 0.99,\n",
    "#         'batch_size': 32\n",
    "#     })\n",
    "NAME = \"dqn_baseline\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # wandb.init(project='epistasis', entity=\"taisikus\", sync_tensorboard=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = EpistasisEnv()\n",
    "    net = DQN(env.observation_space.shape, env.N_SNPS)\n",
    "    net = nn.DataParallel(net)\n",
    "    net = net.to(device)\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    selector = EpsilonGreedyActionSelector(\n",
    "        epsilon=params.epsilon_start)\n",
    "    \n",
    "    epsilon_tracker = EpsilonTracker(selector, params)\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        env, agent, gamma=params.gamma)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "        exp_source, buffer_size=params.replay_size)\n",
    "    optimizer = optim.Adam(net.parameters(),\n",
    "                           lr=params.learning_rate)\n",
    "\n",
    "    def process_batch(engine, batch):\n",
    "        optimizer.zero_grad()\n",
    "        loss_v = calc_loss_dqn(\n",
    "            batch, net, tgt_net.target_model,\n",
    "            gamma=params.gamma, device=device)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        epsilon_tracker.frame(engine.state.iteration)\n",
    "        if engine.state.iteration % params.target_net_sync == 0:\n",
    "            tgt_net.sync()\n",
    "        return {\n",
    "            \"loss\": loss_v.item(),\n",
    "            \"epsilon\": selector.epsilon,\n",
    "        }\n",
    "\n",
    "    engine = Engine(process_batch)\n",
    "    setup_ignite(engine, params, exp_source, NAME, net)\n",
    "    \n",
    "    engine.run(batch_generator(buffer, params.replay_initial,\n",
    "                                      params.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91d3b290-93bc-4a82-8288-2b217081e0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard module is not an IPython extension.\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78c6fa18-4947-4b2c-b10b-fa48b213ac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# a = np.array([[1, 2, 0, 0],[1,0,2,0]])\n",
    "# b = np.zeros((a.size, a.max()+1))\n",
    "# b[np.arange(a.size),a] = 1\n",
    "# b\n",
    "rr = np.array([1, 2, 0, 0])\n",
    "data = torch.tensor(rr)\n",
    "data = F.one_hot(data, 3)\n",
    "\n",
    "print(data.shape)\n",
    "print(rr.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33ebc2e6-a4c7-4145-a555-b50ec02025c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [3]],\n",
       "\n",
       "        [[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "284548cb-ace6-4147-a875-3ba2b8062845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [2]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#как было\n",
    "\n",
    "a = torch.tensor([0, 2])\n",
    "a = torch.unsqueeze(a, -1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06a6a13b-fc8b-44bf-837e-990121481aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 11])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qvalues = torch.tensor([[10, 5, 9], [7, 8, 11]])\n",
    "result = qvalues.gather(1, a)\n",
    "result.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5f6cc64-32d0-4e98-a9f4-e3101b0c6727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9., 11.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#как будет\n",
    "\n",
    "a = torch.tensor([[1, 0, 0, 1],[0,1,0,1]]) #two actions\n",
    "values = torch.tensor([[2, 1, 0.5, 7], [0.1, 5, 4, 6]])\n",
    "\n",
    "torch.sum(a * values, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea1b56-3121-4290-bbf4-716c9000761f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-gen_env]",
   "language": "python",
   "name": "conda-env-.conda-gen_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
