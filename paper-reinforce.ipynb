{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b2dd4-78be-4016-b722-6a0ca21937c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import ptan\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "EPISODE_LENGTH = 1\n",
    "\n",
    "\n",
    "class EpistasisEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.SAMPLE_SIZE = 600 #t1 = t2 = SAMPLE_SIZE\n",
    "        self.reset()\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.N_SNPS,), dtype=np.uint8)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=\n",
    "                        (3, 2*self.SAMPLE_SIZE, self.N_SNPS), dtype=np.uint8)\n",
    "        \n",
    "        \n",
    "    def establish_phen_gen(self, file):\n",
    "        with open(file) as f:\n",
    "            data = json.load(f)\n",
    "            genotype = np.array(data[\"genotype\"])\n",
    "            self.phenotype = np.array(data[\"phenotype\"])\n",
    "            self.genotype = genotype.T\n",
    "            num_phenotypes = max(self.phenotype)+1\n",
    "            self.disease_snps = data[\"disease_snps\"]\n",
    "            self.phen_gen = [[] for _ in range(num_phenotypes)]\n",
    "            for i in range(len(self.genotype)):\n",
    "                self.phen_gen[self.phenotype[i]].append(i)  \n",
    "            return  self.genotype.shape[0], self.genotype.shape[1]\n",
    "        \n",
    "    def normalize_reward(self, current_reward):\n",
    "        maximum_env_reward = self._count_reward(self.disease_snps)\n",
    "        minimal_reward = 0.5\n",
    "        normalized_reward = (current_reward - minimal_reward) / (maximum_env_reward - minimal_reward)\n",
    "        if normalized_reward > 1:\n",
    "            print(\"normalized reward > 1: \\n normalized reward = \", normalized_reward, \"\\n current reward = \", current_reward, \"\\n maximum_env_reward = \", maximum_env_reward )\n",
    "            normalized_reward = 0.1\n",
    "        return normalized_reward\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        snp_ids = self._take_action(action)\n",
    "        reward = self._count_reward(snp_ids)\n",
    "#         без нормализации\n",
    "        # reward = self.normalize_reward(reward)\n",
    "        \n",
    "        self.current_step += 1\n",
    "        if self.current_step == EPISODE_LENGTH:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False  \n",
    "        # done = self.current_step == 1\n",
    "        obs = None if done else self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def _count_reward(self, snp_ids):\n",
    "        \n",
    "        all_existing_seq = defaultdict(lambda: {'control' : 0, 'case' : 0})\n",
    "        for i, idv in enumerate(self.obs):\n",
    "            snp_to_cmp = tuple(idv[snp_id] for snp_id in snp_ids) #tuple of SNP that \n",
    "            if self.obs_phenotypes[i] == 0:\n",
    "                all_existing_seq[snp_to_cmp]['control'] += 1\n",
    "            else:\n",
    "                all_existing_seq[snp_to_cmp]['case'] += 1\n",
    "\n",
    "        #count reward      \n",
    "        TP = 0 #HR case\n",
    "        FP = 0 #HR control\n",
    "        TN = 0 #LR control\n",
    "        FN = 0 #LR case\n",
    "\n",
    "        for case_control_count in all_existing_seq.values():\n",
    "          # if seq is in LR group\n",
    "            if case_control_count['case'] <= case_control_count['control']: #вопрос <= или <\n",
    "                FN += case_control_count['case']\n",
    "                TN += case_control_count['control']\n",
    "            else:\n",
    "          # if seq is in HR group\n",
    "                TP += case_control_count['case']\n",
    "                FP += case_control_count['control']\n",
    "        R = (FP + TN) / (TP + FN)\n",
    "        delta = FP / (TP+0.001)\n",
    "        gamma = (TP + FP + TN + FN) / (TP+0.001)\n",
    "        CCR = 0.5 * (TP / (TP + FN) + TN / (FP + TN))\n",
    "        U = (R - delta)**2 / ((1 + delta) * (gamma - delta - 1 + 0.001))\n",
    "        koef = 1\n",
    "        # if len(snp_ids) > len(self.disease_snps):\n",
    "        #         print(\"len(snp_ids) > len(self.disease_snps)\")\n",
    "        #         koef = 1 / len(snp_ids)\n",
    "\n",
    "        return koef*(CCR + U)\n",
    "\n",
    "  \n",
    "    def reset(self):\n",
    "        \n",
    "        pops = [\"ASW\", \"CEU\", \"CEU+TSI\", \"CHD\", \"GIH\", \"JPT+CHB\", \"LWK\", \"MEX\", \"MKK\", \"TSI\"]\n",
    "        sim_idx = np.random.randint(2500)\n",
    "        corp_idx = np.random.randint(1, 23)\n",
    "        pop_idx = np.random.choice(pops)\n",
    "        \n",
    "        self.filename = f\"/home/tskhakharova/epistasis-rl/epigen/sim/{sim_idx}_{corp_idx}_{pop_idx}.json\"\n",
    "        # filename = f\"/home/tskhakharova/epistasis-rl/epigen/sim/5_7_CEU.json\"\n",
    "        if not os.path.exists(self.filename):\n",
    "            os.system(f\"cd /home/tskhakharova/epistasis-rl/epigen/ && python3 simulate_data.py --sim-ids {sim_idx} --corpus-id {corp_idx} --pop {pop_idx} --inds 5000 --snps 100 --model models/ext_model.ini\")\n",
    "\n",
    "        self.N_IDV, self.N_SNPS = self.establish_phen_gen(self.filename)\n",
    "        \n",
    "        self.obs_phenotypes = None\n",
    "        one_hot_obs = self._next_observation()\n",
    "        self.current_step = 0\n",
    "        \n",
    "        return one_hot_obs\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        chosen_snp_ids = []\n",
    "        for i, choice in enumerate(action):\n",
    "            if choice == 1:\n",
    "                chosen_snp_ids.append(i)\n",
    "        return chosen_snp_ids    \n",
    "    \n",
    "    def _next_observation(self):\n",
    "        id_0 = np.random.choice(self.phen_gen[0], self.SAMPLE_SIZE)\n",
    "        id_1 = np.random.choice(self.phen_gen[1], self.SAMPLE_SIZE)\n",
    "        sample_ids = np.array(list(zip(id_0,id_1))).flatten()\n",
    "        self.obs = np.array([self.genotype[idv] for idv in sample_ids])\n",
    "        self.obs_phenotypes = [self.phenotype[idv] for idv in sample_ids]\n",
    "        \n",
    "        #one_hot\n",
    "        one_hot_obs = F.one_hot(torch.tensor(self.obs), 3)\n",
    "        one_hot_obs = one_hot_obs.movedim(2, 0)\n",
    "\n",
    "        return one_hot_obs\n",
    "    \n",
    "class FixedEpistasisEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, sample_size, n_snps, observation_onehot, filename, observation, obs_phenotypes, disease_snps):\n",
    "        self.one_hot_obs = observation_onehot\n",
    "        self.filename = filename\n",
    "        self.obs = observation\n",
    "        self.obs_phenotypes = obs_phenotypes\n",
    "        self.disease_snps = disease_snps\n",
    "        \n",
    "        self.SAMPLE_SIZE = sample_size #t1 = t2 = SAMPLE_SIZE\n",
    "        self.N_SNPS = n_snps\n",
    "        \n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.N_SNPS,), dtype=np.uint8)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=\n",
    "                        (3, 2*self.SAMPLE_SIZE, self.N_SNPS), dtype=np.uint8)\n",
    "        self.engine = None\n",
    "        \n",
    "        \n",
    "    def normalize_reward(self, current_reward):\n",
    "        maximum_env_reward = self._count_reward(self.disease_snps)\n",
    "        minimal_reward = 0.5\n",
    "        normalized_reward = (current_reward - minimal_reward) / (maximum_env_reward - minimal_reward)\n",
    "        if normalized_reward > 1:\n",
    "            print(\"normalized reward > 1: \\n normalized reward = \", normalized_reward, \"\\n current reward = \", current_reward, \"\\n maximum_env_reward = \", maximum_env_reward )\n",
    "            normalized_reward = 0.1\n",
    "        return normalized_reward\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        snp_ids = self._take_action(action)\n",
    "        reward = self._count_reward(snp_ids)\n",
    "#         без нормализации\n",
    "        # reward = self.normalize_reward(reward)\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = self.current_step == EPISODE_LENGTH\n",
    "        return self.one_hot_obs, reward, done, {}\n",
    "    \n",
    "    def _count_reward(self, snp_ids):\n",
    "        \n",
    "        if set(snp_ids) == set(self.disease_snps):\n",
    "            print(\"Disease snps are found\")\n",
    "            \n",
    "        \n",
    "        all_existing_seq = defaultdict(lambda: {'control' : 0, 'case' : 0})\n",
    "        for i, idv in enumerate(self.obs):\n",
    "            snp_to_cmp = tuple(idv[snp_id] for snp_id in snp_ids) #tuple of SNP that \n",
    "            if self.obs_phenotypes[i] == 0:\n",
    "                all_existing_seq[snp_to_cmp]['control'] += 1\n",
    "            else:\n",
    "                all_existing_seq[snp_to_cmp]['case'] += 1\n",
    "\n",
    "        #count reward      \n",
    "        TP = 0 #HR case\n",
    "        FP = 0 #HR control\n",
    "        TN = 0 #LR control\n",
    "        FN = 0 #LR case\n",
    "\n",
    "        for case_control_count in all_existing_seq.values():\n",
    "          # if seq is in LR group\n",
    "            if case_control_count['case'] <= case_control_count['control']: #вопрос <= или <\n",
    "                FN += case_control_count['case']\n",
    "                TN += case_control_count['control']\n",
    "            else:\n",
    "          # if seq is in HR group\n",
    "                TP += case_control_count['case']\n",
    "                FP += case_control_count['control']\n",
    "        R = (FP + TN) / (TP + FN)\n",
    "        delta = FP / (TP+0.001)\n",
    "        gamma = (TP + FP + TN + FN) / (TP+0.001)\n",
    "        CCR = 0.5 * (TP / (TP + FN) + TN / (FP + TN))\n",
    "        U = (R - delta)**2 / ((1 + delta) * (gamma - delta - 1 + 0.001))\n",
    "        koef = 1\n",
    "        #добавила коэффициент\n",
    "        if len(snp_ids) > len(self.disease_snps):\n",
    "                print(\"len(snp_ids) > len(self.disease_snps)\")\n",
    "                koef = 1 / len(snp_ids)\n",
    "отнимаю 0.5\n",
    "\n",
    "        return (CCR + U - 0.5)*koef\n",
    "\n",
    "  \n",
    "    def reset(self):\n",
    "\n",
    "        self.current_step = 0\n",
    "        \n",
    "        return self.one_hot_obs\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        chosen_snp_ids = []\n",
    "        for i, choice in enumerate(action):\n",
    "            if choice == 1:\n",
    "                chosen_snp_ids.append(i)\n",
    "        return chosen_snp_ids  \n",
    "\n",
    "class EpiProbabilityActionSelector(ptan.actions.ActionSelector):\n",
    "    \"\"\"\n",
    "    Converts probabilities of actions into action by sampling them\n",
    "    \"\"\"\n",
    "    def __call__(self, probs):\n",
    "        assert isinstance(probs, np.ndarray)\n",
    "        assert isinstance(probs[0], np.ndarray)\n",
    "        actions = []\n",
    "#         print(\"EpiProbabilityActionSelector - probs shape:\", probs.shape)\n",
    "        for prob in probs:\n",
    "            # print(prob, end=' ')\n",
    "            num_selected_snps = 0\n",
    "            for oneprob in prob:\n",
    "                if oneprob > 1/len(prob):\n",
    "                    num_selected_snps += 1\n",
    "            wandb.log({\"num_selected_snps\":num_selected_snps}, commit=False)        \n",
    "            # print(f'{num_selected_snps=}')        \n",
    "            if num_selected_snps < 2:\n",
    "                print(\"num_selected_snps < 2\")\n",
    "                num_selected_snps = 2\n",
    "                    \n",
    "        # for prob in probs:\n",
    "        #     num_selected_snps = 2\n",
    "            # num_selected_snps = 0\n",
    "            # amount_of_oneprob_more_than_1_div_n = 0\n",
    "            # while amount_of_oneprob_more_than_1_div_n < 2:\n",
    "            #     amount_of_oneprob_more_than_1_div_n = 0\n",
    "            #     if num_selected_snps > len(prob)/10:\n",
    "            #         num_selected_snps = int(len(prob)/10)\n",
    "            #         break\n",
    "            #     num_selected_snps += 1\n",
    "            #     for oneprob in prob:\n",
    "            #         if oneprob > 1 / num_selected_snps:\n",
    "            #             amount_of_oneprob_more_than_1_div_n += 1\n",
    "            \n",
    "            chosen_snp = np.random.choice(len(prob), size=num_selected_snps, replace=False, p=prob)\n",
    "            action = np.zeros(len(prob))\n",
    "            for snp in chosen_snp:\n",
    "                action[snp] = 1\n",
    "            actions.append(action)\n",
    "        return np.array(actions)\n",
    "    \n",
    "\n",
    "class SnpPGN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(SnpPGN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 64, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float()\n",
    "#         fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc(conv_out)  \n",
    "    \n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "EPISODES_TO_TRAIN = 32\n",
    "COUNT = 1000000\n",
    "WANDB = True\n",
    "AMOUNT_OF_DATA = 1\n",
    "SAMPLE_SIZE = 600 #t1 = t2 = SAMPLE_SIZE\n",
    "\n",
    "\n",
    "def calc_qvals(rewards):\n",
    "    res = []\n",
    "    sum_r = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= GAMMA\n",
    "        sum_r += r\n",
    "        res.append(sum_r)\n",
    "    return list(reversed(res))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    temp_env = EpistasisEnv()\n",
    "    fixed_observation_onehot = temp_env.reset()\n",
    "    fixed_filename = temp_env.filename\n",
    "    fixed_observation = temp_env.obs\n",
    "    fixed_obs_phenotypes = temp_env.obs_phenotypes\n",
    "    fixed_disease_snps = temp_env.disease_snps\n",
    "    fixed_sample_size = temp_env.SAMPLE_SIZE\n",
    "    fixed_n_snps = temp_env.N_SNPS\n",
    "    env = FixedEpistasisEnv(fixed_sample_size, fixed_n_snps, fixed_observation_onehot, fixed_filename, fixed_observation, fixed_obs_phenotypes, fixed_disease_snps)\n",
    "\n",
    "    if WANDB:\n",
    "        wandb.init(project=\"epistasis\", entity=\"taisikus\", config={\n",
    "          \"learning_rate\": LEARNING_RATE,\n",
    "          \"gamma\": GAMMA,\n",
    "          \"episodes_to_train\": EPISODES_TO_TRAIN,\n",
    "          \"steps_number\" : COUNT,\n",
    "          \"data_amount\": AMOUNT_OF_DATA,\n",
    "          \"sample_size\": SAMPLE_SIZE\n",
    "        })\n",
    "        \n",
    "    net = SnpPGN(env.observation_space.shape, env.N_SNPS)\n",
    "    net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "    print(net)\n",
    "    agent = ptan.agent.PolicyAgent(net, action_selector=EpiProbabilityActionSelector(), apply_softmax=True, device=device)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    total_rewards = []\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "\n",
    "    batch_episodes = 0\n",
    "    batch_states, batch_actions, batch_qvals = [], [], []\n",
    "    cur_rewards = []\n",
    "\n",
    "    for step_idx, exp in enumerate(exp_source):\n",
    "        batch_states.append(exp.state)\n",
    "#         batch_actions.append(int(exp.action))\n",
    "        batch_actions.append(exp.action)\n",
    "        cur_rewards.append(exp.reward)\n",
    "\n",
    "        if exp.last_state is None:\n",
    "            batch_qvals.extend(calc_qvals(cur_rewards))\n",
    "            cur_rewards.clear()\n",
    "            batch_episodes += 1\n",
    "\n",
    "        # handle new rewards\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            done_episodes += 1\n",
    "            reward = new_rewards[0]\n",
    "            total_rewards.append(reward)\n",
    "            mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "\n",
    "            if WANDB:\n",
    "                wandb.log({\"reward\": reward, \"mean_100\": mean_rewards, \"episodes\": done_episodes})\n",
    "            # print(done_episodes)    \n",
    "            # if mean_rewards > 0.9:\n",
    "            #     print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "            #     break\n",
    "            if done_episodes > COUNT:\n",
    "                print(f\"done_episodes > {COUNT}\")\n",
    "                break\n",
    "\n",
    "        if batch_episodes < EPISODES_TO_TRAIN:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        states_v = torch.stack(batch_states)\n",
    "        states_v = states_v.to(device)\n",
    "        batch_actions_t = torch.FloatTensor(batch_actions)\n",
    "        batch_actions_t = batch_actions_t.to(device)\n",
    "        batch_qvals_v = torch.FloatTensor(batch_qvals)\n",
    "        batch_qvals_v = batch_qvals_v.to(device)\n",
    "\n",
    "        logits_v = net(states_v)\n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        \n",
    "#         print(log_prob_v.shape)\n",
    "#         print(batch_qvals_v.shape)\n",
    "#         print(len(batch_states))\n",
    "#         print(batch_actions_t.shape)\n",
    "#         log_prob_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]\n",
    "        log_prob_actions_v = batch_qvals_v * torch.diagonal(torch.mm(log_prob_v, torch.transpose(batch_actions_t, 0, 1)))\n",
    "        loss_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_episodes = 0\n",
    "        batch_states.clear()\n",
    "        batch_actions.clear()\n",
    "        batch_qvals.clear()\n",
    "    if WANDB:\n",
    "        wandb.finish()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8704762-e335-4fe0-adc8-1108ab7819eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71743177-d55e-430a-9aaf-f36850272da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2000, 0.2000, 0.2001, 0.2000, 0.2000]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "probs_v = torch.tensor([[0.0001, 0.0001, 0.0005, 0.0001, 0.0001]])\n",
    "probs_v = F.softmax(probs_v, dim=1)\n",
    "print(probs_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc2b347a-2eb9-4bd6-bb98-e85aae4c2dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "lst = '''0.00957609 0.00995838 0.00962002 0.00993365 0.01054768 0.00969078\n",
    " 0.01019476 0.01030528 0.0100608  0.00987881 0.0099819  0.00954287\n",
    " 0.00997524 0.00994093 0.00969229 0.00970027 0.01008452 0.00992992\n",
    " 0.01041642 0.01034096 0.00990605 0.0097714  0.01018124 0.01035526\n",
    " 0.01017827 0.00960341 0.01022878 0.00978832 0.01021083 0.00987902\n",
    " 0.01004206 0.00968949 0.01029002 0.00968409 0.00969813 0.00992512\n",
    " 0.0099074  0.01005321 0.01019682 0.00977789 0.01040583 0.00975953\n",
    " 0.00983522 0.01033232 0.00988222 0.00979967 0.0103385  0.01035107\n",
    " 0.01029485 0.009688   0.01012141 0.00995612 0.00996633 0.01002883\n",
    " 0.0100872  0.00993829 0.00990269 0.00978925 0.00998295 0.00990761\n",
    " 0.01034808 0.01042473 0.01042838 0.00984655 0.00963238 0.01040053\n",
    " 0.01005131 0.01003871 0.00987554 0.01049269 0.01014477 0.00966513\n",
    " 0.00985006 0.00984131 0.01006806 0.00965796 0.01035243 0.01018831\n",
    " 0.01027447 0.00968101 0.00991021 0.00956005 0.00997654 0.01035285\n",
    " 0.00981688 0.01017686 0.0097863  0.01029063 0.0099022  0.01042551\n",
    " 0.00994771 0.01002956 0.00969542 0.00987345 0.0099112  0.01040517\n",
    " 0.00988034 0.00972794 0.01023857 0.00972382'''.split()\n",
    "lst = list(map(float, lst))\n",
    "\n",
    "cnt=0\n",
    "for elem in lst:\n",
    "    if elem > 1/len(lst):\n",
    "        cnt +=1\n",
    "print(cnt)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a46020d9-e681-4d28-a196-26580fddd36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "lst = []\n",
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5,6])\n",
    "lst.append(a)\n",
    "lst.append(b)\n",
    "\n",
    "lst = torch.stack(lst)\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb088815-2172-479c-8cf5-a990b2e1b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('krk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da81c86-f16b-4b42-830d-26e8189ef236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-gen_env]",
   "language": "python",
   "name": "conda-env-.conda-gen_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
