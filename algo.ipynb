{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adaec485-904f-4e6b-9434-75b9a6268551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import ptan\n",
    "\n",
    "\n",
    "\n",
    "class EpistasisEnv(gym.Env):\n",
    "    \n",
    "#metadata = {'render.modes': ['human']}\n",
    "    def __init__(self):\n",
    "        self.SAMPLE_SIZE = 300 #t1 = t2 = SAMPLE_SIZE\n",
    "        filename = \"./epigen/sim/0_2_ASW.json\"\n",
    "\n",
    "        def establish_phen_gen(self, file):\n",
    "            with open(file) as f:\n",
    "                data = json.load(f)\n",
    "                genotype = np.array(data[\"genotype\"])\n",
    "                self.phenotype = np.array(data[\"phenotype\"])\n",
    "                self.genotype = genotype.T\n",
    "                num_phenotypes = max(self.phenotype)+1\n",
    "                self.disease_snps = data[\"disease_snps\"]\n",
    "                self.phen_gen = [[] for _ in range(num_phenotypes)]\n",
    "                for i in range(len(self.genotype)):\n",
    "                    self.phen_gen[self.phenotype[i]].append(i)  \n",
    "                return  self.genotype.shape[0], self.genotype.shape[1]\n",
    "\n",
    "        super(EpistasisEnv, self).__init__()\n",
    "\n",
    "        self.N_IDV, self.N_SNPS = establish_phen_gen(self, filename)\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.N_SNPS,), dtype=np.uint8)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, shape=\n",
    "                        (2*self.SAMPLE_SIZE, self.N_SNPS), dtype=np.uint8)\n",
    "        self.obs_phenotypes = None\n",
    "        self.obs = None\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        snp_ids = self._take_action(action)\n",
    "        print(f\"{snp_ids=}\")\n",
    "        reward = self._count_reward(snp_ids) \n",
    "        self.current_step += 1\n",
    "        done = self.current_step == 1\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "    def _count_reward(self, snp_ids):\n",
    "    \n",
    "        all_existing_seq = defaultdict(lambda: {'control' : 0, 'case' : 0})\n",
    "        \n",
    "\n",
    "        for i, idv in enumerate(self.obs):\n",
    "            snp_to_cmp = tuple(idv[snp_id] for snp_id in snp_ids) #tuple of SNP that \n",
    "            if self.obs_phenotypes[i] == 0:\n",
    "                all_existing_seq[snp_to_cmp]['control'] += 1\n",
    "            else:\n",
    "                all_existing_seq[snp_to_cmp]['case'] += 1\n",
    "\n",
    "        ###count reward      \n",
    "        TP = 0 #HR case\n",
    "        FP = 0 #HR control\n",
    "        TN = 0 #LR control\n",
    "        FN = 0 #LR case\n",
    "\n",
    "        for case_control_count in all_existing_seq.values():\n",
    "          # if seq is in LR group\n",
    "            if case_control_count['case'] <= case_control_count['control']: #вопрос <= или <\n",
    "                FN += case_control_count['case']\n",
    "                TN += case_control_count['control']\n",
    "            else:\n",
    "          # if seq is in HR group\n",
    "                TP += case_control_count['case']\n",
    "                FP += case_control_count['control']\n",
    "        R = (FP + TN) / (TP + FN)\n",
    "        delta = FP / (TP+0.001)\n",
    "        gamma = (TP + FP + TN + FN) / (TP+0.001)\n",
    "        CCR = 0.5 * (TP / (TP + FN) + TN / (FP + TN))\n",
    "#         print(\"delta:\",delta,\"gamma:\", gamma)\n",
    "        U = (R - delta)**2 / ((1 + delta) * (gamma - delta - 1 + 0.001))\n",
    "\n",
    "        return CCR + U\n",
    "\n",
    "  \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.obs = self._next_observation()\n",
    "        return self.obs\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        chosen_snp_ids = []\n",
    "        for i, choice in enumerate(action):\n",
    "            if choice == 1:\n",
    "                chosen_snp_ids.append(i)\n",
    "        return chosen_snp_ids    \n",
    "    def _next_observation(self):\n",
    "        id_0 = np.random.choice(self.phen_gen[0], self.SAMPLE_SIZE)\n",
    "        id_1 = np.random.choice(self.phen_gen[1], self.SAMPLE_SIZE)\n",
    "        sample_ids = np.array(list(zip(id_0,id_1))).flatten()\n",
    "        self.obs = np.array([self.genotype[idv] for idv in sample_ids])\n",
    "        self.obs_phenotypes = [self.phenotype[idv] for idv in sample_ids]\n",
    "\n",
    "        return self.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2f49efa-1d9d-4f04-ae7b-4e0650380d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpiProbabilityActionSelector(ptan.actions.ActionSelector):\n",
    "    \"\"\"\n",
    "    Converts probabilities of actions into action by sampling them\n",
    "    \"\"\"\n",
    "    def __call__(self, probs):\n",
    "        assert isinstance(probs, np.ndarray)\n",
    "        assert isinstance(probs[0], np.ndarray)\n",
    "        actions = []\n",
    "        print(\"EpiProbabilityActionSelector - probs shape:\", probs.shape)\n",
    "        for prob in probs:\n",
    "#             print(\"prob\", prob.shape)\n",
    "            num_selected_snps = 0\n",
    "            for oneprob in prob:\n",
    "                if oneprob > 1/len(prob):\n",
    "                    num_selected_snps += 1\n",
    "#             sum(1 for oneprob in prob if oneprob > 1/len(prob))\n",
    "            chosen_snp = np.random.choice(len(prob), size=num_selected_snps, replace=False, p=prob)\n",
    "            action = np.zeros(len(prob))\n",
    "            for snp in chosen_snp:\n",
    "                action[snp] = 1\n",
    "            actions.append(action)\n",
    "        return np.array(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f73c6a2-56cb-4ea8-ab03-8e3f17493c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0 2]\n",
      "2\n",
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "probs = [[0.8, 0.1, 0.1], [0.4, 0.4, 0.2]]\n",
    "for prob in probs:\n",
    "            num_selected_snps = sum(1 for oneprob in prob if oneprob > 1/len(prob))\n",
    "            print(num_selected_snps)\n",
    "            chosen_snp = np.random.choice(len(prob),size=2, p=prob, replace=False)\n",
    "            print(chosen_snp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6064a970-7645-4c09-b3ab-7e9e3819dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class LinearARD(nn.Module):\n",
    "    def __init__(self, in_features, out_features, threshold, bias=True):\n",
    "        super(LinearARD, self).__init__()\n",
    "        \"\"\"\n",
    "            in_features: int, a number of input features\n",
    "            out_features: int, a number of neurons\n",
    "            threshold: float, a threshold for clipping weights\n",
    "        \"\"\"\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.mu = nn.parameter.Parameter(torch.Tensor(self.out_features, self.in_features)) # torch.nn.parameter.Parameter of size out_features x in_features\n",
    "        self.log_sigma = nn.parameter.Parameter(torch.Tensor(self.out_features, self.in_features)) # torch.nn.parameter.Parameter of size out_features x in_features\n",
    "        self.bias = nn.parameter.Parameter(torch.Tensor(1, self.out_features)) # torch.nn.parameter.Parameter of size 1 x out_features\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.bias.data.zero_()\n",
    "        self.mu.data.normal_(0, 0.02)\n",
    "        self.log_sigma.data.fill_(-5)        \n",
    "        \n",
    "    def forward(self, x):      \n",
    "        # x is a torch.Tensor of shape (number_of_objects, in_features)\n",
    "        # log_alpha is a torch.Tensor of shape (out_features, in_features)\n",
    "        self.log_alpha = 2*self.log_sigma-torch.log(self.mu**2+1e-16)# Compute using self.log_sigma and self.mu\n",
    "        # clipping for a numerical stability\n",
    "        self.log_alpha = torch.clamp(self.log_alpha, -10, 10)   \n",
    "        \n",
    "        if self.training:\n",
    "            # LRT = local reparametrization trick\n",
    "            # lrt_mean is a torch.Tensor of shape (x.shape[0], out_features)\n",
    "            lrt_mean = F.linear(input=x, weight=self.mu, bias=self.bias) # compute mean activation using LRT; you can use F.linear\n",
    "            # lrt_std is a torch.Tensor of shape (x.shape[0], out_features)\n",
    "            lrt_std = torch.sqrt(1e-8+F.linear(input=x**2, weight=torch.exp(2*self.log_sigma), bias=None)) # compute std of activations unsig lrt; you can use F.linear\n",
    "                      # do not forget use torch.sqrt(x + 1e-8) instead of torch.sqrt(x)\n",
    "            # eps is a torch.Tensor of shape (x.shape[0], out_features)\n",
    "            eps = torch.randn_like(lrt_std)# sample of noise for reparametrization\n",
    "            return lrt_mean+lrt_std*eps# sample of activation\n",
    "        \n",
    "        # compute the output of the layer\n",
    "        # use weights W = E q = self.mu\n",
    "        # clip all weight with log_alpha > threshold\n",
    "        return F.linear(input=x, weight=self.mu*(self.log_alpha < self.threshold).float(), bias=self.bias)\n",
    "        \n",
    "    def kl_reg(self):\n",
    "        # kl is a scalar torch.Tensor \n",
    "        # kl = # eval the KL divergence\n",
    "        log_alpha = 2*self.log_sigma-torch.log(self.mu**2+1e-16) # Eval log alpha as a function(log_sigma, W)\n",
    "        log_alpha = torch.clamp(log_alpha, -10, 10)# Clip log alpha to be in [-10, 10] for numerical suability \n",
    "        kl = - 0.5 * torch.log1p(torch.exp(-log_alpha))\n",
    "        KL  = - torch.sum(kl)\n",
    "        return KL\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, threshold):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = LinearARD(100, 300, threshold)\n",
    "        self.fc2 = LinearARD(300,  100, threshold)\n",
    "#         self.fc3 = LinearARD(100,  10, threshold)\n",
    "        self.threshold=threshold\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c11bb3-fafa-48fb-9169-a3654e49f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class SnpPGN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(SnpPGN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "#         fx = x.float() / 256\n",
    "        fx = x.float() / 3\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "452a179a-12d7-4975-a3e9-961392c15dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SnpPGN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv1d(600, 32, kernel_size=(8,), stride=(4,))\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(32, 64, kernel_size=(4,), stride=(2,))\n",
      "    (3): ReLU()\n",
      "    (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=576, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=100, bias=True)\n",
      "  )\n",
      ")\n",
      "probs 1\n",
      "prob (100,)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'done' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     batch_states, batch_actions, batch_qvals \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m     69\u001b[0m     cur_rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step_idx, exp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(exp_source):\n\u001b[1;32m     72\u001b[0m         batch_states\u001b[38;5;241m.\u001b[39mappend(exp\u001b[38;5;241m.\u001b[39mstate)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#         batch_actions.append(int(exp.action))\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/ptan/experience.py:176\u001b[0m, in \u001b[0;36mExperienceSourceFirstLast.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(ExperienceSourceFirstLast, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m():\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m exp[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(exp) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps:\n\u001b[1;32m    178\u001b[0m             last_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/ptan/experience.py:94\u001b[0m, in \u001b[0;36mExperienceSource.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m     next_state_n, r_n, is_done_n, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action_n)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     next_state, r, is_done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_n\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     next_state_n, r_n, is_done_n \u001b[38;5;241m=\u001b[39m [next_state], [r], [is_done]\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ofs, (action, next_state, r, is_done) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(action_n, next_state_n, r_n, is_done_n)):\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mEpistasisEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     45\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_observation()\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, \u001b[43mdone\u001b[49m, {}\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'done' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import gym\n",
    "# import ptan\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "EPISODES_TO_TRAIN = 4\n",
    "\n",
    "\n",
    "# class PGN(nn.Module):\n",
    "#     def __init__(self, input_size, n_actions):\n",
    "#         super(PGN, self).__init__()\n",
    "\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(input_size, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, n_actions)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "\n",
    "def calc_qvals(rewards):\n",
    "    res = []\n",
    "    sum_r = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= GAMMA\n",
    "        sum_r += r\n",
    "        res.append(sum_r)\n",
    "    return list(reversed(res))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SAMPLE_SIZE = 300 #t1 = t2 = SAMPLE_SIZE\n",
    "#     filename = \"./epigen/sim/0_2_ASW.json\"\n",
    "    env = EpistasisEnv()\n",
    "#     wandb.init(project=\"epistasis\", entity=\"taisikus\")\n",
    "#     wandb.config = {\n",
    "#       \"learning_rate\": 0.01,\n",
    "#       \"gamma\": 0.99,\n",
    "#       \"episodes_to_train\": 4\n",
    "#     }\n",
    "#     writer = SummaryWriter(comment=\"-cartpole-reinforce\")\n",
    "\n",
    "#     net = PGN(env.observation_space.shape[1], env.N_SNPS)\n",
    "    net = SnpPGN(env.observation_space.shape, env.N_SNPS)\n",
    "    print(net)\n",
    "    agent = ptan.agent.PolicyAgent(net, action_selector=EpiProbabilityActionSelector(),preprocessor=ptan.agent.float32_preprocessor,\n",
    "                                   apply_softmax=True)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    total_rewards = []\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "\n",
    "    batch_episodes = 0\n",
    "    batch_states, batch_actions, batch_qvals = [], [], []\n",
    "    cur_rewards = []\n",
    "\n",
    "    for step_idx, exp in enumerate(exp_source):\n",
    "        batch_states.append(exp.state)\n",
    "#         batch_actions.append(int(exp.action))\n",
    "        batch_actions.append(exp.action)\n",
    "        cur_rewards.append(exp.reward)\n",
    "\n",
    "        if exp.last_state is None:\n",
    "            batch_qvals.extend(calc_qvals(cur_rewards))\n",
    "            cur_rewards.clear()\n",
    "            batch_episodes += 1\n",
    "\n",
    "        # handle new rewards\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            done_episodes += 1\n",
    "            reward = new_rewards[0]\n",
    "            total_rewards.append(reward)\n",
    "            mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "            print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\" % (\n",
    "                step_idx, reward, mean_rewards, done_episodes))\n",
    "#             wandb.log({\"reward\": reward, \"mean_100\": mean_rewards, \"episodes\": done_episodes})\n",
    "#             writer.add_scalar(\"reward\", reward, step_idx)\n",
    "#             writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "#             writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "            if mean_rewards > 0.96:\n",
    "                print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "                break\n",
    "\n",
    "        if batch_episodes < EPISODES_TO_TRAIN:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        states_v = torch.FloatTensor(batch_states)\n",
    "        batch_actions_t = torch.LongTensor(batch_actions)\n",
    "        batch_qvals_v = torch.FloatTensor(batch_qvals)\n",
    "\n",
    "        logits_v = net(states_v)\n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        log_prob_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]\n",
    "        loss_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_episodes = 0\n",
    "        batch_states.clear()\n",
    "        batch_actions.clear()\n",
    "        batch_qvals.clear()\n",
    "\n",
    "#     writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f829b02-25f7-49c8-b7aa-85c863af4973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 2, (600, 100), uint8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(600, 100)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = EpistasisEnv()\n",
    "print(env.observation_space)\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db75e8b8-3fdf-4716-8b16-deb211dd5e4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 2\u001b[0m observation, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObservation : \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(observation\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReward      : \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(reward))\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mEpistasisEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     41\u001b[0m     snp_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_action(action)\n\u001b[0;32m---> 42\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msnp_ids\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m10\u001b[39m:\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mEpistasisEnv._count_reward\u001b[0;34m(self, snp_ids)\u001b[0m\n\u001b[1;32m     74\u001b[0m         FP \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m case_control_count[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontrol\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     75\u001b[0m R \u001b[38;5;241m=\u001b[39m (FP \u001b[38;5;241m+\u001b[39m TN) \u001b[38;5;241m/\u001b[39m (TP \u001b[38;5;241m+\u001b[39m FN)\n\u001b[0;32m---> 76\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[43mFP\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTP\u001b[49m\n\u001b[1;32m     77\u001b[0m gamma \u001b[38;5;241m=\u001b[39m (TP \u001b[38;5;241m+\u001b[39m FP \u001b[38;5;241m+\u001b[39m TN \u001b[38;5;241m+\u001b[39m FN) \u001b[38;5;241m/\u001b[39m TP\n\u001b[1;32m     78\u001b[0m CCR \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (TP \u001b[38;5;241m/\u001b[39m (TP \u001b[38;5;241m+\u001b[39m FN) \u001b[38;5;241m+\u001b[39m TN \u001b[38;5;241m/\u001b[39m (FP \u001b[38;5;241m+\u001b[39m TN))\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "print('Observation : ' + str(observation.shape))\n",
    "print('Reward      : ' + str(reward))\n",
    "print('Done        : ' + str(done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c9db7-3b24-4063-bcd1-1279b561ea57",
   "metadata": {},
   "source": [
    "## Создали среду и запустили"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3d2fa99-66a7-4c81-afb2-5525af575193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 1, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 1, ..., 2, 0, 0],\n",
       "       [0, 0, 2, ..., 2, 0, 0],\n",
       "       [0, 1, 0, ..., 2, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = EpistasisEnv()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfc0c914-33c6-4eed-a7de-38a8cbf41798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b091aeec-75e1-46f4-8bd9-6b9da7fc52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.random.random(size=100)\n",
    "probs /= probs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01d6fd4c-f9da-472a-a6f2-90f2dd357cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpiProbabilityActionSelector - probs shape: (1, 100)\n",
      "(1, 100) [[0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      "  1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      "  1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1.\n",
      "  1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0.\n",
      "  0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "action_selector = EpiProbabilityActionSelector()\n",
    "probs = np.array([probs])\n",
    "\n",
    "action = action_selector(probs)\n",
    "print(action.shape, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd8cca1e-6903-4300-95fb-30ce0c1a1936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snp_ids=[0, 3, 4, 5, 8, 11, 12, 15, 16, 19, 21, 22, 27, 28, 29, 32, 34, 35, 37, 39, 40, 42, 43, 45, 47, 48, 50, 52, 53, 54, 57, 60, 61, 64, 68, 74, 76, 78, 79, 84, 86, 87, 89, 92, 96, 99]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 1, ..., 1, 0, 0],\n",
       "        [0, 0, 2, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 1, ..., 1, 0, 0],\n",
       "        [0, 0, 2, ..., 1, 0, 0],\n",
       "        [0, 1, 1, ..., 1, 0, 0]]),\n",
       " 1.9990076523764393,\n",
       " True,\n",
       " {})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cff03396-2953-4201-9266-3b2418763ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "print(env.observation_space)\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94219f-e222-427a-95cb-96fbbb11ebed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abcc8f2-c861-4398-9795-f2a4e66e820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for i in range(500):\n",
    "   observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "   print('Observation : ' + str(observation.shape))\n",
    "   print('Reward      : ' + str(reward))\n",
    "   print('Done        : ' + str(done))\n",
    "   if done:\n",
    "        print(observation)\n",
    "   print('---------------------')\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-gen_env]",
   "language": "python",
   "name": "conda-env-.conda-gen_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
