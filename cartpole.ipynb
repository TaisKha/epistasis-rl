{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5afb1f-e0b0-47b6-8868-8c3df18e94da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tskhakharova/.conda/envs/gen_env/lib/python3.9/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "61: reward:  61.00, mean_100:  61.00, episodes: 1\n",
      "86: reward:  25.00, mean_100:  43.00, episodes: 2\n",
      "100: reward:  14.00, mean_100:  33.33, episodes: 3\n",
      "torch.Size([140, 2])\n",
      "tensor([[-0.6635, -0.7237],\n",
      "        [-0.6570, -0.7306],\n",
      "        [-0.6630, -0.7242],\n",
      "        [-0.6568, -0.7309],\n",
      "        [-0.6627, -0.7246],\n",
      "        [-0.6566, -0.7311],\n",
      "        [-0.6623, -0.7250],\n",
      "        [-0.6786, -0.7079],\n",
      "        [-0.6974, -0.6889],\n",
      "        [-0.7082, -0.6783],\n",
      "        [-0.7005, -0.6859],\n",
      "        [-0.7114, -0.6752],\n",
      "        [-0.7103, -0.6762],\n",
      "        [-0.7160, -0.6708],\n",
      "        [-0.7093, -0.6772],\n",
      "        [-0.6969, -0.6894],\n",
      "        [-0.6751, -0.7115],\n",
      "        [-0.6603, -0.7271],\n",
      "        [-0.6800, -0.7065],\n",
      "        [-0.6608, -0.7266],\n",
      "        [-0.6526, -0.7354],\n",
      "        [-0.6423, -0.7467],\n",
      "        [-0.6523, -0.7358],\n",
      "        [-0.6414, -0.7477],\n",
      "        [-0.6518, -0.7363],\n",
      "        [-0.6693, -0.7175],\n",
      "        [-0.6512, -0.7369],\n",
      "        [-0.6425, -0.7465],\n",
      "        [-0.6521, -0.7360],\n",
      "        [-0.6433, -0.7456],\n",
      "        [-0.6535, -0.7344],\n",
      "        [-0.6802, -0.7063],\n",
      "        [-0.6553, -0.7325],\n",
      "        [-0.6830, -0.7034],\n",
      "        [-0.6584, -0.7291],\n",
      "        [-0.6856, -0.7007],\n",
      "        [-0.6631, -0.7242],\n",
      "        [-0.6888, -0.6975],\n",
      "        [-0.6672, -0.7198],\n",
      "        [-0.6460, -0.7426],\n",
      "        [-0.6717, -0.7150],\n",
      "        [-0.6959, -0.6904],\n",
      "        [-0.6760, -0.7106],\n",
      "        [-0.7002, -0.6862],\n",
      "        [-0.6807, -0.7058],\n",
      "        [-0.6607, -0.7266],\n",
      "        [-0.6484, -0.7400],\n",
      "        [-0.6449, -0.7438],\n",
      "        [-0.6518, -0.7362],\n",
      "        [-0.6717, -0.7150],\n",
      "        [-0.6549, -0.7330],\n",
      "        [-0.6504, -0.7378],\n",
      "        [-0.6481, -0.7404],\n",
      "        [-0.6323, -0.7579],\n",
      "        [-0.6553, -0.7325],\n",
      "        [-0.6578, -0.7298],\n",
      "        [-0.6794, -0.7071],\n",
      "        [-0.6613, -0.7261],\n",
      "        [-0.6587, -0.7288],\n",
      "        [-0.6660, -0.7211],\n",
      "        [-0.6894, -0.6969],\n",
      "        [-0.6598, -0.7276],\n",
      "        [-0.6554, -0.7324],\n",
      "        [-0.6440, -0.7449],\n",
      "        [-0.6558, -0.7319],\n",
      "        [-0.6586, -0.7290],\n",
      "        [-0.6697, -0.7172],\n",
      "        [-0.6856, -0.7008],\n",
      "        [-0.6698, -0.7171],\n",
      "        [-0.6852, -0.7011],\n",
      "        [-0.6983, -0.6880],\n",
      "        [-0.6862, -0.7001],\n",
      "        [-0.6999, -0.6864],\n",
      "        [-0.7034, -0.6830],\n",
      "        [-0.7029, -0.6835],\n",
      "        [-0.6940, -0.6923],\n",
      "        [-0.7055, -0.6810],\n",
      "        [-0.6985, -0.6879],\n",
      "        [-0.7085, -0.6780],\n",
      "        [-0.7034, -0.6830],\n",
      "        [-0.7134, -0.6733],\n",
      "        [-0.7139, -0.6728],\n",
      "        [-0.7093, -0.6773],\n",
      "        [-0.7167, -0.6702],\n",
      "        [-0.7219, -0.6652],\n",
      "        [-0.7199, -0.6671],\n",
      "        [-0.6671, -0.7199],\n",
      "        [-0.6799, -0.7066],\n",
      "        [-0.6675, -0.7194],\n",
      "        [-0.6634, -0.7239],\n",
      "        [-0.6674, -0.7196],\n",
      "        [-0.6632, -0.7240],\n",
      "        [-0.6508, -0.7374],\n",
      "        [-0.6403, -0.7489],\n",
      "        [-0.6336, -0.7565],\n",
      "        [-0.6234, -0.7681],\n",
      "        [-0.6285, -0.7622],\n",
      "        [-0.6250, -0.7663],\n",
      "        [-0.6255, -0.7657],\n",
      "        [-0.6230, -0.7686],\n",
      "        [-0.6684, -0.7185],\n",
      "        [-0.6845, -0.7019],\n",
      "        [-0.7037, -0.6827],\n",
      "        [-0.7124, -0.6742],\n",
      "        [-0.7076, -0.6789],\n",
      "        [-0.7162, -0.6707],\n",
      "        [-0.7137, -0.6730],\n",
      "        [-0.7188, -0.6682],\n",
      "        [-0.7183, -0.6687],\n",
      "        [-0.7148, -0.6720],\n",
      "        [-0.6960, -0.6903],\n",
      "        [-0.6730, -0.7137],\n",
      "        [-0.6622, -0.7250],\n",
      "        [-0.6507, -0.7374],\n",
      "        [-0.6431, -0.7458],\n",
      "        [-0.6530, -0.7350],\n",
      "        [-0.6423, -0.7467],\n",
      "        [-0.6379, -0.7516],\n",
      "        [-0.6415, -0.7476],\n",
      "        [-0.6397, -0.7496],\n",
      "        [-0.6438, -0.7451],\n",
      "        [-0.6527, -0.7353],\n",
      "        [-0.6457, -0.7430],\n",
      "        [-0.6403, -0.7490],\n",
      "        [-0.6334, -0.7567],\n",
      "        [-0.6250, -0.7663],\n",
      "        [-0.6330, -0.7572],\n",
      "        [-0.6239, -0.7676],\n",
      "        [-0.6168, -0.7758],\n",
      "        [-0.6217, -0.7700],\n",
      "        [-0.6314, -0.7590],\n",
      "        [-0.6199, -0.7722],\n",
      "        [-0.6294, -0.7612],\n",
      "        [-0.6398, -0.7495],\n",
      "        [-0.6262, -0.7649],\n",
      "        [-0.6357, -0.7541],\n",
      "        [-0.6221, -0.7696],\n",
      "        [-0.6122, -0.7812],\n",
      "        [-0.6076, -0.7867],\n",
      "        [-0.6114, -0.7821]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([140])\n",
      "140\n",
      "torch.Size([140])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/job-747092/ipykernel_207290/1963277201.py:91: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  states_v = torch.FloatTensor(batch_states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140: reward:  40.00, mean_100:  35.00, episodes: 4\n",
      "157: reward:  17.00, mean_100:  31.40, episodes: 5\n",
      "171: reward:  14.00, mean_100:  28.50, episodes: 6\n",
      "209: reward:  38.00, mean_100:  29.86, episodes: 7\n",
      "torch.Size([85, 2])\n",
      "tensor([[-0.8257, -0.5762],\n",
      "        [-0.7913, -0.6038],\n",
      "        [-0.7550, -0.6349],\n",
      "        [-0.7892, -0.6055],\n",
      "        [-0.7521, -0.6375],\n",
      "        [-0.7164, -0.6704],\n",
      "        [-0.6949, -0.6914],\n",
      "        [-0.7100, -0.6766],\n",
      "        [-0.7387, -0.6496],\n",
      "        [-0.7749, -0.6176],\n",
      "        [-0.7309, -0.6567],\n",
      "        [-0.7667, -0.6246],\n",
      "        [-0.7236, -0.6636],\n",
      "        [-0.7571, -0.6330],\n",
      "        [-0.7137, -0.6730],\n",
      "        [-0.7442, -0.6446],\n",
      "        [-0.7034, -0.6830],\n",
      "        [-0.8226, -0.5785],\n",
      "        [-0.7897, -0.6051],\n",
      "        [-0.7526, -0.6370],\n",
      "        [-0.7876, -0.6068],\n",
      "        [-0.7498, -0.6395],\n",
      "        [-0.7162, -0.6706],\n",
      "        [-0.6955, -0.6908],\n",
      "        [-0.7097, -0.6769],\n",
      "        [-0.7362, -0.6518],\n",
      "        [-0.7037, -0.6827],\n",
      "        [-0.6845, -0.7019],\n",
      "        [-0.6959, -0.6904],\n",
      "        [-0.7178, -0.6691],\n",
      "        [-0.6901, -0.6962],\n",
      "        [-0.8252, -0.5766],\n",
      "        [-0.8760, -0.5386],\n",
      "        [-0.9405, -0.4950],\n",
      "        [-1.0127, -0.4514],\n",
      "        [-0.9503, -0.4888],\n",
      "        [-0.8944, -0.5257],\n",
      "        [-0.8439, -0.5621],\n",
      "        [-0.9043, -0.5189],\n",
      "        [-0.8517, -0.5563],\n",
      "        [-0.9166, -0.5106],\n",
      "        [-0.9804, -0.4703],\n",
      "        [-0.9338, -0.4993],\n",
      "        [-0.8797, -0.5360],\n",
      "        [-0.8337, -0.5699],\n",
      "        [-0.8959, -0.5247],\n",
      "        [-0.9611, -0.4820],\n",
      "        [-1.0317, -0.4407],\n",
      "        [-0.9802, -0.4704],\n",
      "        [-0.9440, -0.4928],\n",
      "        [-0.8954, -0.5250],\n",
      "        [-0.8469, -0.5599],\n",
      "        [-0.8177, -0.5824],\n",
      "        [-0.7870, -0.6074],\n",
      "        [-0.7685, -0.6231],\n",
      "        [-0.7473, -0.6418],\n",
      "        [-0.7252, -0.6620],\n",
      "        [-0.7023, -0.6841],\n",
      "        [-0.6829, -0.7035],\n",
      "        [-0.7011, -0.6852],\n",
      "        [-0.6783, -0.7082],\n",
      "        [-0.6675, -0.7195],\n",
      "        [-0.6611, -0.7263],\n",
      "        [-0.6614, -0.7259],\n",
      "        [-0.6639, -0.7233],\n",
      "        [-0.6739, -0.7128],\n",
      "        [-0.6900, -0.6963],\n",
      "        [-0.6670, -0.7200],\n",
      "        [-0.6773, -0.7093],\n",
      "        [-0.8227, -0.5785],\n",
      "        [-0.8711, -0.5421],\n",
      "        [-0.8233, -0.5780],\n",
      "        [-0.7899, -0.6049],\n",
      "        [-0.7545, -0.6353],\n",
      "        [-0.7878, -0.6067],\n",
      "        [-0.8206, -0.5801],\n",
      "        [-0.7862, -0.6080],\n",
      "        [-0.7500, -0.6394],\n",
      "        [-0.7836, -0.6102],\n",
      "        [-0.7460, -0.6430],\n",
      "        [-0.7157, -0.6711],\n",
      "        [-0.6962, -0.6901],\n",
      "        [-0.6901, -0.6963],\n",
      "        [-0.6862, -0.7001],\n",
      "        [-0.6800, -0.7065]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([85])\n",
      "85\n",
      "torch.Size([85])\n",
      "225: reward:  16.00, mean_100:  28.12, episodes: 8\n",
      "244: reward:  19.00, mean_100:  27.11, episodes: 9\n",
      "258: reward:  14.00, mean_100:  25.80, episodes: 10\n",
      "274: reward:  16.00, mean_100:  24.91, episodes: 11\n",
      "torch.Size([75, 2])\n",
      "tensor([[-1.0014, -0.4578],\n",
      "        [-0.9592, -0.4832],\n",
      "        [-0.9988, -0.4594],\n",
      "        [-0.9571, -0.4845],\n",
      "        [-0.9955, -0.4613],\n",
      "        [-0.9545, -0.4861],\n",
      "        [-0.9915, -0.4637],\n",
      "        [-0.9510, -0.4884],\n",
      "        [-0.9864, -0.4667],\n",
      "        [-0.9470, -0.4909],\n",
      "        [-0.9114, -0.5141],\n",
      "        [-0.9409, -0.4947],\n",
      "        [-0.9042, -0.5190],\n",
      "        [-0.8926, -0.5269],\n",
      "        [-0.8993, -0.5223],\n",
      "        [-0.8835, -0.5333],\n",
      "        [-0.8828, -0.5338],\n",
      "        [-0.8757, -0.5389],\n",
      "        [-0.8985, -0.5229],\n",
      "        [-1.0028, -0.4570],\n",
      "        [-1.0759, -0.4170],\n",
      "        [-1.0081, -0.4540],\n",
      "        [-0.9675, -0.4781],\n",
      "        [-0.9359, -0.4980],\n",
      "        [-0.9203, -0.5082],\n",
      "        [-0.9202, -0.5082],\n",
      "        [-0.9337, -0.4994],\n",
      "        [-0.9598, -0.4829],\n",
      "        [-0.9221, -0.5070],\n",
      "        [-0.8949, -0.5253],\n",
      "        [-0.9168, -0.5105],\n",
      "        [-0.9481, -0.4902],\n",
      "        [-0.9116, -0.5139],\n",
      "        [-1.0043, -0.4562],\n",
      "        [-0.9646, -0.4799],\n",
      "        [-0.9356, -0.4982],\n",
      "        [-0.9620, -0.4815],\n",
      "        [-1.0016, -0.4578],\n",
      "        [-0.9603, -0.4825],\n",
      "        [-0.9297, -0.5020],\n",
      "        [-0.9180, -0.5097],\n",
      "        [-0.9214, -0.5074],\n",
      "        [-0.9069, -0.5172],\n",
      "        [-0.9099, -0.5151],\n",
      "        [-0.8980, -0.5232],\n",
      "        [-0.9056, -0.5180],\n",
      "        [-0.9350, -0.4985],\n",
      "        [-0.9002, -0.5217],\n",
      "        [-0.9301, -0.5018],\n",
      "        [-1.0223, -0.4459],\n",
      "        [-0.9813, -0.4697],\n",
      "        [-0.9515, -0.4881],\n",
      "        [-0.9814, -0.4697],\n",
      "        [-1.0282, -0.4426],\n",
      "        [-1.1104, -0.3996],\n",
      "        [-1.0351, -0.4388],\n",
      "        [-0.9886, -0.4654],\n",
      "        [-0.9572, -0.4844],\n",
      "        [-0.9894, -0.4649],\n",
      "        [-1.0427, -0.4346],\n",
      "        [-1.1264, -0.3919],\n",
      "        [-1.0504, -0.4305],\n",
      "        [-0.9968, -0.4606],\n",
      "        [-1.0570, -0.4269],\n",
      "        [-1.0013, -0.4579],\n",
      "        [-0.9645, -0.4799],\n",
      "        [-0.9478, -0.4904],\n",
      "        [-0.9446, -0.4924],\n",
      "        [-0.9457, -0.4917],\n",
      "        [-0.9346, -0.4988],\n",
      "        [-0.9350, -0.4986],\n",
      "        [-0.9463, -0.4913],\n",
      "        [-0.9748, -0.4736],\n",
      "        [-1.0029, -0.4570],\n",
      "        [-1.0328, -0.4401]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([75])\n",
      "75\n",
      "torch.Size([75])\n",
      "300: reward:  26.00, mean_100:  25.00, episodes: 12\n",
      "324: reward:  24.00, mean_100:  24.92, episodes: 13\n",
      "348: reward:  24.00, mean_100:  24.86, episodes: 14\n",
      "358: reward:  10.00, mean_100:  23.87, episodes: 15\n",
      "torch.Size([72, 2])\n",
      "tensor([[-1.1706, -0.3713],\n",
      "        [-1.2846, -0.3240],\n",
      "        [-1.4340, -0.2723],\n",
      "        [-1.2940, -0.3205],\n",
      "        [-1.4459, -0.2686],\n",
      "        [-1.3082, -0.3151],\n",
      "        [-1.1961, -0.3601],\n",
      "        [-1.1236, -0.3932],\n",
      "        [-1.2041, -0.3566],\n",
      "        [-1.1292, -0.3905],\n",
      "        [-1.2117, -0.3534],\n",
      "        [-1.1350, -0.3878],\n",
      "        [-1.0868, -0.4114],\n",
      "        [-1.0695, -0.4203],\n",
      "        [-1.0843, -0.4127],\n",
      "        [-1.0635, -0.4235],\n",
      "        [-1.0796, -0.4151],\n",
      "        [-1.0567, -0.4271],\n",
      "        [-1.0636, -0.4234],\n",
      "        [-1.0805, -0.4146],\n",
      "        [-1.1085, -0.4006],\n",
      "        [-1.1478, -0.3817],\n",
      "        [-1.0945, -0.4075],\n",
      "        [-1.1366, -0.3870],\n",
      "        [-1.1620, -0.3752],\n",
      "        [-1.0998, -0.4049],\n",
      "        [-1.0586, -0.4261],\n",
      "        [-1.0997, -0.4049],\n",
      "        [-1.0572, -0.4268],\n",
      "        [-1.0979, -0.4058],\n",
      "        [-1.1660, -0.3734],\n",
      "        [-1.2738, -0.3282],\n",
      "        [-1.1684, -0.3723],\n",
      "        [-1.1000, -0.4048],\n",
      "        [-1.0559, -0.4275],\n",
      "        [-1.0971, -0.4062],\n",
      "        [-1.0515, -0.4299],\n",
      "        [-1.0371, -0.4377],\n",
      "        [-1.0426, -0.4347],\n",
      "        [-1.0270, -0.4433],\n",
      "        [-1.0315, -0.4408],\n",
      "        [-1.0723, -0.4189],\n",
      "        [-1.0205, -0.4470],\n",
      "        [-1.0070, -0.4546],\n",
      "        [-1.0197, -0.4474],\n",
      "        [-1.0553, -0.4278],\n",
      "        [-1.0058, -0.4553],\n",
      "        [-1.0477, -0.4319],\n",
      "        [-1.1544, -0.3787],\n",
      "        [-1.0913, -0.4092],\n",
      "        [-1.0521, -0.4295],\n",
      "        [-1.0468, -0.4324],\n",
      "        [-1.0602, -0.4253],\n",
      "        [-1.0868, -0.4114],\n",
      "        [-1.1217, -0.3941],\n",
      "        [-1.1664, -0.3732],\n",
      "        [-1.2080, -0.3550],\n",
      "        [-1.2455, -0.3394],\n",
      "        [-1.1736, -0.3700],\n",
      "        [-1.2902, -0.3219],\n",
      "        [-1.1811, -0.3666],\n",
      "        [-1.1115, -0.3991],\n",
      "        [-1.0658, -0.4223],\n",
      "        [-1.0521, -0.4296],\n",
      "        [-1.0602, -0.4253],\n",
      "        [-1.0813, -0.4142],\n",
      "        [-1.1111, -0.3993],\n",
      "        [-1.1511, -0.3802],\n",
      "        [-1.0967, -0.4064],\n",
      "        [-1.0446, -0.4336],\n",
      "        [-1.0021, -0.4574],\n",
      "        [-0.9791, -0.4710]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([72])\n",
      "72\n",
      "torch.Size([72])\n",
      "372: reward:  14.00, mean_100:  23.25, episodes: 16\n",
      "390: reward:  18.00, mean_100:  22.94, episodes: 17\n",
      "401: reward:  11.00, mean_100:  22.28, episodes: 18\n",
      "411: reward:  10.00, mean_100:  21.63, episodes: 19\n",
      "torch.Size([55, 2])\n",
      "tensor([[-1.2266, -0.3471],\n",
      "        [-1.3873, -0.2873],\n",
      "        [-1.5953, -0.2267],\n",
      "        [-1.4004, -0.2830],\n",
      "        [-1.6134, -0.2222],\n",
      "        [-1.4206, -0.2765],\n",
      "        [-1.2644, -0.3319],\n",
      "        [-1.1558, -0.3781],\n",
      "        [-1.0828, -0.4135],\n",
      "        [-1.0560, -0.4275],\n",
      "        [-1.0571, -0.4269],\n",
      "        [-1.0664, -0.4220],\n",
      "        [-1.0777, -0.4161],\n",
      "        [-1.0470, -0.4323],\n",
      "        [-1.0571, -0.4269],\n",
      "        [-1.0231, -0.4455],\n",
      "        [-1.0361, -0.4382],\n",
      "        [-1.0612, -0.4247],\n",
      "        [-1.2148, -0.3520],\n",
      "        [-1.1088, -0.4004],\n",
      "        [-1.0324, -0.4403],\n",
      "        [-1.0086, -0.4537],\n",
      "        [-1.0188, -0.4479],\n",
      "        [-1.0816, -0.4141],\n",
      "        [-1.0060, -0.4552],\n",
      "        [-0.9845, -0.4678],\n",
      "        [-0.9844, -0.4678],\n",
      "        [-0.9997, -0.4589],\n",
      "        [-0.9644, -0.4800],\n",
      "        [-1.2409, -0.3413],\n",
      "        [-1.1422, -0.3844],\n",
      "        [-1.0737, -0.4182],\n",
      "        [-1.0518, -0.4297],\n",
      "        [-1.0526, -0.4293],\n",
      "        [-1.0634, -0.4235],\n",
      "        [-1.0739, -0.4181],\n",
      "        [-1.0908, -0.4094],\n",
      "        [-1.1159, -0.3969],\n",
      "        [-1.1385, -0.3861],\n",
      "        [-1.2352, -0.3436],\n",
      "        [-1.1316, -0.3894],\n",
      "        [-1.0616, -0.4245],\n",
      "        [-1.1261, -0.3920],\n",
      "        [-1.2269, -0.3470],\n",
      "        [-1.1211, -0.3944],\n",
      "        [-1.0507, -0.4303],\n",
      "        [-1.0280, -0.4428],\n",
      "        [-1.0378, -0.4373],\n",
      "        [-1.0985, -0.4055],\n",
      "        [-1.0243, -0.4448],\n",
      "        [-1.0036, -0.4566],\n",
      "        [-1.0062, -0.4551],\n",
      "        [-1.0158, -0.4496],\n",
      "        [-0.9796, -0.4707],\n",
      "        [-0.9983, -0.4596]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([55])\n",
      "55\n",
      "torch.Size([55])\n",
      "427: reward:  16.00, mean_100:  21.35, episodes: 20\n",
      "441: reward:  14.00, mean_100:  21.00, episodes: 21\n",
      "455: reward:  14.00, mean_100:  20.68, episodes: 22\n",
      "467: reward:  12.00, mean_100:  20.30, episodes: 23\n",
      "torch.Size([59, 2])\n",
      "tensor([[-1.3318, -0.3065],\n",
      "        [-1.2049, -0.3563],\n",
      "        [-1.1176, -0.3961],\n",
      "        [-1.2028, -0.3572],\n",
      "        [-1.1131, -0.3983],\n",
      "        [-1.0859, -0.4119],\n",
      "        [-1.1023, -0.4036],\n",
      "        [-1.0727, -0.4187],\n",
      "        [-1.0872, -0.4112],\n",
      "        [-1.0574, -0.4267],\n",
      "        [-1.0668, -0.4218],\n",
      "        [-1.0839, -0.4129],\n",
      "        [-1.1095, -0.4001],\n",
      "        [-1.1444, -0.3833],\n",
      "        [-1.3146, -0.3128],\n",
      "        [-1.4964, -0.2535],\n",
      "        [-1.3052, -0.3163],\n",
      "        [-1.1792, -0.3675],\n",
      "        [-1.0886, -0.4105],\n",
      "        [-1.0675, -0.4214],\n",
      "        [-1.0788, -0.4155],\n",
      "        [-1.0481, -0.4317],\n",
      "        [-1.0492, -0.4311],\n",
      "        [-1.0295, -0.4419],\n",
      "        [-1.0273, -0.4431],\n",
      "        [-1.0087, -0.4537],\n",
      "        [-1.0017, -0.4577],\n",
      "        [-0.9855, -0.4672],\n",
      "        [-1.3114, -0.3139],\n",
      "        [-1.1878, -0.3637],\n",
      "        [-1.0988, -0.4054],\n",
      "        [-1.0729, -0.4186],\n",
      "        [-1.0894, -0.4101],\n",
      "        [-1.1733, -0.3701],\n",
      "        [-1.0784, -0.4157],\n",
      "        [-1.0496, -0.4309],\n",
      "        [-1.0584, -0.4262],\n",
      "        [-1.0781, -0.4159],\n",
      "        [-1.1040, -0.4028],\n",
      "        [-1.1384, -0.3861],\n",
      "        [-1.2968, -0.3194],\n",
      "        [-1.1804, -0.3670],\n",
      "        [-1.2946, -0.3202],\n",
      "        [-1.1778, -0.3681],\n",
      "        [-1.2911, -0.3216],\n",
      "        [-1.1738, -0.3699],\n",
      "        [-1.0953, -0.4071],\n",
      "        [-1.1656, -0.3736],\n",
      "        [-1.2729, -0.3285],\n",
      "        [-1.1564, -0.3778],\n",
      "        [-1.0771, -0.4164],\n",
      "        [-1.0656, -0.4224],\n",
      "        [-1.0776, -0.4161],\n",
      "        [-1.0968, -0.4064],\n",
      "        [-1.0492, -0.4311],\n",
      "        [-1.0137, -0.4508],\n",
      "        [-1.0022, -0.4574],\n",
      "        [-1.0400, -0.4361],\n",
      "        [-0.9756, -0.4731]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([59])\n",
      "59\n",
      "torch.Size([59])\n",
      "486: reward:  19.00, mean_100:  20.25, episodes: 24\n",
      "496: reward:  10.00, mean_100:  19.84, episodes: 25\n",
      "511: reward:  15.00, mean_100:  19.65, episodes: 26\n",
      "520: reward:   9.00, mean_100:  19.26, episodes: 27\n",
      "torch.Size([64, 2])\n",
      "tensor([[-1.3278, -0.3080],\n",
      "        [-1.1759, -0.3690],\n",
      "        [-1.0689, -0.4206],\n",
      "        [-1.0428, -0.4346],\n",
      "        [-1.0483, -0.4316],\n",
      "        [-1.0220, -0.4461],\n",
      "        [-1.0152, -0.4499],\n",
      "        [-1.0154, -0.4498],\n",
      "        [-0.9791, -0.4711],\n",
      "        [-0.9830, -0.4687],\n",
      "        [-1.3867, -0.2876],\n",
      "        [-1.2315, -0.3451],\n",
      "        [-1.1155, -0.3971],\n",
      "        [-1.0734, -0.4183],\n",
      "        [-1.1109, -0.3994],\n",
      "        [-1.0633, -0.4236],\n",
      "        [-1.0533, -0.4289],\n",
      "        [-1.0445, -0.4337],\n",
      "        [-1.0323, -0.4403],\n",
      "        [-1.0306, -0.4413],\n",
      "        [-1.0012, -0.4580],\n",
      "        [-0.9835, -0.4684],\n",
      "        [-0.9708, -0.4761],\n",
      "        [-0.9508, -0.4884],\n",
      "        [-0.9591, -0.4833],\n",
      "        [-1.3131, -0.3133],\n",
      "        [-1.1673, -0.3728],\n",
      "        [-1.0642, -0.4231],\n",
      "        [-1.0414, -0.4353],\n",
      "        [-1.0383, -0.4370],\n",
      "        [-1.0370, -0.4378],\n",
      "        [-1.0450, -0.4334],\n",
      "        [-1.0024, -0.4573],\n",
      "        [-0.9665, -0.4787],\n",
      "        [-1.3455, -0.3017],\n",
      "        [-1.1975, -0.3594],\n",
      "        [-1.3525, -0.2992],\n",
      "        [-1.2014, -0.3578],\n",
      "        [-1.3574, -0.2975],\n",
      "        [-1.5819, -0.2301],\n",
      "        [-1.8908, -0.1636],\n",
      "        [-1.5998, -0.2256],\n",
      "        [-1.3821, -0.2891],\n",
      "        [-1.2281, -0.3465],\n",
      "        [-1.3941, -0.2851],\n",
      "        [-1.6354, -0.2167],\n",
      "        [-1.9575, -0.1522],\n",
      "        [-2.3347, -0.1019],\n",
      "        [-1.9987, -0.1456],\n",
      "        [-1.7160, -0.1982],\n",
      "        [-1.5013, -0.2521],\n",
      "        [-1.3273, -0.3082],\n",
      "        [-1.2072, -0.3553],\n",
      "        [-1.1519, -0.3798],\n",
      "        [-1.1453, -0.3829],\n",
      "        [-1.1532, -0.3792],\n",
      "        [-1.1426, -0.3842],\n",
      "        [-1.1372, -0.3867],\n",
      "        [-1.1451, -0.3830],\n",
      "        [-1.1426, -0.3842],\n",
      "        [-1.1493, -0.3811],\n",
      "        [-1.1564, -0.3778],\n",
      "        [-1.1682, -0.3724],\n",
      "        [-1.1785, -0.3678]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([64])\n",
      "64\n",
      "torch.Size([64])\n",
      "550: reward:  30.00, mean_100:  19.64, episodes: 28\n",
      "566: reward:  16.00, mean_100:  19.52, episodes: 29\n",
      "580: reward:  14.00, mean_100:  19.33, episodes: 30\n",
      "601: reward:  21.00, mean_100:  19.39, episodes: 31\n",
      "torch.Size([69, 2])\n",
      "tensor([[-1.2895, -0.3222],\n",
      "        [-1.5144, -0.2484],\n",
      "        [-1.2764, -0.3272],\n",
      "        [-1.1109, -0.3994],\n",
      "        [-1.2605, -0.3334],\n",
      "        [-1.4777, -0.2590],\n",
      "        [-1.2464, -0.3390],\n",
      "        [-1.0870, -0.4113],\n",
      "        [-0.9717, -0.4755],\n",
      "        [-1.0688, -0.4207],\n",
      "        [-0.9558, -0.4853],\n",
      "        [-0.9143, -0.5121],\n",
      "        [-0.8936, -0.5263],\n",
      "        [-0.8816, -0.5346],\n",
      "        [-0.8760, -0.5386],\n",
      "        [-0.8434, -0.5626],\n",
      "        [-1.3137, -0.3131],\n",
      "        [-1.1391, -0.3858],\n",
      "        [-1.3021, -0.3174],\n",
      "        [-1.1297, -0.3903],\n",
      "        [-1.0002, -0.4586],\n",
      "        [-0.9507, -0.4885],\n",
      "        [-0.9806, -0.4701],\n",
      "        [-0.9298, -0.5019],\n",
      "        [-0.9554, -0.4856],\n",
      "        [-1.0569, -0.4270],\n",
      "        [-0.9318, -0.5006],\n",
      "        [-0.8801, -0.5357],\n",
      "        [-0.8588, -0.5510],\n",
      "        [-0.8464, -0.5603],\n",
      "        [-1.3139, -0.3130],\n",
      "        [-1.1441, -0.3835],\n",
      "        [-1.0214, -0.4464],\n",
      "        [-1.1415, -0.3847],\n",
      "        [-1.0153, -0.4499],\n",
      "        [-0.9671, -0.4783],\n",
      "        [-1.0013, -0.4579],\n",
      "        [-0.9510, -0.4883],\n",
      "        [-0.9807, -0.4701],\n",
      "        [-0.9298, -0.5019],\n",
      "        [-0.9549, -0.4859],\n",
      "        [-1.0548, -0.4281],\n",
      "        [-1.1770, -0.3685],\n",
      "        [-1.0172, -0.4488],\n",
      "        [-1.1362, -0.3872],\n",
      "        [-0.9769, -0.4724],\n",
      "        [-1.0923, -0.4086],\n",
      "        [-0.9346, -0.4988],\n",
      "        [-0.8507, -0.5571],\n",
      "        [-0.8867, -0.5310],\n",
      "        [-0.9833, -0.4685],\n",
      "        [-1.2973, -0.3192],\n",
      "        [-1.1315, -0.3894],\n",
      "        [-1.0081, -0.4540],\n",
      "        [-0.9592, -0.4832],\n",
      "        [-0.9389, -0.4961],\n",
      "        [-0.9365, -0.4976],\n",
      "        [-0.9139, -0.5124],\n",
      "        [-0.9064, -0.5175],\n",
      "        [-0.9345, -0.4989],\n",
      "        [-1.0337, -0.4396],\n",
      "        [-1.1509, -0.3803],\n",
      "        [-1.2987, -0.3187],\n",
      "        [-1.1065, -0.4015],\n",
      "        [-1.2391, -0.3420],\n",
      "        [-1.0602, -0.4253],\n",
      "        [-0.8983, -0.5230],\n",
      "        [-0.8236, -0.5778],\n",
      "        [-0.8514, -0.5565]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([69])\n",
      "69\n",
      "torch.Size([69])\n",
      "619: reward:  18.00, mean_100:  19.34, episodes: 32\n",
      "642: reward:  23.00, mean_100:  19.45, episodes: 33\n",
      "653: reward:  11.00, mean_100:  19.21, episodes: 34\n",
      "679: reward:  26.00, mean_100:  19.40, episodes: 35\n",
      "torch.Size([81, 2])\n",
      "tensor([[-1.2286, -0.3463],\n",
      "        [-1.0363, -0.4382],\n",
      "        [-1.2283, -0.3464],\n",
      "        [-1.4941, -0.2542],\n",
      "        [-1.2304, -0.3456],\n",
      "        [-1.4978, -0.2531],\n",
      "        [-1.8595, -0.1693],\n",
      "        [-1.5104, -0.2495],\n",
      "        [-1.2497, -0.3377],\n",
      "        [-1.0561, -0.4274],\n",
      "        [-1.2589, -0.3340],\n",
      "        [-1.0623, -0.4241],\n",
      "        [-1.2655, -0.3314],\n",
      "        [-1.0665, -0.4219],\n",
      "        [-0.9134, -0.5127],\n",
      "        [-0.8357, -0.5684],\n",
      "        [-0.7876, -0.6068],\n",
      "        [-0.8167, -0.5832],\n",
      "        [-0.7657, -0.6255],\n",
      "        [-0.7226, -0.6645],\n",
      "        [-0.6847, -0.7017],\n",
      "        [-0.6482, -0.7402],\n",
      "        [-0.6387, -0.7508],\n",
      "        [-1.2417, -0.3409],\n",
      "        [-1.0426, -0.4347],\n",
      "        [-0.8965, -0.5242],\n",
      "        [-1.0291, -0.4421],\n",
      "        [-0.8821, -0.5343],\n",
      "        [-0.8080, -0.5902],\n",
      "        [-0.7592, -0.6312],\n",
      "        [-0.7782, -0.6148],\n",
      "        [-0.7267, -0.6607],\n",
      "        [-0.6835, -0.7029],\n",
      "        [-0.6453, -0.7434],\n",
      "        [-1.2220, -0.3490],\n",
      "        [-1.0304, -0.4414],\n",
      "        [-1.2141, -0.3524],\n",
      "        [-1.0228, -0.4456],\n",
      "        [-1.2032, -0.3570],\n",
      "        [-1.4474, -0.2681],\n",
      "        [-1.1936, -0.3612],\n",
      "        [-1.0059, -0.4553],\n",
      "        [-1.1808, -0.3668],\n",
      "        [-0.9941, -0.4621],\n",
      "        [-0.8569, -0.5525],\n",
      "        [-0.9767, -0.4725],\n",
      "        [-1.1396, -0.3856],\n",
      "        [-0.9550, -0.4858],\n",
      "        [-1.1119, -0.3989],\n",
      "        [-1.3197, -0.3109],\n",
      "        [-1.6153, -0.2217],\n",
      "        [-1.2903, -0.3218],\n",
      "        [-1.0622, -0.4242],\n",
      "        [-0.8882, -0.5300],\n",
      "        [-0.7791, -0.6140],\n",
      "        [-0.7195, -0.6674],\n",
      "        [-0.7480, -0.6411],\n",
      "        [-0.6886, -0.6977],\n",
      "        [-0.6465, -0.7420],\n",
      "        [-0.6480, -0.7404],\n",
      "        [-1.2624, -0.3326],\n",
      "        [-1.0651, -0.4226],\n",
      "        [-0.9129, -0.5131],\n",
      "        [-1.0659, -0.4222],\n",
      "        [-1.2675, -0.3307],\n",
      "        [-1.0652, -0.4226],\n",
      "        [-1.2653, -0.3315],\n",
      "        [-1.0627, -0.4239],\n",
      "        [-0.9056, -0.5180],\n",
      "        [-1.0543, -0.4284],\n",
      "        [-0.8957, -0.5248],\n",
      "        [-1.0408, -0.4357],\n",
      "        [-1.2231, -0.3486],\n",
      "        [-1.0249, -0.4445],\n",
      "        [-0.8672, -0.5449],\n",
      "        [-0.7848, -0.6092],\n",
      "        [-0.7341, -0.6538],\n",
      "        [-0.7537, -0.6361],\n",
      "        [-0.7033, -0.6830],\n",
      "        [-0.6624, -0.7249],\n",
      "        [-0.6638, -0.7234]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([81])\n",
      "81\n",
      "torch.Size([81])\n",
      "700: reward:  21.00, mean_100:  19.44, episodes: 36\n",
      "720: reward:  20.00, mean_100:  19.46, episodes: 37\n",
      "736: reward:  16.00, mean_100:  19.37, episodes: 38\n",
      "748: reward:  12.00, mean_100:  19.18, episodes: 39\n",
      "torch.Size([94, 2])\n",
      "tensor([[-1.1775, -0.3682],\n",
      "        [-0.9599, -0.4828],\n",
      "        [-0.7894, -0.6053],\n",
      "        [-0.6910, -0.6953],\n",
      "        [-0.7904, -0.6045],\n",
      "        [-0.9698, -0.4767],\n",
      "        [-1.1974, -0.3595],\n",
      "        [-0.9677, -0.4779],\n",
      "        [-0.7851, -0.6090],\n",
      "        [-0.6741, -0.7125],\n",
      "        [-0.6014, -0.7942],\n",
      "        [-0.5448, -0.8674],\n",
      "        [-0.5731, -0.8296],\n",
      "        [-0.6133, -0.7799],\n",
      "        [-0.5428, -0.8702],\n",
      "        [-0.5767, -0.8250],\n",
      "        [-0.6436, -0.7453],\n",
      "        [-0.7682, -0.6233],\n",
      "        [-0.5929, -0.8046],\n",
      "        [-0.6922, -0.6941],\n",
      "        [-1.1414, -0.3847],\n",
      "        [-1.4198, -0.2768],\n",
      "        [-1.1231, -0.3935],\n",
      "        [-0.9045, -0.5188],\n",
      "        [-0.7419, -0.6467],\n",
      "        [-0.8840, -0.5329],\n",
      "        [-0.7220, -0.6651],\n",
      "        [-0.8573, -0.5522],\n",
      "        [-0.6986, -0.6877],\n",
      "        [-0.8227, -0.5784],\n",
      "        [-0.6701, -0.7168],\n",
      "        [-0.5816, -0.8187],\n",
      "        [-0.5179, -0.9058],\n",
      "        [-0.5427, -0.8703],\n",
      "        [-0.5821, -0.8181],\n",
      "        [-0.6524, -0.7357],\n",
      "        [-1.1634, -0.3746],\n",
      "        [-0.9345, -0.4989],\n",
      "        [-0.7595, -0.6309],\n",
      "        [-0.6622, -0.7251],\n",
      "        [-0.5930, -0.8045],\n",
      "        [-0.6314, -0.7589],\n",
      "        [-0.5606, -0.8460],\n",
      "        [-0.5940, -0.8032],\n",
      "        [-0.6503, -0.7379],\n",
      "        [-0.7579, -0.6323],\n",
      "        [-0.6036, -0.7916],\n",
      "        [-0.6880, -0.6983],\n",
      "        [-1.1576, -0.3772],\n",
      "        [-0.9437, -0.4930],\n",
      "        [-1.1774, -0.3683],\n",
      "        [-1.4880, -0.2560],\n",
      "        [-1.2005, -0.3582],\n",
      "        [-1.5196, -0.2469],\n",
      "        [-1.2293, -0.3460],\n",
      "        [-1.0047, -0.4559],\n",
      "        [-1.2568, -0.3349],\n",
      "        [-1.0267, -0.4434],\n",
      "        [-0.8477, -0.5593],\n",
      "        [-1.0438, -0.4340],\n",
      "        [-1.3015, -0.3176],\n",
      "        [-1.0612, -0.4247],\n",
      "        [-0.8706, -0.5425],\n",
      "        [-0.7588, -0.6316],\n",
      "        [-0.8733, -0.5405],\n",
      "        [-0.7549, -0.6350],\n",
      "        [-0.8702, -0.5428],\n",
      "        [-1.0649, -0.4228],\n",
      "        [-1.3093, -0.3147],\n",
      "        [-1.0586, -0.4261],\n",
      "        [-0.8578, -0.5518],\n",
      "        [-1.0484, -0.4316],\n",
      "        [-1.2869, -0.3231],\n",
      "        [-1.0385, -0.4369],\n",
      "        [-0.8396, -0.5654],\n",
      "        [-1.0242, -0.4448],\n",
      "        [-0.8265, -0.5755],\n",
      "        [-1.0051, -0.4557],\n",
      "        [-1.2282, -0.3464],\n",
      "        [-0.9860, -0.4669],\n",
      "        [-1.2050, -0.3562],\n",
      "        [-0.9666, -0.4786],\n",
      "        [-1.1812, -0.3666],\n",
      "        [-0.9459, -0.4916],\n",
      "        [-0.7614, -0.6292],\n",
      "        [-0.6435, -0.7454],\n",
      "        [-0.7362, -0.6519],\n",
      "        [-0.8866, -0.5312],\n",
      "        [-0.7038, -0.6826],\n",
      "        [-0.5914, -0.8064],\n",
      "        [-0.6623, -0.7250],\n",
      "        [-0.5590, -0.8481],\n",
      "        [-0.4956, -0.9396],\n",
      "        [-0.5199, -0.9028]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([94])\n",
      "94\n",
      "torch.Size([94])\n",
      "794: reward:  46.00, mean_100:  19.85, episodes: 40\n",
      "829: reward:  35.00, mean_100:  20.22, episodes: 41\n",
      "841: reward:  12.00, mean_100:  20.02, episodes: 42\n",
      "874: reward:  33.00, mean_100:  20.33, episodes: 43\n",
      "torch.Size([95, 2])\n",
      "tensor([[-1.0421, -0.4350],\n",
      "        [-0.8051, -0.5925],\n",
      "        [-0.6353, -0.7545],\n",
      "        [-0.5332, -0.8836],\n",
      "        [-0.6288, -0.7619],\n",
      "        [-0.5234, -0.8978],\n",
      "        [-0.6154, -0.7775],\n",
      "        [-0.7815, -0.6119],\n",
      "        [-1.0015, -0.4578],\n",
      "        [-1.2897, -0.3221],\n",
      "        [-1.6871, -0.2046],\n",
      "        [-1.2664, -0.3311],\n",
      "        [-0.9607, -0.4823],\n",
      "        [-0.7320, -0.6558],\n",
      "        [-0.9410, -0.4947],\n",
      "        [-0.7152, -0.6715],\n",
      "        [-0.5528, -0.8565],\n",
      "        [-0.6920, -0.6943],\n",
      "        [-0.5311, -0.8867],\n",
      "        [-0.6607, -0.7267],\n",
      "        [-0.5046, -0.9257],\n",
      "        [-0.6198, -0.7723],\n",
      "        [-0.7878, -0.6067],\n",
      "        [-0.9917, -0.4635],\n",
      "        [-1.2899, -0.3220],\n",
      "        [-1.6794, -0.2064],\n",
      "        [-1.2163, -0.3514],\n",
      "        [-0.8771, -0.5378],\n",
      "        [-0.6468, -0.7417],\n",
      "        [-0.8233, -0.5780],\n",
      "        [-0.6016, -0.7939],\n",
      "        [-0.4531, -1.0097],\n",
      "        [-0.3603, -1.1955],\n",
      "        [-0.4163, -1.0772],\n",
      "        [-0.4922, -0.9450],\n",
      "        [-1.0286, -0.4424],\n",
      "        [-0.7849, -0.6091],\n",
      "        [-0.6070, -0.7875],\n",
      "        [-0.7660, -0.6252],\n",
      "        [-0.5887, -0.8098],\n",
      "        [-0.4834, -0.9589],\n",
      "        [-0.5620, -0.8442],\n",
      "        [-0.4578, -1.0015],\n",
      "        [-0.3812, -1.1490],\n",
      "        [-0.4234, -1.0637],\n",
      "        [-0.3495, -1.2208],\n",
      "        [-0.2866, -1.3895],\n",
      "        [-1.0614, -0.4246],\n",
      "        [-0.8127, -0.5864],\n",
      "        [-1.0578, -0.4265],\n",
      "        [-1.3882, -0.2870],\n",
      "        [-1.0572, -0.4268],\n",
      "        [-0.8096, -0.5888],\n",
      "        [-0.6264, -0.7647],\n",
      "        [-0.8025, -0.5945],\n",
      "        [-1.0394, -0.4364],\n",
      "        [-0.7925, -0.6028],\n",
      "        [-1.0239, -0.4450],\n",
      "        [-0.7798, -0.6134],\n",
      "        [-0.5985, -0.7977],\n",
      "        [-0.4881, -0.9515],\n",
      "        [-0.5772, -0.8243],\n",
      "        [-0.7303, -0.6573],\n",
      "        [-0.5518, -0.8578],\n",
      "        [-0.6928, -0.6935],\n",
      "        [-0.8725, -0.5411],\n",
      "        [-0.6502, -0.7380],\n",
      "        [-0.8220, -0.5791],\n",
      "        [-1.0454, -0.4331],\n",
      "        [-1.3652, -0.2948],\n",
      "        [-0.9783, -0.4715],\n",
      "        [-0.7239, -0.6633],\n",
      "        [-0.5292, -0.8893],\n",
      "        [-0.6711, -0.7157],\n",
      "        [-0.8419, -0.5637],\n",
      "        [-0.6157, -0.7771],\n",
      "        [-0.4549, -1.0065],\n",
      "        [-0.5559, -0.8523],\n",
      "        [-0.6950, -0.6913],\n",
      "        [-0.4972, -0.9372],\n",
      "        [-1.0506, -0.4304],\n",
      "        [-0.7963, -0.5996],\n",
      "        [-1.0216, -0.4463],\n",
      "        [-1.3321, -0.3064],\n",
      "        [-0.9948, -0.4617],\n",
      "        [-0.7552, -0.6347],\n",
      "        [-0.5823, -0.8178],\n",
      "        [-0.4735, -0.9751],\n",
      "        [-0.5563, -0.8517],\n",
      "        [-0.6902, -0.6961],\n",
      "        [-0.5256, -0.8945],\n",
      "        [-0.4236, -1.0634],\n",
      "        [-0.3487, -1.2229],\n",
      "        [-0.3873, -1.1359],\n",
      "        [-0.4373, -1.0379]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([95])\n",
      "95\n",
      "torch.Size([95])\n",
      "889: reward:  15.00, mean_100:  20.20, episodes: 44\n",
      "919: reward:  30.00, mean_100:  20.42, episodes: 45\n",
      "971: reward:  52.00, mean_100:  21.11, episodes: 46\n",
      "1003: reward:  32.00, mean_100:  21.34, episodes: 47\n",
      "torch.Size([153, 2])\n",
      "tensor([[-0.9831, -0.4686],\n",
      "        [-0.7153, -0.6715],\n",
      "        [-0.5248, -0.8957],\n",
      "        [-0.4090, -1.0915],\n",
      "        [-0.5174, -0.9065],\n",
      "        [-0.3988, -1.1121],\n",
      "        [-0.5021, -0.9295],\n",
      "        [-0.6779, -0.7086],\n",
      "        [-0.4829, -0.9596],\n",
      "        [-0.6494, -0.7389],\n",
      "        [-0.8718, -0.5417],\n",
      "        [-1.1773, -0.3683],\n",
      "        [-0.8301, -0.5727],\n",
      "        [-0.5867, -0.8123],\n",
      "        [-0.7886, -0.6060],\n",
      "        [-1.0693, -0.4205],\n",
      "        [-1.4605, -0.2641],\n",
      "        [-1.0199, -0.4473],\n",
      "        [-0.7142, -0.6726],\n",
      "        [-0.4964, -0.9383],\n",
      "        [-0.3633, -1.1887],\n",
      "        [-0.2757, -1.4233],\n",
      "        [-0.3335, -1.2602],\n",
      "        [-0.4189, -1.0722],\n",
      "        [-0.5585, -0.8488],\n",
      "        [-0.3761, -1.1601],\n",
      "        [-0.4916, -0.9458],\n",
      "        [-0.6421, -0.7469],\n",
      "        [-0.8517, -0.5563],\n",
      "        [-0.5645, -0.8409],\n",
      "        [-0.9883, -0.4656],\n",
      "        [-0.7160, -0.6708],\n",
      "        [-0.9901, -0.4645],\n",
      "        [-0.7167, -0.6701],\n",
      "        [-0.9897, -0.4647],\n",
      "        [-1.3574, -0.2975],\n",
      "        [-0.9927, -0.4629],\n",
      "        [-1.3647, -0.2950],\n",
      "        [-1.0008, -0.4582],\n",
      "        [-0.7275, -0.6599],\n",
      "        [-1.0077, -0.4542],\n",
      "        [-1.3859, -0.2878],\n",
      "        [-1.0188, -0.4479],\n",
      "        [-0.7431, -0.6456],\n",
      "        [-1.0292, -0.4420],\n",
      "        [-1.4155, -0.2781],\n",
      "        [-1.9154, -0.1593],\n",
      "        [-1.4471, -0.2682],\n",
      "        [-1.0819, -0.4139],\n",
      "        [-0.7947, -0.6010],\n",
      "        [-1.1131, -0.3983],\n",
      "        [-0.8180, -0.5822],\n",
      "        [-0.5982, -0.7980],\n",
      "        [-0.8384, -0.5664],\n",
      "        [-0.6106, -0.7831],\n",
      "        [-0.8536, -0.5549],\n",
      "        [-1.1803, -0.3670],\n",
      "        [-0.8706, -0.5425],\n",
      "        [-1.2014, -0.3578],\n",
      "        [-0.8901, -0.5287],\n",
      "        [-0.6466, -0.7419],\n",
      "        [-0.4944, -0.9415],\n",
      "        [-0.6542, -0.7336],\n",
      "        [-0.4931, -0.9434],\n",
      "        [-0.3873, -1.1360],\n",
      "        [-0.3064, -1.3322],\n",
      "        [-0.2419, -1.5378],\n",
      "        [-0.2869, -1.3887],\n",
      "        [-0.3408, -1.2421],\n",
      "        [-0.4170, -1.0760],\n",
      "        [-0.5410, -0.8726],\n",
      "        [-0.7165, -0.6704],\n",
      "        [-0.9401, -0.4952],\n",
      "        [-0.6548, -0.7330],\n",
      "        [-0.8594, -0.5507],\n",
      "        [-0.5937, -0.8036],\n",
      "        [-0.7788, -0.6143],\n",
      "        [-1.0272, -0.4432],\n",
      "        [-0.7059, -0.6806],\n",
      "        [-0.4712, -0.9789],\n",
      "        [-0.3233, -1.2863],\n",
      "        [-0.2371, -1.5556],\n",
      "        [-0.8885, -0.5298],\n",
      "        [-0.6363, -0.7534],\n",
      "        [-0.4642, -0.9906],\n",
      "        [-0.6190, -0.7732],\n",
      "        [-0.8394, -0.5656],\n",
      "        [-1.1493, -0.3811],\n",
      "        [-0.8122, -0.5868],\n",
      "        [-1.1143, -0.3977],\n",
      "        [-0.7861, -0.6081],\n",
      "        [-0.5553, -0.8530],\n",
      "        [-0.7560, -0.6340],\n",
      "        [-1.0363, -0.4382],\n",
      "        [-1.4243, -0.2753],\n",
      "        [-0.9993, -0.4591],\n",
      "        [-1.3796, -0.2899],\n",
      "        [-1.8657, -0.1682],\n",
      "        [-1.3496, -0.3002],\n",
      "        [-0.9510, -0.4883],\n",
      "        [-1.3263, -0.3085],\n",
      "        [-0.9342, -0.4990],\n",
      "        [-0.6594, -0.7281],\n",
      "        [-0.4738, -0.9746],\n",
      "        [-0.6392, -0.7501],\n",
      "        [-0.4573, -1.0023],\n",
      "        [-0.6126, -0.7808],\n",
      "        [-0.4360, -1.0402],\n",
      "        [-0.3220, -1.2900],\n",
      "        [-0.4057, -1.0981],\n",
      "        [-0.5309, -0.8869],\n",
      "        [-0.3725, -1.1681],\n",
      "        [-0.2719, -1.4351],\n",
      "        [-0.2026, -1.6963],\n",
      "        [-0.9182, -0.5096],\n",
      "        [-1.2699, -0.3297],\n",
      "        [-0.9282, -0.5029],\n",
      "        [-0.6720, -0.7147],\n",
      "        [-0.4942, -0.9417],\n",
      "        [-0.6726, -0.7141],\n",
      "        [-0.9295, -0.5021],\n",
      "        [-0.6689, -0.7180],\n",
      "        [-0.9219, -0.5071],\n",
      "        [-0.6625, -0.7248],\n",
      "        [-0.4801, -0.9642],\n",
      "        [-0.6498, -0.7385],\n",
      "        [-0.4671, -0.9856],\n",
      "        [-0.3550, -1.2078],\n",
      "        [-0.4452, -1.0237],\n",
      "        [-0.5973, -0.7992],\n",
      "        [-0.7996, -0.5969],\n",
      "        [-1.0795, -0.4151],\n",
      "        [-0.7543, -0.6355],\n",
      "        [-1.0201, -0.4472],\n",
      "        [-0.7125, -0.6741],\n",
      "        [-0.4901, -0.9482],\n",
      "        [-0.6678, -0.7192],\n",
      "        [-0.4546, -1.0070],\n",
      "        [-0.3284, -1.2732],\n",
      "        [-0.4134, -1.0829],\n",
      "        [-0.5535, -0.8555],\n",
      "        [-0.7281, -0.6594],\n",
      "        [-0.4920, -0.9452],\n",
      "        [-0.6463, -0.7423],\n",
      "        [-0.8665, -0.5455],\n",
      "        [-1.1778, -0.3681],\n",
      "        [-0.7605, -0.6300],\n",
      "        [-1.0393, -0.4365],\n",
      "        [-1.4023, -0.2824],\n",
      "        [-0.9170, -0.5103],\n",
      "        [-0.6006, -0.7951],\n",
      "        [-0.8086, -0.5896],\n",
      "        [-0.5272, -0.8923]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([153])\n",
      "153\n",
      "torch.Size([153])\n",
      "1042: reward:  39.00, mean_100:  21.71, episodes: 48\n",
      "1064: reward:  22.00, mean_100:  21.71, episodes: 49\n",
      "1088: reward:  24.00, mean_100:  21.76, episodes: 50\n",
      "1134: reward:  46.00, mean_100:  22.24, episodes: 51\n",
      "torch.Size([176, 2])\n",
      "tensor([[-0.7941, -0.6015],\n",
      "        [-0.5307, -0.8871],\n",
      "        [-0.3633, -1.1887],\n",
      "        [-0.2590, -1.4775],\n",
      "        [-0.3469, -1.2272],\n",
      "        [-0.4873, -0.9526],\n",
      "        [-0.6991, -0.6873],\n",
      "        [-1.0039, -0.4564],\n",
      "        [-0.6575, -0.7301],\n",
      "        [-0.9479, -0.4903],\n",
      "        [-0.6185, -0.7738],\n",
      "        [-0.3986, -1.1125],\n",
      "        [-0.2710, -1.4379],\n",
      "        [-0.3661, -1.1824],\n",
      "        [-0.5278, -0.8913],\n",
      "        [-0.3315, -1.2652],\n",
      "        [-0.4706, -0.9799],\n",
      "        [-0.2952, -1.3642],\n",
      "        [-0.4108, -1.0881],\n",
      "        [-0.5718, -0.8312],\n",
      "        [-0.3531, -1.2124],\n",
      "        [-0.2273, -1.5929],\n",
      "        [-0.8150, -0.5845],\n",
      "        [-0.5512, -0.8587],\n",
      "        [-0.3768, -1.1586],\n",
      "        [-0.2691, -1.4441],\n",
      "        [-0.3708, -1.1717],\n",
      "        [-0.5429, -0.8700],\n",
      "        [-0.7945, -0.6011],\n",
      "        [-1.1497, -0.3809],\n",
      "        [-0.7760, -0.6167],\n",
      "        [-0.5167, -0.9076],\n",
      "        [-0.3426, -1.2376],\n",
      "        [-0.2412, -1.5402],\n",
      "        [-0.3239, -1.2848],\n",
      "        [-0.2262, -1.5973],\n",
      "        [-0.2987, -1.3541],\n",
      "        [-0.4211, -1.0681],\n",
      "        [-0.6037, -0.7914],\n",
      "        [-0.3757, -1.1610],\n",
      "        [-0.2447, -1.5276],\n",
      "        [-0.3286, -1.2727],\n",
      "        [-0.4689, -0.9827],\n",
      "        [-0.2859, -1.3916],\n",
      "        [-0.4003, -1.1090],\n",
      "        [-0.5510, -0.8589],\n",
      "        [-0.8732, -0.5406],\n",
      "        [-1.2604, -0.3334],\n",
      "        [-1.7694, -0.1868],\n",
      "        [-1.2602, -0.3335],\n",
      "        [-1.7785, -0.1850],\n",
      "        [-1.2754, -0.3276],\n",
      "        [-0.8913, -0.5278],\n",
      "        [-1.2982, -0.3189],\n",
      "        [-0.9134, -0.5128],\n",
      "        [-0.6233, -0.7682],\n",
      "        [-0.4316, -1.0484],\n",
      "        [-0.3164, -1.3047],\n",
      "        [-0.4298, -1.0516],\n",
      "        [-0.6275, -0.7634],\n",
      "        [-0.9179, -0.5097],\n",
      "        [-1.3190, -0.3112],\n",
      "        [-0.9142, -0.5122],\n",
      "        [-0.6194, -0.7727],\n",
      "        [-0.9088, -0.5158],\n",
      "        [-0.6152, -0.7777],\n",
      "        [-0.9013, -0.5209],\n",
      "        [-0.6093, -0.7847],\n",
      "        [-0.8917, -0.5276],\n",
      "        [-0.6016, -0.7939],\n",
      "        [-0.4063, -1.0970],\n",
      "        [-0.5881, -0.8105],\n",
      "        [-0.8549, -0.5539],\n",
      "        [-0.5711, -0.8322],\n",
      "        [-0.8299, -0.5729],\n",
      "        [-1.1946, -0.3607],\n",
      "        [-0.8075, -0.5906],\n",
      "        [-0.5381, -0.8767],\n",
      "        [-0.3637, -1.1879],\n",
      "        [-0.2571, -1.4839],\n",
      "        [-0.3437, -1.2349],\n",
      "        [-0.2400, -1.5449],\n",
      "        [-0.3163, -1.3051],\n",
      "        [-0.2191, -1.6260],\n",
      "        [-0.2838, -1.3981],\n",
      "        [-0.3845, -1.1420],\n",
      "        [-0.5473, -0.8639],\n",
      "        [-0.7539, -0.6358],\n",
      "        [-0.4761, -0.9707],\n",
      "        [-0.6573, -0.7303],\n",
      "        [-0.4092, -1.0912],\n",
      "        [-0.2556, -1.4892],\n",
      "        [-0.8028, -0.5943],\n",
      "        [-1.1732, -0.3701],\n",
      "        [-0.8068, -0.5911],\n",
      "        [-0.5439, -0.8686],\n",
      "        [-0.3794, -1.1529],\n",
      "        [-0.5404, -0.8735],\n",
      "        [-0.7948, -0.6009],\n",
      "        [-0.5321, -0.8852],\n",
      "        [-0.7801, -0.6132],\n",
      "        [-0.5207, -0.9016],\n",
      "        [-0.7616, -0.6291],\n",
      "        [-1.1002, -0.4047],\n",
      "        [-0.7431, -0.6456],\n",
      "        [-1.0781, -0.4159],\n",
      "        [-0.7272, -0.6602],\n",
      "        [-1.0582, -0.4263],\n",
      "        [-1.5193, -0.2470],\n",
      "        [-1.0473, -0.4321],\n",
      "        [-0.7059, -0.6806],\n",
      "        [-0.4690, -0.9825],\n",
      "        [-0.6940, -0.6923],\n",
      "        [-1.0155, -0.4498],\n",
      "        [-0.6805, -0.7059],\n",
      "        [-0.9973, -0.4603],\n",
      "        [-0.6670, -0.7200],\n",
      "        [-0.4407, -1.0316],\n",
      "        [-0.3090, -1.3250],\n",
      "        [-0.4234, -1.0637],\n",
      "        [-0.2932, -1.3699],\n",
      "        [-0.3988, -1.1121],\n",
      "        [-0.5772, -0.8244],\n",
      "        [-0.8250, -0.5766],\n",
      "        [-1.1870, -0.3641],\n",
      "        [-1.6706, -0.2084],\n",
      "        [-1.1239, -0.3930],\n",
      "        [-1.5987, -0.2259],\n",
      "        [-2.1193, -0.1280],\n",
      "        [-1.5476, -0.2392],\n",
      "        [-1.0406, -0.4358],\n",
      "        [-0.6740, -0.7126],\n",
      "        [-1.0091, -0.4534],\n",
      "        [-0.6519, -0.7361],\n",
      "        [-0.9760, -0.4729],\n",
      "        [-1.4258, -0.2749],\n",
      "        [-0.9469, -0.4909],\n",
      "        [-1.3916, -0.2859],\n",
      "        [-0.9234, -0.5061],\n",
      "        [-1.3642, -0.2952],\n",
      "        [-0.9039, -0.5191],\n",
      "        [-1.3417, -0.3030],\n",
      "        [-0.8878, -0.5303],\n",
      "        [-0.5824, -0.8178],\n",
      "        [-0.8680, -0.5444],\n",
      "        [-0.5690, -0.8350],\n",
      "        [-0.3812, -1.1489],\n",
      "        [-0.2624, -1.4662],\n",
      "        [-0.3613, -1.1932],\n",
      "        [-0.2447, -1.5277],\n",
      "        [-0.3334, -1.2605],\n",
      "        [-0.4631, -0.9925],\n",
      "        [-0.6653, -0.7218],\n",
      "        [-0.9633, -0.4807],\n",
      "        [-1.3815, -0.2893],\n",
      "        [-1.8753, -0.1664],\n",
      "        [-1.2749, -0.3278],\n",
      "        [-1.7584, -0.1891],\n",
      "        [-1.1858, -0.3645],\n",
      "        [-0.7468, -0.6423],\n",
      "        [-1.1029, -0.4033],\n",
      "        [-0.6884, -0.6979],\n",
      "        [-1.0181, -0.4483],\n",
      "        [-1.4465, -0.2684],\n",
      "        [-1.9377, -0.1555],\n",
      "        [-1.3655, -0.2947],\n",
      "        [-1.8358, -0.1737],\n",
      "        [-1.2887, -0.3225],\n",
      "        [-0.8132, -0.5860],\n",
      "        [-0.4983, -0.9354],\n",
      "        [-0.3215, -1.2912],\n",
      "        [-0.4492, -1.0164],\n",
      "        [-0.2917, -1.3744],\n",
      "        [-0.3972, -1.1153],\n",
      "        [-0.5762, -0.8255],\n",
      "        [-0.8416, -0.5639]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([176])\n",
      "176\n",
      "torch.Size([176])\n",
      "1218: reward:  84.00, mean_100:  23.42, episodes: 52\n",
      "1309: reward:  91.00, mean_100:  24.70, episodes: 53\n",
      "1431: reward: 122.00, mean_100:  26.50, episodes: 54\n",
      "1475: reward:  44.00, mean_100:  26.82, episodes: 55\n",
      "torch.Size([315, 2])\n",
      "tensor([[-0.7667, -0.6247],\n",
      "        [-1.1621, -0.3752],\n",
      "        [-1.6902, -0.2039],\n",
      "        [-1.1677, -0.3726],\n",
      "        [-0.7744, -0.6180],\n",
      "        [-0.4911, -0.9467],\n",
      "        [-0.7816, -0.6119],\n",
      "        [-0.4954, -0.9399],\n",
      "        [-0.3180, -1.3006],\n",
      "        [-0.4938, -0.9423],\n",
      "        [-0.7764, -0.6163],\n",
      "        [-0.4888, -0.9503],\n",
      "        [-0.7669, -0.6245],\n",
      "        [-0.4818, -0.9614],\n",
      "        [-0.7545, -0.6353],\n",
      "        [-1.1428, -0.3841],\n",
      "        [-1.6648, -0.2098],\n",
      "        [-1.1427, -0.3841],\n",
      "        [-1.6750, -0.2074],\n",
      "        [-1.1586, -0.3767],\n",
      "        [-0.7729, -0.6193],\n",
      "        [-0.4920, -0.9452],\n",
      "        [-0.7883, -0.6063],\n",
      "        [-1.2002, -0.3583],\n",
      "        [-0.8081, -0.5900],\n",
      "        [-0.5162, -0.9083],\n",
      "        [-0.3329, -1.2619],\n",
      "        [-0.5243, -0.8964],\n",
      "        [-0.8327, -0.5707],\n",
      "        [-1.2494, -0.3378],\n",
      "        [-0.8464, -0.5603],\n",
      "        [-0.5416, -0.8718],\n",
      "        [-0.8601, -0.5501],\n",
      "        [-0.5515, -0.8582],\n",
      "        [-0.3507, -1.2181],\n",
      "        [-0.2371, -1.5555],\n",
      "        [-0.3471, -1.2267],\n",
      "        [-0.5462, -0.8654],\n",
      "        [-0.8494, -0.5580],\n",
      "        [-0.5368, -0.8786],\n",
      "        [-0.3336, -1.2599],\n",
      "        [-0.2204, -1.6204],\n",
      "        [-0.3201, -1.2950],\n",
      "        [-0.4962, -0.9386],\n",
      "        [-0.7582, -0.6321],\n",
      "        [-0.4700, -0.9809],\n",
      "        [-0.7204, -0.6666],\n",
      "        [-1.0690, -0.4206],\n",
      "        [-1.5596, -0.2360],\n",
      "        [-1.0324, -0.4403],\n",
      "        [-0.6625, -0.7248],\n",
      "        [-1.0008, -0.4582],\n",
      "        [-0.6411, -0.7481],\n",
      "        [-0.3954, -1.1190],\n",
      "        [-0.6173, -0.7753],\n",
      "        [-0.3786, -1.1546],\n",
      "        [-0.5896, -0.8087],\n",
      "        [-0.8972, -0.5238],\n",
      "        [-1.3517, -0.2995],\n",
      "        [-0.8668, -0.5452],\n",
      "        [-0.5427, -0.8703],\n",
      "        [-0.3283, -1.2734],\n",
      "        [-0.5184, -0.9051],\n",
      "        [-0.7975, -0.5987],\n",
      "        [-1.2115, -0.3535],\n",
      "        [-0.7648, -0.6263],\n",
      "        [-0.4727, -0.9764],\n",
      "        [-0.7313, -0.6564],\n",
      "        [-0.4500, -1.0151],\n",
      "        [-0.2675, -1.4494],\n",
      "        [-0.4222, -1.0659],\n",
      "        [-0.6455, -0.7432],\n",
      "        [-0.3898, -1.1308],\n",
      "        [-0.2312, -1.5780],\n",
      "        [-0.1468, -1.9915],\n",
      "        [-0.2070, -1.6767],\n",
      "        [-0.3078, -1.3284],\n",
      "        [-0.4641, -0.9907],\n",
      "        [-0.7042, -0.6823],\n",
      "        [-1.0602, -0.4252],\n",
      "        [-0.6080, -0.7862],\n",
      "        [-0.3510, -1.2174],\n",
      "        [-0.5213, -0.9007],\n",
      "        [-0.8039, -0.5935],\n",
      "        [-0.4485, -1.0177],\n",
      "        [-0.6845, -0.7018],\n",
      "        [-0.3837, -1.1437],\n",
      "        [-0.5798, -0.8210],\n",
      "        [-0.3222, -1.2894],\n",
      "        [-0.4855, -0.9555],\n",
      "        [-0.2681, -1.4476],\n",
      "        [-0.7226, -0.6646],\n",
      "        [-1.1152, -0.3973],\n",
      "        [-1.6620, -0.2104],\n",
      "        [-1.1366, -0.3870],\n",
      "        [-0.7517, -0.6378],\n",
      "        [-0.4784, -0.9670],\n",
      "        [-0.3062, -1.3328],\n",
      "        [-0.4859, -0.9549],\n",
      "        [-0.7734, -0.6189],\n",
      "        [-0.4897, -0.9489],\n",
      "        [-0.7771, -0.6157],\n",
      "        [-0.4915, -0.9461],\n",
      "        [-0.7781, -0.6148],\n",
      "        [-0.4913, -0.9463],\n",
      "        [-0.3082, -1.3271],\n",
      "        [-0.2018, -1.6996],\n",
      "        [-0.2992, -1.3524],\n",
      "        [-0.4674, -0.9852],\n",
      "        [-0.7247, -0.6626],\n",
      "        [-0.4479, -1.0188],\n",
      "        [-0.6922, -0.6940],\n",
      "        [-1.0457, -0.4330],\n",
      "        [-1.5504, -0.2385],\n",
      "        [-2.1035, -0.1301],\n",
      "        [-1.5314, -0.2436],\n",
      "        [-2.0995, -0.1307],\n",
      "        [-1.5356, -0.2425],\n",
      "        [-1.0236, -0.4452],\n",
      "        [-1.5566, -0.2368],\n",
      "        [-1.0482, -0.4316],\n",
      "        [-0.6843, -0.7021],\n",
      "        [-1.0764, -0.4168],\n",
      "        [-0.7068, -0.6797],\n",
      "        [-0.4479, -1.0188],\n",
      "        [-0.7262, -0.6612],\n",
      "        [-1.1314, -0.3895],\n",
      "        [-0.7480, -0.6412],\n",
      "        [-0.4759, -0.9711],\n",
      "        [-0.7670, -0.6243],\n",
      "        [-0.4885, -0.9507],\n",
      "        [-0.3133, -1.3132],\n",
      "        [-0.4951, -0.9403],\n",
      "        [-0.7856, -0.6085],\n",
      "        [-1.2032, -0.3570],\n",
      "        [-0.7951, -0.6007],\n",
      "        [-1.2185, -0.3505],\n",
      "        [-0.8106, -0.5881],\n",
      "        [-1.2411, -0.3412],\n",
      "        [-1.8133, -0.1781],\n",
      "        [-1.2804, -0.3256],\n",
      "        [-0.8762, -0.5385],\n",
      "        [-0.5651, -0.8400],\n",
      "        [-0.9130, -0.5130],\n",
      "        [-0.5919, -0.8058],\n",
      "        [-0.3796, -1.1525],\n",
      "        [-0.6147, -0.7783],\n",
      "        [-0.9756, -0.4731],\n",
      "        [-0.6373, -0.7523],\n",
      "        [-1.0070, -0.4546],\n",
      "        [-1.4961, -0.2536],\n",
      "        [-2.0752, -0.1341],\n",
      "        [-1.5644, -0.2347],\n",
      "        [-1.1248, -0.3926],\n",
      "        [-0.7729, -0.6193],\n",
      "        [-0.5089, -0.9192],\n",
      "        [-0.8318, -0.5714],\n",
      "        [-0.5516, -0.8581],\n",
      "        [-0.8916, -0.5276],\n",
      "        [-0.5963, -0.8004],\n",
      "        [-0.3995, -1.1106],\n",
      "        [-0.6373, -0.7523],\n",
      "        [-0.9993, -0.4591],\n",
      "        [-1.4556, -0.2656],\n",
      "        [-1.0653, -0.4226],\n",
      "        [-0.7381, -0.6501],\n",
      "        [-0.4995, -0.9334],\n",
      "        [-0.7953, -0.6005],\n",
      "        [-0.5403, -0.8737],\n",
      "        [-0.8496, -0.5579],\n",
      "        [-1.2669, -0.3309],\n",
      "        [-1.7741, -0.1859],\n",
      "        [-1.3587, -0.2971],\n",
      "        [-1.0111, -0.4523],\n",
      "        [-1.4679, -0.2619],\n",
      "        [-1.1168, -0.3965],\n",
      "        [-0.8007, -0.5960],\n",
      "        [-0.5920, -0.8057],\n",
      "        [-0.4355, -1.0411],\n",
      "        [-0.6518, -0.7363],\n",
      "        [-0.9878, -0.4658],\n",
      "        [-1.4252, -0.2750],\n",
      "        [-1.0944, -0.4076],\n",
      "        [-0.8060, -0.5917],\n",
      "        [-0.6101, -0.7837],\n",
      "        [-0.4563, -1.0041],\n",
      "        [-0.3346, -1.2575],\n",
      "        [-0.4979, -0.9359],\n",
      "        [-0.7250, -0.6623],\n",
      "        [-0.5448, -0.8674],\n",
      "        [-0.3926, -1.1248],\n",
      "        [-0.2831, -1.4001],\n",
      "        [-0.4118, -1.0861],\n",
      "        [-0.6130, -0.7803],\n",
      "        [-0.8651, -0.5465],\n",
      "        [-1.2362, -0.3432],\n",
      "        [-0.9145, -0.5120],\n",
      "        [-0.6802, -0.7063],\n",
      "        [-0.4947, -0.9410],\n",
      "        [-0.3498, -1.2202],\n",
      "        [-0.5172, -0.9068],\n",
      "        [-0.7420, -0.6466],\n",
      "        [-1.0544, -0.4283],\n",
      "        [-1.4651, -0.2627],\n",
      "        [-1.9501, -0.1534],\n",
      "        [-1.5551, -0.2372],\n",
      "        [-1.2161, -0.3515],\n",
      "        [-0.9221, -0.5070],\n",
      "        [-1.3228, -0.3098],\n",
      "        [-1.8006, -0.1806],\n",
      "        [-1.4606, -0.2641],\n",
      "        [-1.9810, -0.1484],\n",
      "        [-1.6301, -0.2180],\n",
      "        [-0.6965, -0.6898],\n",
      "        [-1.0532, -0.4290],\n",
      "        [-1.5574, -0.2366],\n",
      "        [-1.0400, -0.4361],\n",
      "        [-0.6731, -0.7136],\n",
      "        [-1.0323, -0.4404],\n",
      "        [-1.5442, -0.2401],\n",
      "        [-1.0372, -0.4376],\n",
      "        [-0.6745, -0.7121],\n",
      "        [-1.0457, -0.4330],\n",
      "        [-1.5675, -0.2339],\n",
      "        [-2.1492, -0.1240],\n",
      "        [-1.6081, -0.2235],\n",
      "        [-1.1135, -0.3981],\n",
      "        [-0.7485, -0.6407],\n",
      "        [-0.4809, -0.9629],\n",
      "        [-0.3148, -1.3092],\n",
      "        [-0.2147, -1.6438],\n",
      "        [-0.3196, -1.2962],\n",
      "        [-0.2147, -1.6438],\n",
      "        [-0.3177, -1.3012],\n",
      "        [-0.5009, -0.9314],\n",
      "        [-0.3117, -1.3174],\n",
      "        [-0.2039, -1.6902],\n",
      "        [-0.1363, -2.0603],\n",
      "        [-0.1908, -1.7502],\n",
      "        [-0.2756, -1.4235],\n",
      "        [-0.4244, -1.0617],\n",
      "        [-0.2493, -1.5112],\n",
      "        [-0.1583, -1.9211],\n",
      "        [-0.2199, -1.6225],\n",
      "        [-0.3305, -1.2678],\n",
      "        [-0.4997, -0.9332],\n",
      "        [-0.7271, -0.6603],\n",
      "        [-1.0710, -0.4195],\n",
      "        [-0.6377, -0.7518],\n",
      "        [-0.3722, -1.1688],\n",
      "        [-0.5503, -0.8599],\n",
      "        [-0.8309, -0.5721],\n",
      "        [-0.4738, -0.9745],\n",
      "        [-0.2692, -1.4439],\n",
      "        [-0.1582, -1.9222],\n",
      "        [-0.2254, -1.6005],\n",
      "        [-0.3317, -1.2649],\n",
      "        [-0.7489, -0.6404],\n",
      "        [-0.4750, -0.9725],\n",
      "        [-0.3008, -1.3481],\n",
      "        [-0.4791, -0.9659],\n",
      "        [-0.2999, -1.3505],\n",
      "        [-0.4760, -0.9708],\n",
      "        [-0.7498, -0.6395],\n",
      "        [-1.1392, -0.3858],\n",
      "        [-1.6732, -0.2078],\n",
      "        [-1.1426, -0.3842],\n",
      "        [-0.7522, -0.6374],\n",
      "        [-0.4753, -0.9720],\n",
      "        [-0.2985, -1.3544],\n",
      "        [-0.4743, -0.9738],\n",
      "        [-0.7488, -0.6404],\n",
      "        [-0.4696, -0.9816],\n",
      "        [-0.7402, -0.6483],\n",
      "        [-1.1252, -0.3924],\n",
      "        [-0.7340, -0.6539],\n",
      "        [-1.1199, -0.3950],\n",
      "        [-0.7320, -0.6558],\n",
      "        [-0.4594, -0.9987],\n",
      "        [-0.7281, -0.6594],\n",
      "        [-1.1119, -0.3989],\n",
      "        [-1.6472, -0.2139],\n",
      "        [-1.1208, -0.3946],\n",
      "        [-0.7378, -0.6504],\n",
      "        [-0.4663, -0.9870],\n",
      "        [-0.7454, -0.6435],\n",
      "        [-0.4705, -0.9800],\n",
      "        [-0.7495, -0.6398],\n",
      "        [-1.1480, -0.3816],\n",
      "        [-0.7565, -0.6336],\n",
      "        [-1.1606, -0.3759],\n",
      "        [-1.7126, -0.1989],\n",
      "        [-1.1878, -0.3637],\n",
      "        [-1.7539, -0.1901],\n",
      "        [-1.2327, -0.3446],\n",
      "        [-1.8150, -0.1777],\n",
      "        [-1.2983, -0.3188],\n",
      "        [-0.9045, -0.5188],\n",
      "        [-1.3754, -0.2913],\n",
      "        [-1.9777, -0.1490],\n",
      "        [-1.4781, -0.2589],\n",
      "        [-2.0890, -0.1322],\n",
      "        [-1.6077, -0.2236],\n",
      "        [-1.1985, -0.3590],\n",
      "        [-1.7497, -0.1910],\n",
      "        [-1.3414, -0.3031],\n",
      "        [-0.9954, -0.4614],\n",
      "        [-1.4992, -0.2527],\n",
      "        [-1.1458, -0.3827],\n",
      "        [-0.8373, -0.5672],\n",
      "        [-1.3080, -0.3152],\n",
      "        [-0.9893, -0.4649],\n",
      "        [-0.7147, -0.6721],\n",
      "        [-1.1480, -0.3816],\n",
      "        [-1.6325, -0.2175]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([315])\n",
      "315\n",
      "torch.Size([315])\n",
      "1533: reward:  58.00, mean_100:  27.38, episodes: 56\n",
      "1610: reward:  77.00, mean_100:  28.25, episodes: 57\n",
      "1671: reward:  61.00, mean_100:  28.81, episodes: 58\n",
      "1711: reward:  40.00, mean_100:  29.00, episodes: 59\n",
      "torch.Size([219, 2])\n",
      "tensor([[-0.5818, -0.8185],\n",
      "        [-0.3388, -1.2470],\n",
      "        [-0.2016, -1.7006],\n",
      "        [-0.1227, -2.1584],\n",
      "        [-0.1935, -1.7377],\n",
      "        [-0.3195, -1.2966],\n",
      "        [-0.5321, -0.8853],\n",
      "        [-0.3010, -1.3475],\n",
      "        [-0.4999, -0.9330],\n",
      "        [-0.2806, -1.4080],\n",
      "        [-0.4658, -0.9878],\n",
      "        [-0.7493, -0.6400],\n",
      "        [-0.4326, -1.0465],\n",
      "        [-0.7009, -0.6854],\n",
      "        [-1.1134, -0.3981],\n",
      "        [-1.6544, -0.2122],\n",
      "        [-1.0625, -0.4240],\n",
      "        [-0.6308, -0.7597],\n",
      "        [-1.0226, -0.4457],\n",
      "        [-1.5778, -0.2312],\n",
      "        [-2.1590, -0.1227],\n",
      "        [-2.7772, -0.0642],\n",
      "        [-2.1798, -0.1200],\n",
      "        [-1.6070, -0.2237],\n",
      "        [-1.0430, -0.4345],\n",
      "        [-0.6514, -0.7367],\n",
      "        [-0.3851, -1.1406],\n",
      "        [-0.6776, -0.7090],\n",
      "        [-0.4006, -1.1085],\n",
      "        [-0.7007, -0.6856],\n",
      "        [-0.4147, -1.0805],\n",
      "        [-0.2455, -1.5248],\n",
      "        [-0.4236, -1.0634],\n",
      "        [-0.2480, -1.5157],\n",
      "        [-0.4255, -1.0598],\n",
      "        [-0.7233, -0.6638],\n",
      "        [-0.4244, -1.0619],\n",
      "        [-0.2457, -1.5240],\n",
      "        [-0.1491, -1.9766],\n",
      "        [-0.2373, -1.5546],\n",
      "        [-0.1420, -2.0224],\n",
      "        [-0.2230, -1.6098],\n",
      "        [-0.3726, -1.1677],\n",
      "        [-0.6077, -0.7866],\n",
      "        [-0.3440, -1.2342],\n",
      "        [-0.5574, -0.8503],\n",
      "        [-0.9060, -0.5177],\n",
      "        [-0.5107, -0.9165],\n",
      "        [-0.8423, -0.5633],\n",
      "        [-0.4697, -0.9814],\n",
      "        [-0.2611, -1.4705],\n",
      "        [-0.4307, -1.0500],\n",
      "        [-0.7115, -0.6752],\n",
      "        [-1.1389, -0.3859],\n",
      "        [-0.6493, -0.7390],\n",
      "        [-1.0556, -0.4277],\n",
      "        [-1.6111, -0.2227],\n",
      "        [-0.9871, -0.4663],\n",
      "        [-0.5532, -0.8559],\n",
      "        [-0.3049, -1.3363],\n",
      "        [-0.5082, -0.9203],\n",
      "        [-0.8560, -0.5531],\n",
      "        [-1.3610, -0.2962],\n",
      "        [-0.7968, -0.5992],\n",
      "        [-1.2845, -0.3241],\n",
      "        [-0.7446, -0.6442],\n",
      "        [-0.4082, -1.0931],\n",
      "        [-0.2248, -1.6030],\n",
      "        [-0.1298, -2.1061],\n",
      "        [-0.2022, -1.6979],\n",
      "        [-0.3277, -1.2749],\n",
      "        [-0.5458, -0.8660],\n",
      "        [-0.2853, -1.3934],\n",
      "        [-0.1552, -1.9396],\n",
      "        [-0.2418, -1.5381],\n",
      "        [-0.1318, -2.0918],\n",
      "        [-0.0725, -2.6601],\n",
      "        [-0.6531, -0.7349],\n",
      "        [-0.3826, -1.1460],\n",
      "        [-0.6540, -0.7339],\n",
      "        [-1.0522, -0.4295],\n",
      "        [-0.6586, -0.7289],\n",
      "        [-0.3865, -1.1377],\n",
      "        [-0.6614, -0.7259],\n",
      "        [-1.0628, -0.4238],\n",
      "        [-0.6682, -0.7187],\n",
      "        [-0.3931, -1.1239],\n",
      "        [-0.2329, -1.5712],\n",
      "        [-0.3921, -1.1259],\n",
      "        [-0.6652, -0.7219],\n",
      "        [-0.3881, -1.1342],\n",
      "        [-0.2270, -1.5943],\n",
      "        [-0.3793, -1.1530],\n",
      "        [-0.6354, -0.7544],\n",
      "        [-1.0143, -0.4505],\n",
      "        [-0.6209, -0.7711],\n",
      "        [-0.3600, -1.1963],\n",
      "        [-0.6050, -0.7899],\n",
      "        [-0.9721, -0.4753],\n",
      "        [-0.5914, -0.8065],\n",
      "        [-0.9555, -0.4855],\n",
      "        [-0.5812, -0.8192],\n",
      "        [-0.9436, -0.4930],\n",
      "        [-1.4728, -0.2604],\n",
      "        [-0.9454, -0.4919],\n",
      "        [-1.4820, -0.2577],\n",
      "        [-0.9602, -0.4826],\n",
      "        [-0.5928, -0.8047],\n",
      "        [-0.3477, -1.2252],\n",
      "        [-0.6026, -0.7927],\n",
      "        [-0.3531, -1.2124],\n",
      "        [-0.6085, -0.7856],\n",
      "        [-0.9936, -0.4624],\n",
      "        [-1.5435, -0.2403],\n",
      "        [-1.0160, -0.4495],\n",
      "        [-1.5787, -0.2310],\n",
      "        [-2.1871, -0.1191],\n",
      "        [-1.6449, -0.2145],\n",
      "        [-1.1254, -0.3924],\n",
      "        [-0.7460, -0.6430],\n",
      "        [-1.2024, -0.3574],\n",
      "        [-0.8135, -0.5857],\n",
      "        [-1.2953, -0.3200],\n",
      "        [-1.9129, -0.1598],\n",
      "        [-1.4149, -0.2783],\n",
      "        [-1.0040, -0.4564],\n",
      "        [-0.6691, -0.7178],\n",
      "        [-1.1192, -0.3953],\n",
      "        [-0.7697, -0.6220],\n",
      "        [-0.4992, -0.9340],\n",
      "        [-0.8789, -0.5366],\n",
      "        [-1.3739, -0.2918],\n",
      "        [-0.9998, -0.4588],\n",
      "        [-1.5217, -0.2463],\n",
      "        [-1.1446, -0.3832],\n",
      "        [-1.6802, -0.2062],\n",
      "        [-1.3069, -0.3156],\n",
      "        [-1.8522, -0.1707],\n",
      "        [-0.6196, -0.7726],\n",
      "        [-1.0243, -0.4448],\n",
      "        [-1.6086, -0.2234],\n",
      "        [-1.0644, -0.4230],\n",
      "        [-0.6781, -0.7085],\n",
      "        [-0.4057, -1.0981],\n",
      "        [-0.7119, -0.6747],\n",
      "        [-0.4263, -1.0582],\n",
      "        [-0.7444, -0.6444],\n",
      "        [-0.4462, -1.0218],\n",
      "        [-0.7757, -0.6169],\n",
      "        [-1.2401, -0.3416],\n",
      "        [-0.8151, -0.5845],\n",
      "        [-1.2956, -0.3199],\n",
      "        [-0.8626, -0.5483],\n",
      "        [-1.3647, -0.2950],\n",
      "        [-0.9229, -0.5065],\n",
      "        [-1.4474, -0.2681],\n",
      "        [-0.9994, -0.4590],\n",
      "        [-1.5480, -0.2391],\n",
      "        [-2.1747, -0.1206],\n",
      "        [-1.6731, -0.2079],\n",
      "        [-1.2290, -0.3461],\n",
      "        [-0.8621, -0.5487],\n",
      "        [-0.5630, -0.8427],\n",
      "        [-0.3739, -1.1650],\n",
      "        [-0.6537, -0.7342],\n",
      "        [-1.0972, -0.4062],\n",
      "        [-1.6374, -0.2163],\n",
      "        [-1.2428, -0.3405],\n",
      "        [-0.8972, -0.5238],\n",
      "        [-1.3869, -0.2875],\n",
      "        [-1.0486, -0.4314],\n",
      "        [-0.7297, -0.6578],\n",
      "        [-1.2010, -0.3580],\n",
      "        [-0.8799, -0.5359],\n",
      "        [-0.6351, -0.7548],\n",
      "        [-1.0402, -0.4360],\n",
      "        [-0.7537, -0.6361],\n",
      "        [-1.2014, -0.3578],\n",
      "        [-0.5871, -0.8118],\n",
      "        [-0.3425, -1.2379],\n",
      "        [-0.2066, -1.6786],\n",
      "        [-0.1268, -2.1280],\n",
      "        [-0.0781, -2.5889],\n",
      "        [-0.1194, -2.1841],\n",
      "        [-0.0719, -2.6678],\n",
      "        [-0.1083, -2.2768],\n",
      "        [-0.1629, -1.8951],\n",
      "        [-0.2550, -1.4914],\n",
      "        [-0.1416, -2.0249],\n",
      "        [-0.2152, -1.6420],\n",
      "        [-0.3509, -1.2176],\n",
      "        [-0.5396, -0.8747],\n",
      "        [-0.8495, -0.5580],\n",
      "        [-0.4533, -1.0093],\n",
      "        [-0.2403, -1.5436],\n",
      "        [-0.3766, -1.1590],\n",
      "        [-0.5980, -0.7983],\n",
      "        [-0.9415, -0.4944],\n",
      "        [-0.4910, -0.9468],\n",
      "        [-0.7915, -0.6036],\n",
      "        [-0.4051, -1.0993],\n",
      "        [-0.6538, -0.7341],\n",
      "        [-0.3351, -1.2561],\n",
      "        [-0.5362, -0.8794],\n",
      "        [-0.8505, -0.5572],\n",
      "        [-0.4376, -1.0374],\n",
      "        [-0.7029, -0.6834],\n",
      "        [-1.1060, -0.4018],\n",
      "        [-0.5812, -0.8193],\n",
      "        [-0.9255, -0.5047],\n",
      "        [-0.4764, -0.9703],\n",
      "        [-0.7722, -0.6199],\n",
      "        [-1.1828, -0.3659],\n",
      "        [-1.6740, -0.2076],\n",
      "        [-1.0134, -0.4510],\n",
      "        [-0.5335, -0.8832],\n",
      "        [-0.8637, -0.5475],\n",
      "        [-0.4411, -1.0310],\n",
      "        [-0.7199, -0.6671]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([219])\n",
      "219\n",
      "torch.Size([219])\n",
      "1752: reward:  41.00, mean_100:  29.20, episodes: 60\n",
      "1830: reward:  78.00, mean_100:  30.00, episodes: 61\n",
      "1910: reward:  80.00, mean_100:  30.81, episodes: 62\n",
      "1992: reward:  82.00, mean_100:  31.62, episodes: 63\n",
      "torch.Size([269, 2])\n",
      "tensor([[-0.5333, -0.8835],\n",
      "        [-0.2990, -1.3531],\n",
      "        [-0.5577, -0.8498],\n",
      "        [-0.3123, -1.3158],\n",
      "        [-0.5790, -0.8221],\n",
      "        [-0.3244, -1.2835],\n",
      "        [-0.5971, -0.7994],\n",
      "        [-0.3346, -1.2574],\n",
      "        [-0.6123, -0.7811],\n",
      "        [-0.3432, -1.2361],\n",
      "        [-0.6251, -0.7662],\n",
      "        [-0.3506, -1.2183],\n",
      "        [-0.6359, -0.7539],\n",
      "        [-0.3569, -1.2034],\n",
      "        [-0.6451, -0.7437],\n",
      "        [-0.3623, -1.1910],\n",
      "        [-0.6528, -0.7352],\n",
      "        [-0.3669, -1.1805],\n",
      "        [-0.2035, -1.6923],\n",
      "        [-0.3672, -1.1798],\n",
      "        [-0.6536, -0.7344],\n",
      "        [-1.0742, -0.4179],\n",
      "        [-0.6570, -0.7307],\n",
      "        [-0.3692, -1.1754],\n",
      "        [-0.6604, -0.7270],\n",
      "        [-0.3712, -1.1709],\n",
      "        [-0.2044, -1.6883],\n",
      "        [-0.3690, -1.1757],\n",
      "        [-0.6531, -0.7349],\n",
      "        [-0.3650, -1.1849],\n",
      "        [-0.6457, -0.7429],\n",
      "        [-1.0606, -0.4250],\n",
      "        [-0.6447, -0.7441],\n",
      "        [-0.3611, -1.1937],\n",
      "        [-0.6433, -0.7456],\n",
      "        [-1.0609, -0.4249],\n",
      "        [-1.6304, -0.2180],\n",
      "        [-2.2498, -0.1114],\n",
      "        [-1.6796, -0.2063],\n",
      "        [-1.1385, -0.3861],\n",
      "        [-0.7315, -0.6562],\n",
      "        [-0.4304, -1.0506],\n",
      "        [-0.2441, -1.5297],\n",
      "        [-0.4616, -0.9950],\n",
      "        [-0.8213, -0.5796],\n",
      "        [-0.4942, -0.9417],\n",
      "        [-0.2779, -1.4161],\n",
      "        [-0.5246, -0.8959],\n",
      "        [-0.9023, -0.5202],\n",
      "        [-0.5587, -0.8485],\n",
      "        [-0.3172, -1.3026],\n",
      "        [-0.5928, -0.8046],\n",
      "        [-0.3385, -1.2477],\n",
      "        [-0.6261, -0.7650],\n",
      "        [-0.3601, -1.1961],\n",
      "        [-0.6577, -0.7299],\n",
      "        [-0.3818, -1.1478],\n",
      "        [-0.6900, -0.6963],\n",
      "        [-1.1093, -0.4001],\n",
      "        [-1.6828, -0.2056],\n",
      "        [-1.1793, -0.3674],\n",
      "        [-0.7963, -0.5997],\n",
      "        [-0.4935, -0.9429],\n",
      "        [-0.2934, -1.3695],\n",
      "        [-0.5401, -0.8739],\n",
      "        [-0.3220, -1.2899],\n",
      "        [-0.5861, -0.8131],\n",
      "        [-0.3508, -1.2179],\n",
      "        [-0.6322, -0.7581],\n",
      "        [-1.0369, -0.4378],\n",
      "        [-1.5579, -0.2364],\n",
      "        [-1.1243, -0.3928],\n",
      "        [-1.6792, -0.2064],\n",
      "        [-2.3347, -0.1019],\n",
      "        [-1.8416, -0.1726],\n",
      "        [-1.4081, -0.2805],\n",
      "        [-2.0241, -0.1417],\n",
      "        [-1.6069, -0.2238],\n",
      "        [-0.5797, -0.8212],\n",
      "        [-0.3226, -1.2884],\n",
      "        [-0.5874, -0.8114],\n",
      "        [-0.3267, -1.2777],\n",
      "        [-0.5922, -0.8054],\n",
      "        [-0.9924, -0.4631],\n",
      "        [-0.6013, -0.7943],\n",
      "        [-0.3353, -1.2556],\n",
      "        [-0.6087, -0.7854],\n",
      "        [-0.3393, -1.2458],\n",
      "        [-0.1898, -1.7553],\n",
      "        [-0.1094, -2.2672],\n",
      "        [-0.1850, -1.7785],\n",
      "        [-0.3265, -1.2781],\n",
      "        [-0.1767, -1.8202],\n",
      "        [-0.3110, -1.3194],\n",
      "        [-0.5411, -0.8725],\n",
      "        [-0.8891, -0.5294],\n",
      "        [-1.4065, -0.2810],\n",
      "        [-0.8619, -0.5488],\n",
      "        [-1.3856, -0.2879],\n",
      "        [-0.8497, -0.5578],\n",
      "        [-0.4944, -0.9414],\n",
      "        [-0.2705, -1.4398],\n",
      "        [-0.4851, -0.9562],\n",
      "        [-0.2641, -1.4607],\n",
      "        [-0.4716, -0.9782],\n",
      "        [-0.2553, -1.4904],\n",
      "        [-0.4538, -1.0083],\n",
      "        [-0.7679, -0.6236],\n",
      "        [-1.2637, -0.3321],\n",
      "        [-1.8525, -0.1706],\n",
      "        [-2.4624, -0.0891],\n",
      "        [-1.8744, -0.1666],\n",
      "        [-2.5044, -0.0853],\n",
      "        [-1.9360, -0.1558],\n",
      "        [-1.3690, -0.2935],\n",
      "        [-0.8801, -0.5357],\n",
      "        [-1.4582, -0.2648],\n",
      "        [-0.9575, -0.4843],\n",
      "        [-1.5641, -0.2348],\n",
      "        [-1.0542, -0.4284],\n",
      "        [-0.6882, -0.6981],\n",
      "        [-1.1666, -0.3731],\n",
      "        [-0.7813, -0.6122],\n",
      "        [-0.4779, -0.9678],\n",
      "        [-0.8806, -0.5354],\n",
      "        [-0.5571, -0.8507],\n",
      "        [-0.9904, -0.4643],\n",
      "        [-0.6460, -0.7426],\n",
      "        [-0.3941, -1.1218],\n",
      "        [-0.7437, -0.6451],\n",
      "        [-0.4599, -0.9978],\n",
      "        [-0.2876, -1.3864],\n",
      "        [-0.5287, -0.8901],\n",
      "        [-0.3302, -1.2687],\n",
      "        [-0.2114, -1.6580],\n",
      "        [-0.3722, -1.1686],\n",
      "        [-0.6643, -0.7229],\n",
      "        [-0.4181, -1.0739],\n",
      "        [-0.7341, -0.6538],\n",
      "        [-0.4660, -0.9876],\n",
      "        [-0.8109, -0.5878],\n",
      "        [-1.2797, -0.3259],\n",
      "        [-0.9061, -0.5177],\n",
      "        [-0.5875, -0.8113],\n",
      "        [-1.0042, -0.4562],\n",
      "        [-1.5141, -0.2485],\n",
      "        [-1.1247, -0.3927],\n",
      "        [-0.7870, -0.6074],\n",
      "        [-0.5329, -0.8841],\n",
      "        [-0.9088, -0.5159],\n",
      "        [-0.6236, -0.7679],\n",
      "        [-0.4344, -1.0431],\n",
      "        [-0.7275, -0.6599],\n",
      "        [-1.1757, -0.3690],\n",
      "        [-0.8468, -0.5600],\n",
      "        [-1.3408, -0.3033],\n",
      "        [-0.9907, -0.4641],\n",
      "        [-0.7264, -0.6610],\n",
      "        [-0.5388, -0.8758],\n",
      "        [-0.2993, -1.3523],\n",
      "        [-0.5464, -0.8652],\n",
      "        [-0.3031, -1.3415],\n",
      "        [-0.5503, -0.8598],\n",
      "        [-0.9339, -0.4992],\n",
      "        [-0.5567, -0.8512],\n",
      "        [-0.9458, -0.4916],\n",
      "        [-1.5096, -0.2497],\n",
      "        [-0.9723, -0.4751],\n",
      "        [-0.5939, -0.8034],\n",
      "        [-0.3341, -1.2586],\n",
      "        [-0.6151, -0.7778],\n",
      "        [-0.3466, -1.2278],\n",
      "        [-0.6344, -0.7556],\n",
      "        [-1.0622, -0.4242],\n",
      "        [-1.6527, -0.2126],\n",
      "        [-1.1104, -0.3996],\n",
      "        [-0.7060, -0.6805],\n",
      "        [-1.1710, -0.3711],\n",
      "        [-0.7595, -0.6309],\n",
      "        [-1.2467, -0.3389],\n",
      "        [-0.8246, -0.5770],\n",
      "        [-0.5023, -0.9292],\n",
      "        [-0.2896, -1.3806],\n",
      "        [-0.1771, -1.8181],\n",
      "        [-0.1079, -2.2803],\n",
      "        [-0.0657, -2.7548],\n",
      "        [-0.0392, -3.2588],\n",
      "        [-0.0644, -2.7745],\n",
      "        [-0.1044, -2.3111],\n",
      "        [-0.1728, -1.8408],\n",
      "        [-0.2916, -1.3746],\n",
      "        [-0.5113, -0.9156],\n",
      "        [-0.8293, -0.5733],\n",
      "        [-1.2778, -0.3266],\n",
      "        [-0.8012, -0.5957],\n",
      "        [-1.2513, -0.3371],\n",
      "        [-1.8294, -0.1750],\n",
      "        [-2.4350, -0.0917],\n",
      "        [-3.0874, -0.0467],\n",
      "        [-2.5005, -0.0856],\n",
      "        [-1.9465, -0.1541],\n",
      "        [-1.4106, -0.2797],\n",
      "        [-0.9626, -0.4811],\n",
      "        [-0.6259, -0.7653],\n",
      "        [-0.3727, -1.1676],\n",
      "        [-0.2252, -1.6011],\n",
      "        [-0.4183, -1.0735],\n",
      "        [-0.7599, -0.6306],\n",
      "        [-0.4668, -0.9862],\n",
      "        [-0.2780, -1.4158],\n",
      "        [-0.5160, -0.9086],\n",
      "        [-0.8927, -0.5269],\n",
      "        [-0.5703, -0.8332],\n",
      "        [-0.3438, -1.2346],\n",
      "        [-0.6251, -0.7661],\n",
      "        [-1.0364, -0.4381],\n",
      "        [-0.6890, -0.6973],\n",
      "        [-0.4265, -1.0578],\n",
      "        [-0.7587, -0.6316],\n",
      "        [-1.2115, -0.3535],\n",
      "        [-0.8443, -0.5619],\n",
      "        [-0.5456, -0.8663],\n",
      "        [-0.9370, -0.4973],\n",
      "        [-0.6235, -0.7680],\n",
      "        [-0.4063, -1.0970],\n",
      "        [-0.2618, -1.4682],\n",
      "        [-0.4558, -1.0050],\n",
      "        [-0.2956, -1.3629],\n",
      "        [-0.1876, -1.7656],\n",
      "        [-0.3282, -1.2738],\n",
      "        [-0.5544, -0.8543],\n",
      "        [-0.9211, -0.5077],\n",
      "        [-0.6139, -0.7792],\n",
      "        [-1.0000, -0.4587],\n",
      "        [-0.6867, -0.6997],\n",
      "        [-1.0951, -0.4072],\n",
      "        [-1.6283, -0.2185],\n",
      "        [-1.2255, -0.3476],\n",
      "        [-1.8028, -0.1801],\n",
      "        [-2.4478, -0.0905],\n",
      "        [-0.5437, -0.8690],\n",
      "        [-0.2992, -1.3524],\n",
      "        [-0.5396, -0.8746],\n",
      "        [-0.9188, -0.5091],\n",
      "        [-0.5373, -0.8778],\n",
      "        [-0.9184, -0.5094],\n",
      "        [-1.4878, -0.2560],\n",
      "        [-2.0949, -0.1313],\n",
      "        [-1.5258, -0.2452],\n",
      "        [-0.9726, -0.4750],\n",
      "        [-0.5909, -0.8071],\n",
      "        [-1.0179, -0.4484],\n",
      "        [-0.6283, -0.7625],\n",
      "        [-1.0705, -0.4198],\n",
      "        [-1.7013, -0.2014],\n",
      "        [-2.3323, -0.1021],\n",
      "        [-1.7988, -0.1809],\n",
      "        [-1.2624, -0.3326],\n",
      "        [-1.9204, -0.1585],\n",
      "        [-1.4091, -0.2802],\n",
      "        [-2.0672, -0.1353],\n",
      "        [-1.5734, -0.2323],\n",
      "        [-1.1349, -0.3878],\n",
      "        [-0.7803, -0.6129],\n",
      "        [-1.3097, -0.3146],\n",
      "        [-0.9336, -0.4994],\n",
      "        [-1.4851, -0.2568],\n",
      "        [-1.1145, -0.3976],\n",
      "        [-0.7846, -0.6094]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([269])\n",
      "269\n",
      "torch.Size([269])\n",
      "2021: reward:  29.00, mean_100:  31.58, episodes: 64\n",
      "2081: reward:  60.00, mean_100:  32.02, episodes: 65\n",
      "2141: reward:  60.00, mean_100:  32.44, episodes: 66\n",
      "2256: reward: 115.00, mean_100:  33.67, episodes: 67\n",
      "torch.Size([276, 2])\n",
      "tensor([[-0.5287, -0.8900],\n",
      "        [-0.2816, -1.4048],\n",
      "        [-0.5296, -0.8888],\n",
      "        [-0.2815, -1.4052],\n",
      "        [-0.1542, -1.9454],\n",
      "        [-0.0863, -2.4925],\n",
      "        [-0.1468, -1.9913],\n",
      "        [-0.2611, -1.4706],\n",
      "        [-0.4705, -0.9800],\n",
      "        [-0.8008, -0.5960],\n",
      "        [-0.4426, -1.0282],\n",
      "        [-0.7622, -0.6285],\n",
      "        [-1.2588, -0.3341],\n",
      "        [-0.7359, -0.6521],\n",
      "        [-1.2286, -0.3463],\n",
      "        [-1.8195, -0.1769],\n",
      "        [-1.2209, -0.3495],\n",
      "        [-0.7251, -0.6622],\n",
      "        [-1.2270, -0.3470],\n",
      "        [-0.7362, -0.6519],\n",
      "        [-0.4073, -1.0949],\n",
      "        [-0.7474, -0.6416],\n",
      "        [-0.4146, -1.0806],\n",
      "        [-0.2220, -1.6141],\n",
      "        [-0.4170, -1.0759],\n",
      "        [-0.7569, -0.6332],\n",
      "        [-0.4188, -1.0724],\n",
      "        [-0.2229, -1.6103],\n",
      "        [-0.1244, -2.1459],\n",
      "        [-0.2176, -1.6318],\n",
      "        [-0.4008, -1.1080],\n",
      "        [-0.2091, -1.6676],\n",
      "        [-0.3828, -1.1456],\n",
      "        [-0.6795, -0.7070],\n",
      "        [-0.3639, -1.1872],\n",
      "        [-0.6493, -0.7390],\n",
      "        [-0.3454, -1.2309],\n",
      "        [-0.6185, -0.7738],\n",
      "        [-1.0462, -0.4327],\n",
      "        [-1.6322, -0.2175],\n",
      "        [-2.2149, -0.1156],\n",
      "        [-1.6357, -0.2167],\n",
      "        [-2.2323, -0.1135],\n",
      "        [-1.6687, -0.2089],\n",
      "        [-1.0704, -0.4199],\n",
      "        [-0.6447, -0.7440],\n",
      "        [-1.1180, -0.3959],\n",
      "        [-1.7743, -0.1858],\n",
      "        [-2.4018, -0.0949],\n",
      "        [-1.8628, -0.1687],\n",
      "        [-2.5157, -0.0843],\n",
      "        [-1.9856, -0.1477],\n",
      "        [-1.4623, -0.2636],\n",
      "        [-0.9901, -0.4645],\n",
      "        [-1.6238, -0.2196],\n",
      "        [-2.2803, -0.1079],\n",
      "        [-1.8059, -0.1795],\n",
      "        [-1.3436, -0.3023],\n",
      "        [-0.9549, -0.4859],\n",
      "        [-1.5382, -0.2418],\n",
      "        [-0.5100, -0.9176],\n",
      "        [-0.2688, -1.4452],\n",
      "        [-0.4954, -0.9398],\n",
      "        [-0.8558, -0.5532],\n",
      "        [-0.4829, -0.9597],\n",
      "        [-0.2538, -1.4954],\n",
      "        [-0.1374, -2.0531],\n",
      "        [-0.2423, -1.5364],\n",
      "        [-0.4411, -1.0309],\n",
      "        [-0.7602, -0.6303],\n",
      "        [-0.4173, -1.0753],\n",
      "        [-0.2149, -1.6429],\n",
      "        [-0.3915, -1.1272],\n",
      "        [-0.6835, -0.7029],\n",
      "        [-0.3665, -1.1813],\n",
      "        [-0.1862, -1.7726],\n",
      "        [-0.3388, -1.2469],\n",
      "        [-0.5964, -0.8003],\n",
      "        [-0.9993, -0.4591],\n",
      "        [-0.5563, -0.8517],\n",
      "        [-0.9440, -0.4928],\n",
      "        [-1.4989, -0.2528],\n",
      "        [-2.0790, -0.1336],\n",
      "        [-2.6970, -0.0698],\n",
      "        [-2.0905, -0.1320],\n",
      "        [-1.5056, -0.2509],\n",
      "        [-0.9306, -0.5014],\n",
      "        [-1.5465, -0.2395],\n",
      "        [-0.9694, -0.4769],\n",
      "        [-1.6064, -0.2239],\n",
      "        [-1.0256, -0.4440],\n",
      "        [-0.6235, -0.7680],\n",
      "        [-1.0915, -0.4090],\n",
      "        [-1.7593, -0.1889],\n",
      "        [-1.1814, -0.3665],\n",
      "        [-0.7528, -0.6368],\n",
      "        [-1.2874, -0.3229],\n",
      "        [-0.8347, -0.5691],\n",
      "        [-0.4951, -0.9404],\n",
      "        [-0.9232, -0.5062],\n",
      "        [-0.5661, -0.8387],\n",
      "        [-1.0188, -0.4479],\n",
      "        [-0.6442, -0.7446],\n",
      "        [-1.1257, -0.3922],\n",
      "        [-1.7580, -0.1892],\n",
      "        [-1.2638, -0.3321],\n",
      "        [-0.8554, -0.5536],\n",
      "        [-0.5274, -0.8919],\n",
      "        [-0.9807, -0.4701],\n",
      "        [-1.5526, -0.2379],\n",
      "        [-1.1344, -0.3880],\n",
      "        [-0.7609, -0.6297],\n",
      "        [-1.2888, -0.3224],\n",
      "        [-0.9138, -0.5125],\n",
      "        [-1.4498, -0.2674],\n",
      "        [-1.0781, -0.4159],\n",
      "        [-0.7439, -0.6448],\n",
      "        [-0.4951, -0.9403],\n",
      "        [-0.9003, -0.5216],\n",
      "        [-1.4165, -0.2778],\n",
      "        [-0.5262, -0.8937],\n",
      "        [-0.9124, -0.5134],\n",
      "        [-0.5199, -0.9028],\n",
      "        [-0.9064, -0.5175],\n",
      "        [-0.5169, -0.9072],\n",
      "        [-0.9049, -0.5185],\n",
      "        [-0.5170, -0.9071],\n",
      "        [-0.9075, -0.5167],\n",
      "        [-1.4780, -0.2589],\n",
      "        [-0.9239, -0.5058],\n",
      "        [-0.5365, -0.8790],\n",
      "        [-0.2883, -1.3846],\n",
      "        [-0.1616, -1.9025],\n",
      "        [-0.0920, -2.4317],\n",
      "        [-0.1583, -1.9215],\n",
      "        [-0.2823, -1.4026],\n",
      "        [-0.1516, -1.9611],\n",
      "        [-0.2685, -1.4460],\n",
      "        [-0.1423, -2.0202],\n",
      "        [-0.2493, -1.5113],\n",
      "        [-0.4424, -1.0287],\n",
      "        [-0.7627, -0.6281],\n",
      "        [-0.4070, -1.0956],\n",
      "        [-0.7116, -0.6750],\n",
      "        [-1.1803, -0.3670],\n",
      "        [-0.6690, -0.7179],\n",
      "        [-0.3524, -1.2141],\n",
      "        [-0.1809, -1.7987],\n",
      "        [-0.3263, -1.2787],\n",
      "        [-0.5775, -0.8239],\n",
      "        [-0.9806, -0.4702],\n",
      "        [-0.5349, -0.8812],\n",
      "        [-0.2761, -1.4217],\n",
      "        [-0.4918, -0.9455],\n",
      "        [-0.2513, -1.5042],\n",
      "        [-0.4454, -1.0233],\n",
      "        [-0.7814, -0.6121],\n",
      "        [-1.2872, -0.3230],\n",
      "        [-0.7250, -0.6623],\n",
      "        [-0.3710, -1.1712],\n",
      "        [-0.6711, -0.7157],\n",
      "        [-1.1407, -0.3851],\n",
      "        [-0.6231, -0.7685],\n",
      "        [-1.0763, -0.4168],\n",
      "        [-0.5818, -0.8185],\n",
      "        [-1.0194, -0.4475],\n",
      "        [-0.5462, -0.8655],\n",
      "        [-0.9688, -0.4773],\n",
      "        [-0.5142, -0.9112],\n",
      "        [-0.9233, -0.5062],\n",
      "        [-0.4858, -0.9551],\n",
      "        [-0.8800, -0.5358],\n",
      "        [-0.4605, -0.9968],\n",
      "        [-0.8377, -0.5668],\n",
      "        [-0.4358, -1.0405],\n",
      "        [-0.7957, -0.6001],\n",
      "        [-0.4110, -1.0876],\n",
      "        [-0.2096, -1.6656],\n",
      "        [-0.3817, -1.1480],\n",
      "        [-0.1930, -1.7401],\n",
      "        [-0.3467, -1.2277],\n",
      "        [-0.6262, -0.7649],\n",
      "        [-1.0671, -0.4216],\n",
      "        [-0.5698, -0.8339],\n",
      "        [-0.9872, -0.4662],\n",
      "        [-1.5162, -0.2479],\n",
      "        [-0.9235, -0.5061],\n",
      "        [-1.4358, -0.2717],\n",
      "        [-0.8684, -0.5440],\n",
      "        [-1.3664, -0.2944],\n",
      "        [-0.8220, -0.5790],\n",
      "        [-1.3063, -0.3158],\n",
      "        [-0.7810, -0.6124],\n",
      "        [-0.4010, -1.1075],\n",
      "        [-0.7361, -0.6520],\n",
      "        [-1.1918, -0.3619],\n",
      "        [-1.7444, -0.1921],\n",
      "        [-1.1497, -0.3809],\n",
      "        [-1.6891, -0.2042],\n",
      "        [-1.1184, -0.3957],\n",
      "        [-1.6496, -0.2133],\n",
      "        [-1.0965, -0.4065],\n",
      "        [-0.6385, -0.7510],\n",
      "        [-0.3337, -1.2598],\n",
      "        [-0.6146, -0.7784],\n",
      "        [-0.3172, -1.3026],\n",
      "        [-0.5799, -0.8209],\n",
      "        [-0.2951, -1.3644],\n",
      "        [-0.5383, -0.8764],\n",
      "        [-0.2694, -1.4433],\n",
      "        [-0.4896, -0.9490],\n",
      "        [-0.8271, -0.5750],\n",
      "        [-1.2471, -0.3388],\n",
      "        [-0.7546, -0.6353],\n",
      "        [-0.3949, -1.1200],\n",
      "        [-0.6763, -0.7103],\n",
      "        [-1.0520, -0.4296],\n",
      "        [-1.5064, -0.2506],\n",
      "        [-2.0495, -0.1379],\n",
      "        [-1.4070, -0.2809],\n",
      "        [-0.8960, -0.5246],\n",
      "        [-0.5000, -0.9328],\n",
      "        [-0.2477, -1.5168],\n",
      "        [-0.4371, -1.0382],\n",
      "        [-0.7217, -0.6654],\n",
      "        [-1.0684, -0.4209],\n",
      "        [-1.5109, -0.2494],\n",
      "        [-0.9632, -0.4807],\n",
      "        [-1.3739, -0.2919],\n",
      "        [-0.8613, -0.5492],\n",
      "        [-0.4850, -0.9563],\n",
      "        [-0.7606, -0.6299],\n",
      "        [-0.4108, -1.0881],\n",
      "        [-0.6603, -0.7271],\n",
      "        [-0.3344, -1.2579],\n",
      "        [-0.5930, -0.8044],\n",
      "        [-0.3219, -1.2901],\n",
      "        [-0.6118, -0.7816],\n",
      "        [-1.0410, -0.4355],\n",
      "        [-0.6361, -0.7536],\n",
      "        [-0.3493, -1.2213],\n",
      "        [-0.6601, -0.7274],\n",
      "        [-0.3644, -1.1861],\n",
      "        [-0.6839, -0.7024],\n",
      "        [-0.3796, -1.1524],\n",
      "        [-0.7079, -0.6786],\n",
      "        [-0.3954, -1.1190],\n",
      "        [-0.7329, -0.6549],\n",
      "        [-0.4120, -1.0857],\n",
      "        [-0.2222, -1.6131],\n",
      "        [-0.4246, -1.0614],\n",
      "        [-0.7721, -0.6200],\n",
      "        [-0.4383, -1.0361],\n",
      "        [-0.7919, -0.6033],\n",
      "        [-1.2748, -0.3278],\n",
      "        [-1.8855, -0.1646],\n",
      "        [-2.5207, -0.0838],\n",
      "        [-1.9801, -0.1486],\n",
      "        [-1.4548, -0.2659],\n",
      "        [-1.0008, -0.4582],\n",
      "        [-1.5835, -0.2297],\n",
      "        [-1.1224, -0.3938],\n",
      "        [-0.7547, -0.6351],\n",
      "        [-0.4587, -1.0000],\n",
      "        [-0.8631, -0.5479],\n",
      "        [-0.5415, -0.8719],\n",
      "        [-0.3326, -1.2625],\n",
      "        [-0.2096, -1.6656],\n",
      "        [-0.3871, -1.1365],\n",
      "        [-0.7148, -0.6720],\n",
      "        [-1.1915, -0.3621],\n",
      "        [-1.7707, -0.1866],\n",
      "        [-2.4178, -0.0933],\n",
      "        [-1.9405, -0.1551],\n",
      "        [-1.5339, -0.2430],\n",
      "        [-1.1690, -0.3721]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([276])\n",
      "276\n",
      "torch.Size([276])\n",
      "2297: reward:  41.00, mean_100:  33.78, episodes: 68\n",
      "2332: reward:  35.00, mean_100:  33.80, episodes: 69\n",
      "2394: reward:  62.00, mean_100:  34.20, episodes: 70\n",
      "2431: reward:  37.00, mean_100:  34.24, episodes: 71\n",
      "torch.Size([233, 2])\n",
      "tensor([[-0.4925, -0.9444],\n",
      "        [-0.8899, -0.5288],\n",
      "        [-1.4634, -0.2632],\n",
      "        [-2.0925, -0.1317],\n",
      "        [-2.7486, -0.0662],\n",
      "        [-2.1891, -0.1188],\n",
      "        [-1.6557, -0.2119],\n",
      "        [-1.1301, -0.3901],\n",
      "        [-0.7330, -0.6548],\n",
      "        [-1.2613, -0.3331],\n",
      "        [-0.8411, -0.5643],\n",
      "        [-0.5083, -0.9200],\n",
      "        [-0.9605, -0.4824],\n",
      "        [-1.5589, -0.2362],\n",
      "        [-1.1060, -0.4018],\n",
      "        [-0.7310, -0.6567],\n",
      "        [-0.4351, -1.0419],\n",
      "        [-0.2651, -1.4573],\n",
      "        [-0.5257, -0.8944],\n",
      "        [-0.9860, -0.4669],\n",
      "        [-0.6328, -0.7573],\n",
      "        [-1.1297, -0.3903],\n",
      "        [-0.7606, -0.6300],\n",
      "        [-1.2857, -0.3236],\n",
      "        [-0.9099, -0.5151],\n",
      "        [-1.4538, -0.2662],\n",
      "        [-1.0821, -0.4138],\n",
      "        [-0.7274, -0.6600],\n",
      "        [-0.4916, -0.9459],\n",
      "        [-0.8942, -0.5259],\n",
      "        [-0.6004, -0.7954],\n",
      "        [-0.4133, -1.0832],\n",
      "        [-0.7331, -0.6548],\n",
      "        [-1.2357, -0.3434],\n",
      "        [-1.8041, -0.1799],\n",
      "        [-0.5111, -0.9159],\n",
      "        [-0.9217, -0.5073],\n",
      "        [-1.5048, -0.2511],\n",
      "        [-0.9762, -0.4728],\n",
      "        [-1.5796, -0.2308],\n",
      "        [-1.0508, -0.4303],\n",
      "        [-0.6529, -0.7350],\n",
      "        [-0.3602, -1.1958],\n",
      "        [-0.7163, -0.6705],\n",
      "        [-0.4013, -1.1071],\n",
      "        [-0.7815, -0.6119],\n",
      "        [-1.3034, -0.3169],\n",
      "        [-0.8618, -0.5489],\n",
      "        [-0.5150, -0.9101],\n",
      "        [-0.2828, -1.4011],\n",
      "        [-0.5775, -0.8239],\n",
      "        [-0.3179, -1.3009],\n",
      "        [-0.6406, -0.7486],\n",
      "        [-0.3578, -1.2013],\n",
      "        [-0.2037, -1.6911],\n",
      "        [-0.3958, -1.1183],\n",
      "        [-0.2208, -1.6190],\n",
      "        [-0.1290, -2.1121],\n",
      "        [-0.2337, -1.5681],\n",
      "        [-0.4502, -1.0147],\n",
      "        [-0.2451, -1.5261],\n",
      "        [-0.4698, -0.9811],\n",
      "        [-0.8396, -0.5655],\n",
      "        [-1.3218, -0.3101],\n",
      "        [-0.8842, -0.5328],\n",
      "        [-1.3845, -0.2883],\n",
      "        [-0.9473, -0.4907],\n",
      "        [-0.5859, -0.8133],\n",
      "        [-0.3345, -1.2578],\n",
      "        [-0.6358, -0.7540],\n",
      "        [-0.3665, -1.1815],\n",
      "        [-0.6875, -0.6988],\n",
      "        [-1.1434, -0.3838],\n",
      "        [-1.7159, -0.1982],\n",
      "        [-1.2476, -0.3385],\n",
      "        [-1.8495, -0.1712],\n",
      "        [-1.3834, -0.2886],\n",
      "        [-0.9850, -0.4675],\n",
      "        [-0.6438, -0.7451],\n",
      "        [-1.1171, -0.3964],\n",
      "        [-0.7650, -0.6261],\n",
      "        [-0.4947, -0.9410],\n",
      "        [-0.3239, -1.2850],\n",
      "        [-0.2040, -1.6901],\n",
      "        [-0.3833, -1.1445],\n",
      "        [-0.6780, -0.7086],\n",
      "        [-0.4456, -1.0228],\n",
      "        [-0.7820, -0.6115],\n",
      "        [-0.5140, -0.9116],\n",
      "        [-0.8978, -0.5233],\n",
      "        [-1.4280, -0.2741],\n",
      "        [-1.0458, -0.4330],\n",
      "        [-0.7299, -0.6577],\n",
      "        [-0.4999, -0.9329],\n",
      "        [-0.8617, -0.5490],\n",
      "        [-1.3809, -0.2895],\n",
      "        [-1.0294, -0.4419],\n",
      "        [-0.5287, -0.8900],\n",
      "        [-0.9243, -0.5055],\n",
      "        [-0.5245, -0.8962],\n",
      "        [-0.9217, -0.5073],\n",
      "        [-1.4857, -0.2566],\n",
      "        [-0.9344, -0.4989],\n",
      "        [-0.5399, -0.8742],\n",
      "        [-0.2804, -1.4084],\n",
      "        [-0.5491, -0.8614],\n",
      "        [-0.9633, -0.4807],\n",
      "        [-1.5464, -0.2395],\n",
      "        [-0.9948, -0.4617],\n",
      "        [-1.5961, -0.2265],\n",
      "        [-2.2225, -0.1147],\n",
      "        [-1.6799, -0.2063],\n",
      "        [-1.1388, -0.3860],\n",
      "        [-0.7260, -0.6613],\n",
      "        [-0.4069, -1.0957],\n",
      "        [-0.7993, -0.5972],\n",
      "        [-0.4619, -0.9945],\n",
      "        [-0.8786, -0.5368],\n",
      "        [-0.5249, -0.8955],\n",
      "        [-0.2884, -1.3841],\n",
      "        [-0.5861, -0.8130],\n",
      "        [-0.3229, -1.2874],\n",
      "        [-0.1868, -1.7699],\n",
      "        [-0.3555, -1.2067],\n",
      "        [-0.6969, -0.6894],\n",
      "        [-1.1728, -0.3703],\n",
      "        [-1.7728, -0.1862],\n",
      "        [-2.4146, -0.0937],\n",
      "        [-3.0894, -0.0466],\n",
      "        [-2.5720, -0.0795],\n",
      "        [-2.0967, -0.1311],\n",
      "        [-1.6528, -0.2126],\n",
      "        [-1.2551, -0.3355],\n",
      "        [-0.8982, -0.5230],\n",
      "        [-0.5226, -0.8989],\n",
      "        [-0.8929, -0.5267],\n",
      "        [-0.4947, -0.9410],\n",
      "        [-0.8562, -0.5530],\n",
      "        [-1.3971, -0.2841],\n",
      "        [-0.8346, -0.5692],\n",
      "        [-1.3831, -0.2887],\n",
      "        [-0.8275, -0.5748],\n",
      "        [-0.4580, -1.0012],\n",
      "        [-0.8235, -0.5778],\n",
      "        [-0.4557, -1.0051],\n",
      "        [-0.8223, -0.5788],\n",
      "        [-0.4552, -1.0061],\n",
      "        [-0.8236, -0.5778],\n",
      "        [-1.3833, -0.2887],\n",
      "        [-0.8381, -0.5666],\n",
      "        [-0.4686, -0.9832],\n",
      "        [-0.2429, -1.5341],\n",
      "        [-0.4745, -0.9733],\n",
      "        [-0.2451, -1.5263],\n",
      "        [-0.4753, -0.9720],\n",
      "        [-0.8563, -0.5529],\n",
      "        [-0.4771, -0.9691],\n",
      "        [-0.8616, -0.5490],\n",
      "        [-0.4811, -0.9625],\n",
      "        [-0.2485, -1.5140],\n",
      "        [-0.4813, -0.9622],\n",
      "        [-0.2478, -1.5165],\n",
      "        [-0.4766, -0.9700],\n",
      "        [-0.2445, -1.5282],\n",
      "        [-0.1297, -2.1070],\n",
      "        [-0.0709, -2.6823],\n",
      "        [-0.1214, -2.1684],\n",
      "        [-0.2178, -1.6310],\n",
      "        [-0.3976, -1.1144],\n",
      "        [-0.1963, -1.7245],\n",
      "        [-0.1007, -2.3451],\n",
      "        [-0.1723, -1.8437],\n",
      "        [-0.3150, -1.3084],\n",
      "        [-0.1493, -1.9753],\n",
      "        [-0.2706, -1.4395],\n",
      "        [-0.4729, -0.9761],\n",
      "        [-0.2272, -1.5932],\n",
      "        [-0.3973, -1.1151],\n",
      "        [-0.6982, -0.6881],\n",
      "        [-0.3321, -1.2638],\n",
      "        [-0.5949, -0.8021],\n",
      "        [-0.2787, -1.4138],\n",
      "        [-0.4988, -0.9345],\n",
      "        [-0.8629, -0.5480],\n",
      "        [-0.4160, -1.0779],\n",
      "        [-0.1966, -1.7231],\n",
      "        [-0.3434, -1.2357],\n",
      "        [-0.6105, -0.7833],\n",
      "        [-1.0388, -0.4368],\n",
      "        [-1.5613, -0.2356],\n",
      "        [-0.9061, -0.5177],\n",
      "        [-0.4340, -1.0439],\n",
      "        [-0.2001, -1.7075],\n",
      "        [-0.3644, -1.1863],\n",
      "        [-0.6539, -0.7340],\n",
      "        [-0.3038, -1.3393],\n",
      "        [-0.5464, -0.8652],\n",
      "        [-0.2505, -1.5071],\n",
      "        [-0.4491, -1.0166],\n",
      "        [-0.7888, -0.6059],\n",
      "        [-0.3666, -1.1812],\n",
      "        [-0.6554, -0.7324],\n",
      "        [-1.0631, -0.4237],\n",
      "        [-0.5416, -0.8719],\n",
      "        [-0.9021, -0.5204],\n",
      "        [-0.4443, -1.0253],\n",
      "        [-0.7611, -0.6296],\n",
      "        [-1.1947, -0.3607],\n",
      "        [-1.7168, -0.1980],\n",
      "        [-1.0347, -0.4390],\n",
      "        [-1.5454, -0.2398],\n",
      "        [-2.1274, -0.1269],\n",
      "        [-1.3985, -0.2837],\n",
      "        [-0.8027, -0.5944],\n",
      "        [-0.4168, -1.0762],\n",
      "        [-0.7093, -0.6773],\n",
      "        [-1.1192, -0.3953],\n",
      "        [-0.6242, -0.7672],\n",
      "        [-0.9905, -0.4643],\n",
      "        [-0.5461, -0.8656],\n",
      "        [-0.8730, -0.5408],\n",
      "        [-1.3284, -0.3078],\n",
      "        [-0.7662, -0.6251],\n",
      "        [-1.1762, -0.3688],\n",
      "        [-0.6712, -0.7156],\n",
      "        [-1.0369, -0.4378],\n",
      "        [-0.5807, -0.8199],\n",
      "        [-0.9046, -0.5187],\n",
      "        [-1.3477, -0.3009],\n",
      "        [-0.7885, -0.6061],\n",
      "        [-0.4219, -1.0665],\n",
      "        [-0.1903, -1.7530],\n",
      "        [-0.3408, -1.2421]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([233])\n",
      "233\n",
      "torch.Size([233])\n",
      "2530: reward:  99.00, mean_100:  35.14, episodes: 72\n",
      "2601: reward:  71.00, mean_100:  35.63, episodes: 73\n",
      "2630: reward:  29.00, mean_100:  35.54, episodes: 74\n",
      "2693: reward:  63.00, mean_100:  35.91, episodes: 75\n",
      "torch.Size([254, 2])\n",
      "tensor([[-0.5129, -0.9132],\n",
      "        [-0.9246, -0.5053],\n",
      "        [-0.5111, -0.9158],\n",
      "        [-0.9250, -0.5050],\n",
      "        [-1.5005, -0.2523],\n",
      "        [-0.9411, -0.4946],\n",
      "        [-0.5301, -0.8880],\n",
      "        [-0.2666, -1.4525],\n",
      "        [-0.5403, -0.8736],\n",
      "        [-0.9726, -0.4749],\n",
      "        [-0.5540, -0.8548],\n",
      "        [-0.2791, -1.4125],\n",
      "        [-0.5644, -0.8409],\n",
      "        [-0.2839, -1.3976],\n",
      "        [-0.5709, -0.8325],\n",
      "        [-1.0095, -0.4532],\n",
      "        [-0.5817, -0.8186],\n",
      "        [-0.2934, -1.3692],\n",
      "        [-0.5899, -0.8083],\n",
      "        [-0.2972, -1.3583],\n",
      "        [-0.5946, -0.8025],\n",
      "        [-0.2991, -1.3528],\n",
      "        [-0.1561, -1.9342],\n",
      "        [-0.2956, -1.3628],\n",
      "        [-0.5809, -0.8196],\n",
      "        [-1.0082, -0.4539],\n",
      "        [-0.5746, -0.8276],\n",
      "        [-0.2867, -1.3892],\n",
      "        [-0.5658, -0.8391],\n",
      "        [-0.9899, -0.4646],\n",
      "        [-1.5685, -0.2337],\n",
      "        [-2.1785, -0.1202],\n",
      "        [-1.6070, -0.2238],\n",
      "        [-1.0468, -0.4324],\n",
      "        [-1.6665, -0.2094],\n",
      "        [-1.1089, -0.4004],\n",
      "        [-0.6810, -0.7054],\n",
      "        [-1.1830, -0.3658],\n",
      "        [-0.7426, -0.6460],\n",
      "        [-0.4029, -1.1037],\n",
      "        [-0.8060, -0.5918],\n",
      "        [-0.4485, -1.0177],\n",
      "        [-0.8736, -0.5404],\n",
      "        [-1.4379, -0.2711],\n",
      "        [-0.9596, -0.4830],\n",
      "        [-1.5507, -0.2384],\n",
      "        [-1.0653, -0.4225],\n",
      "        [-0.6655, -0.7216],\n",
      "        [-1.1826, -0.3660],\n",
      "        [-0.7701, -0.6217],\n",
      "        [-0.4430, -1.0275],\n",
      "        [-0.2576, -1.4823],\n",
      "        [-0.5154, -0.9095],\n",
      "        [-0.9752, -0.4734],\n",
      "        [-1.5579, -0.2364],\n",
      "        [-1.1034, -0.4031],\n",
      "        [-0.7115, -0.6751],\n",
      "        [-0.4297, -1.0518],\n",
      "        [-0.8286, -0.5739],\n",
      "        [-1.3781, -0.2904],\n",
      "        [-0.9680, -0.4778],\n",
      "        [-1.5339, -0.2430],\n",
      "        [-1.1293, -0.3905],\n",
      "        [-1.7088, -0.1998],\n",
      "        [-1.3056, -0.3161],\n",
      "        [-0.9331, -0.4998],\n",
      "        [-0.6233, -0.7682],\n",
      "        [-0.4231, -1.0643],\n",
      "        [-0.7641, -0.6269],\n",
      "        [-1.2988, -0.3186],\n",
      "        [-1.8988, -0.1622],\n",
      "        [-0.5082, -0.9202],\n",
      "        [-0.9057, -0.5180],\n",
      "        [-1.4548, -0.2658],\n",
      "        [-0.8997, -0.5221],\n",
      "        [-1.4582, -0.2648],\n",
      "        [-0.9086, -0.5160],\n",
      "        [-0.5043, -0.9261],\n",
      "        [-0.2518, -1.5025],\n",
      "        [-0.5082, -0.9203],\n",
      "        [-0.9230, -0.5064],\n",
      "        [-1.4941, -0.2542],\n",
      "        [-2.1104, -0.1292],\n",
      "        [-2.7630, -0.0652],\n",
      "        [-2.1916, -0.1185],\n",
      "        [-1.6535, -0.2124],\n",
      "        [-1.1195, -0.3952],\n",
      "        [-1.7653, -0.1877],\n",
      "        [-1.2477, -0.3385],\n",
      "        [-1.9064, -0.1609],\n",
      "        [-1.4078, -0.2806],\n",
      "        [-2.0729, -0.1345],\n",
      "        [-1.5871, -0.2288],\n",
      "        [-1.1472, -0.3820],\n",
      "        [-0.7590, -0.6314],\n",
      "        [-1.3408, -0.3033],\n",
      "        [-0.9366, -0.4975],\n",
      "        [-1.5299, -0.2441],\n",
      "        [-1.1340, -0.3882],\n",
      "        [-1.7383, -0.1934],\n",
      "        [-0.5362, -0.8794],\n",
      "        [-0.2650, -1.4575],\n",
      "        [-0.1378, -2.0504],\n",
      "        [-0.2524, -1.5002],\n",
      "        [-0.4837, -0.9583],\n",
      "        [-0.2367, -1.5569],\n",
      "        [-0.4511, -1.0131],\n",
      "        [-0.8069, -0.5910],\n",
      "        [-0.4217, -1.0669],\n",
      "        [-0.7643, -0.6267],\n",
      "        [-0.3951, -1.1196],\n",
      "        [-0.1929, -1.7404],\n",
      "        [-0.0990, -2.3613],\n",
      "        [-0.1742, -1.8334],\n",
      "        [-0.3273, -1.2762],\n",
      "        [-0.1550, -1.9405],\n",
      "        [-0.2896, -1.3806],\n",
      "        [-0.1356, -2.0654],\n",
      "        [-0.2493, -1.5110],\n",
      "        [-0.4456, -1.0229],\n",
      "        [-0.2082, -1.6716],\n",
      "        [-0.3744, -1.1638],\n",
      "        [-0.6682, -0.7188],\n",
      "        [-0.3131, -1.3137],\n",
      "        [-0.5670, -0.8376],\n",
      "        [-0.2610, -1.4709],\n",
      "        [-0.1202, -2.1783],\n",
      "        [-0.2148, -1.6435],\n",
      "        [-0.3775, -1.1571],\n",
      "        [-0.1720, -1.8452],\n",
      "        [-0.2963, -1.3609],\n",
      "        [-0.1346, -2.0720],\n",
      "        [-0.2320, -1.5747],\n",
      "        [-0.4064, -1.0967],\n",
      "        [-0.1790, -1.8087],\n",
      "        [-0.3089, -1.3251],\n",
      "        [-0.5356, -0.8803],\n",
      "        [-0.9124, -0.5135],\n",
      "        [-1.3844, -0.2883],\n",
      "        [-0.7246, -0.6626],\n",
      "        [-1.1639, -0.3743],\n",
      "        [-1.6857, -0.2050],\n",
      "        [-0.9801, -0.4704],\n",
      "        [-0.4706, -0.9799],\n",
      "        [-0.8211, -0.5797],\n",
      "        [-1.2861, -0.3234],\n",
      "        [-1.8415, -0.1727],\n",
      "        [-1.1255, -0.3923],\n",
      "        [-0.5888, -0.8097],\n",
      "        [-0.9776, -0.4720],\n",
      "        [-0.4979, -0.9361],\n",
      "        [-0.8408, -0.5645],\n",
      "        [-0.4174, -1.0751],\n",
      "        [-0.7181, -0.6688],\n",
      "        [-0.3454, -1.2308],\n",
      "        [-0.6117, -0.7818],\n",
      "        [-0.2825, -1.4019],\n",
      "        [-0.5118, -0.9149],\n",
      "        [-0.2257, -1.5994],\n",
      "        [-0.4128, -1.0841],\n",
      "        [-0.6767, -0.7099],\n",
      "        [-0.3267, -1.2776],\n",
      "        [-0.1339, -2.0767],\n",
      "        [-0.5568, -0.8510],\n",
      "        [-0.9822, -0.4692],\n",
      "        [-0.5513, -0.8585],\n",
      "        [-0.9788, -0.4712],\n",
      "        [-1.5630, -0.2351],\n",
      "        [-0.9927, -0.4629],\n",
      "        [-0.5669, -0.8377],\n",
      "        [-1.0132, -0.4511],\n",
      "        [-0.5850, -0.8145],\n",
      "        [-0.2966, -1.3599],\n",
      "        [-0.6005, -0.7952],\n",
      "        [-1.0579, -0.4265],\n",
      "        [-0.6215, -0.7703],\n",
      "        [-1.0891, -0.4103],\n",
      "        [-0.6500, -0.7382],\n",
      "        [-0.3358, -1.2544],\n",
      "        [-0.6770, -0.7095],\n",
      "        [-0.3513, -1.2167],\n",
      "        [-0.7019, -0.6844],\n",
      "        [-1.1933, -0.3613],\n",
      "        [-0.7347, -0.6533],\n",
      "        [-1.2427, -0.3405],\n",
      "        [-0.7773, -0.6156],\n",
      "        [-1.3066, -0.3157],\n",
      "        [-0.8335, -0.5701],\n",
      "        [-0.4655, -0.9884],\n",
      "        [-0.2464, -1.5215],\n",
      "        [-0.1391, -2.0413],\n",
      "        [-0.0791, -2.5760],\n",
      "        [-0.1422, -2.0211],\n",
      "        [-0.2622, -1.4669],\n",
      "        [-0.5223, -0.8993],\n",
      "        [-0.9334, -0.4996],\n",
      "        [-0.5307, -0.8872],\n",
      "        [-0.9467, -0.4910],\n",
      "        [-0.5435, -0.8692],\n",
      "        [-0.2762, -1.4214],\n",
      "        [-0.5525, -0.8568],\n",
      "        [-0.9741, -0.4740],\n",
      "        [-0.5669, -0.8377],\n",
      "        [-0.2893, -1.3816],\n",
      "        [-0.5777, -0.8237],\n",
      "        [-0.2951, -1.3645],\n",
      "        [-0.5856, -0.8137],\n",
      "        [-0.2991, -1.3527],\n",
      "        [-0.5908, -0.8072],\n",
      "        [-0.3017, -1.3455],\n",
      "        [-0.1593, -1.9158],\n",
      "        [-0.2982, -1.3554],\n",
      "        [-0.5808, -0.8198],\n",
      "        [-0.2927, -1.3713],\n",
      "        [-0.5700, -0.8336],\n",
      "        [-0.9760, -0.4729],\n",
      "        [-0.5654, -0.8397],\n",
      "        [-0.2846, -1.3956],\n",
      "        [-0.5583, -0.8490],\n",
      "        [-0.9622, -0.4813],\n",
      "        [-0.5568, -0.8510],\n",
      "        [-0.9643, -0.4801],\n",
      "        [-1.5087, -0.2500],\n",
      "        [-0.9863, -0.4667],\n",
      "        [-1.5479, -0.2391],\n",
      "        [-2.1866, -0.1191],\n",
      "        [-1.6270, -0.2188],\n",
      "        [-1.1135, -0.3981],\n",
      "        [-0.7084, -0.6781],\n",
      "        [-0.3940, -1.1219],\n",
      "        [-0.7735, -0.6188],\n",
      "        [-0.4418, -1.0296],\n",
      "        [-0.2414, -1.5396],\n",
      "        [-0.4891, -0.9497],\n",
      "        [-0.9035, -0.5194],\n",
      "        [-1.4390, -0.2707],\n",
      "        [-0.9906, -0.4642],\n",
      "        [-0.6122, -0.7812],\n",
      "        [-0.3479, -1.2247],\n",
      "        [-0.6813, -0.7051],\n",
      "        [-0.3937, -1.1226],\n",
      "        [-0.7563, -0.6337],\n",
      "        [-1.2653, -0.3315],\n",
      "        [-0.8513, -0.5567],\n",
      "        [-1.3850, -0.2881],\n",
      "        [-0.9648, -0.4797],\n",
      "        [-0.6083, -0.7858],\n",
      "        [-1.0836, -0.4130],\n",
      "        [-1.6796, -0.2063],\n",
      "        [-1.2366, -0.3430],\n",
      "        [-1.8716, -0.1671],\n",
      "        [-1.4293, -0.2738],\n",
      "        [-1.0419, -0.4351],\n",
      "        [-1.6448, -0.2145]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([254])\n",
      "254\n",
      "torch.Size([254])\n",
      "2784: reward:  91.00, mean_100:  36.63, episodes: 76\n",
      "2846: reward:  62.00, mean_100:  36.96, episodes: 77\n",
      "2913: reward:  67.00, mean_100:  37.35, episodes: 78\n",
      "2937: reward:  24.00, mean_100:  37.18, episodes: 79\n",
      "torch.Size([210, 2])\n",
      "tensor([[-0.5943, -0.8028],\n",
      "        [-1.0495, -0.4310],\n",
      "        [-0.5759, -0.8261],\n",
      "        [-1.0270, -0.4433],\n",
      "        [-0.5624, -0.8436],\n",
      "        [-1.0116, -0.4520],\n",
      "        [-1.6253, -0.2192],\n",
      "        [-1.0160, -0.4495],\n",
      "        [-0.5610, -0.8454],\n",
      "        [-0.2735, -1.4302],\n",
      "        [-0.5622, -0.8439],\n",
      "        [-0.2732, -1.4311],\n",
      "        [-0.5585, -0.8488],\n",
      "        [-1.0145, -0.4504],\n",
      "        [-0.5574, -0.8503],\n",
      "        [-0.2706, -1.4394],\n",
      "        [-0.5529, -0.8563],\n",
      "        [-1.0063, -0.4550],\n",
      "        [-0.5510, -0.8590],\n",
      "        [-1.0075, -0.4543],\n",
      "        [-0.5529, -0.8563],\n",
      "        [-1.0139, -0.4507],\n",
      "        [-0.5591, -0.8479],\n",
      "        [-1.0258, -0.4440],\n",
      "        [-1.6616, -0.2105],\n",
      "        [-1.0557, -0.4277],\n",
      "        [-0.5994, -0.7966],\n",
      "        [-0.2968, -1.3596],\n",
      "        [-0.6218, -0.7700],\n",
      "        [-1.1212, -0.3944],\n",
      "        [-0.6502, -0.7380],\n",
      "        [-1.1631, -0.3747],\n",
      "        [-0.6864, -0.7000],\n",
      "        [-0.3462, -1.2290],\n",
      "        [-0.1774, -1.8167],\n",
      "        [-0.3604, -1.1953],\n",
      "        [-0.7363, -0.6518],\n",
      "        [-1.2723, -0.3287],\n",
      "        [-1.9347, -0.1560],\n",
      "        [-1.3319, -0.3065],\n",
      "        [-0.8258, -0.5761],\n",
      "        [-0.4379, -1.0367],\n",
      "        [-0.2217, -1.6151],\n",
      "        [-0.4702, -0.9805],\n",
      "        [-0.9199, -0.5084],\n",
      "        [-1.5179, -0.2474],\n",
      "        [-0.9829, -0.4687],\n",
      "        [-0.5590, -0.8481],\n",
      "        [-0.2826, -1.4017],\n",
      "        [-0.6064, -0.7882],\n",
      "        [-1.1076, -0.4010],\n",
      "        [-0.6599, -0.7275],\n",
      "        [-1.1790, -0.3676],\n",
      "        [-0.7222, -0.6649],\n",
      "        [-1.2630, -0.3324],\n",
      "        [-1.9223, -0.1581],\n",
      "        [-2.6038, -0.0769],\n",
      "        [-2.0714, -0.1347],\n",
      "        [-2.7599, -0.0654],\n",
      "        [-3.4997, -0.0307],\n",
      "        [-2.9800, -0.0521],\n",
      "        [-2.5055, -0.0852],\n",
      "        [-0.4917, -0.9457],\n",
      "        [-0.2334, -1.5694],\n",
      "        [-0.4582, -1.0008],\n",
      "        [-0.8400, -0.5651],\n",
      "        [-0.4269, -1.0572],\n",
      "        [-0.2006, -1.7050],\n",
      "        [-0.3933, -1.1234],\n",
      "        [-0.1825, -1.7909],\n",
      "        [-0.3568, -1.2037],\n",
      "        [-0.1638, -1.8899],\n",
      "        [-0.0813, -2.5503],\n",
      "        [-0.1421, -2.0214],\n",
      "        [-0.2699, -1.4417],\n",
      "        [-0.5004, -0.9321],\n",
      "        [-0.8805, -0.5354],\n",
      "        [-0.4246, -1.0614],\n",
      "        [-0.7717, -0.6203],\n",
      "        [-0.3569, -1.2034],\n",
      "        [-0.6681, -0.7188],\n",
      "        [-0.3015, -1.3461],\n",
      "        [-0.5724, -0.8305],\n",
      "        [-1.0109, -0.4524],\n",
      "        [-0.4878, -0.9519],\n",
      "        [-0.8891, -0.5293],\n",
      "        [-0.4152, -1.0795],\n",
      "        [-0.7775, -0.6154],\n",
      "        [-0.3539, -1.2105],\n",
      "        [-0.6728, -0.7140],\n",
      "        [-1.1702, -0.3715],\n",
      "        [-1.7629, -0.1882],\n",
      "        [-2.3739, -0.0977],\n",
      "        [-1.6735, -0.2077],\n",
      "        [-0.9987, -0.4595],\n",
      "        [-1.5991, -0.2258],\n",
      "        [-0.9469, -0.4909],\n",
      "        [-0.4586, -1.0001],\n",
      "        [-0.2115, -1.6575],\n",
      "        [-0.4236, -1.0632],\n",
      "        [-0.1930, -1.7399],\n",
      "        [-0.0921, -2.4303],\n",
      "        [-0.1696, -1.8578],\n",
      "        [-0.0808, -2.5563],\n",
      "        [-0.1442, -2.0078],\n",
      "        [-0.2743, -1.4277],\n",
      "        [-0.1190, -2.1876],\n",
      "        [-0.2191, -1.6259],\n",
      "        [-0.4098, -1.0899],\n",
      "        [-0.1719, -1.8454],\n",
      "        [-0.3203, -1.2943],\n",
      "        [-0.5830, -0.8169],\n",
      "        [-0.9647, -0.4798],\n",
      "        [-1.4540, -0.2661],\n",
      "        [-2.0315, -0.1406],\n",
      "        [-1.2702, -0.3296],\n",
      "        [-0.6699, -0.7170],\n",
      "        [-0.3047, -1.3370],\n",
      "        [-0.5580, -0.8495],\n",
      "        [-0.9237, -0.5059],\n",
      "        [-1.4219, -0.2761],\n",
      "        [-0.7801, -0.6132],\n",
      "        [-0.3835, -1.1441],\n",
      "        [-0.6551, -0.7327],\n",
      "        [-0.3102, -1.3216],\n",
      "        [-0.1258, -2.1351],\n",
      "        [-0.2423, -1.5362],\n",
      "        [-0.0960, -2.3912],\n",
      "        [-0.1791, -1.8082],\n",
      "        [-0.6461, -0.7425],\n",
      "        [-1.1623, -0.3751],\n",
      "        [-0.6894, -0.6969],\n",
      "        [-1.2249, -0.3478],\n",
      "        [-0.7434, -0.6453],\n",
      "        [-1.2994, -0.3184],\n",
      "        [-0.8099, -0.5886],\n",
      "        [-1.3896, -0.2866],\n",
      "        [-2.0836, -0.1329],\n",
      "        [-1.5143, -0.2484],\n",
      "        [-1.0052, -0.4557],\n",
      "        [-0.5987, -0.7975],\n",
      "        [-0.3134, -1.3128],\n",
      "        [-0.6872, -0.6991],\n",
      "        [-0.3651, -1.1846],\n",
      "        [-0.7798, -0.6134],\n",
      "        [-1.3511, -0.2997],\n",
      "        [-2.0330, -0.1403],\n",
      "        [-2.7318, -0.0673],\n",
      "        [-2.2190, -0.1151],\n",
      "        [-1.7303, -0.1951],\n",
      "        [-1.2547, -0.3357],\n",
      "        [-1.9308, -0.1567],\n",
      "        [-2.6451, -0.0736],\n",
      "        [-0.5779, -0.8234],\n",
      "        [-0.2756, -1.4233],\n",
      "        [-0.5580, -0.8494],\n",
      "        [-0.2650, -1.4575],\n",
      "        [-0.5339, -0.8826],\n",
      "        [-0.9677, -0.4780],\n",
      "        [-0.5134, -0.9125],\n",
      "        [-0.9425, -0.4937],\n",
      "        [-1.5250, -0.2454],\n",
      "        [-0.9341, -0.4991],\n",
      "        [-0.4945, -0.9413],\n",
      "        [-0.9288, -0.5026],\n",
      "        [-1.5209, -0.2466],\n",
      "        [-0.9381, -0.4965],\n",
      "        [-1.5421, -0.2407],\n",
      "        [-0.9631, -0.4808],\n",
      "        [-1.5822, -0.2301],\n",
      "        [-1.0047, -0.4559],\n",
      "        [-0.5635, -0.8422],\n",
      "        [-0.2782, -1.4153],\n",
      "        [-0.5934, -0.8039],\n",
      "        [-0.2928, -1.3710],\n",
      "        [-0.6194, -0.7728],\n",
      "        [-0.3058, -1.3339],\n",
      "        [-0.6416, -0.7475],\n",
      "        [-0.3171, -1.3029],\n",
      "        [-0.6604, -0.7270],\n",
      "        [-1.1638, -0.3744],\n",
      "        [-1.8062, -0.1795],\n",
      "        [-1.2150, -0.3520],\n",
      "        [-0.7378, -0.6504],\n",
      "        [-0.3781, -1.1558],\n",
      "        [-0.1963, -1.7246],\n",
      "        [-0.4000, -1.1096],\n",
      "        [-0.8086, -0.5896],\n",
      "        [-0.4211, -1.0681],\n",
      "        [-0.8410, -0.5644],\n",
      "        [-1.3993, -0.2834],\n",
      "        [-2.0545, -0.1371],\n",
      "        [-1.4809, -0.2580],\n",
      "        [-0.9686, -0.4774],\n",
      "        [-0.5528, -0.8565],\n",
      "        [-1.0477, -0.4319],\n",
      "        [-0.6197, -0.7724],\n",
      "        [-1.1360, -0.3873],\n",
      "        [-1.7802, -0.1847],\n",
      "        [-1.2538, -0.3361],\n",
      "        [-1.9137, -0.1596],\n",
      "        [-1.4001, -0.2831],\n",
      "        [-2.0660, -0.1355],\n",
      "        [-1.5795, -0.2308],\n",
      "        [-2.2452, -0.1119],\n",
      "        [-1.7781, -0.1851],\n",
      "        [-1.3304, -0.3070],\n",
      "        [-0.9223, -0.5069],\n",
      "        [-1.5324, -0.2434],\n",
      "        [-1.1324, -0.3890]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([210])\n",
      "210\n",
      "torch.Size([210])\n",
      "2994: reward:  57.00, mean_100:  37.42, episodes: 80\n",
      "3062: reward:  68.00, mean_100:  37.80, episodes: 81\n",
      "3147: reward:  85.00, mean_100:  38.38, episodes: 82\n",
      "3171: reward:  24.00, mean_100:  38.20, episodes: 83\n",
      "torch.Size([316, 2])\n",
      "tensor([[-0.5884, -0.8101],\n",
      "        [-1.0861, -0.4118],\n",
      "        [-0.5808, -0.8197],\n",
      "        [-1.0795, -0.4151],\n",
      "        [-0.5775, -0.8239],\n",
      "        [-1.0788, -0.4155],\n",
      "        [-0.5784, -0.8228],\n",
      "        [-1.0837, -0.4130],\n",
      "        [-0.5838, -0.8159],\n",
      "        [-1.0944, -0.4076],\n",
      "        [-0.5936, -0.8037],\n",
      "        [-1.1111, -0.3993],\n",
      "        [-0.6082, -0.7860],\n",
      "        [-0.2878, -1.3859],\n",
      "        [-0.6185, -0.7738],\n",
      "        [-1.1446, -0.3832],\n",
      "        [-1.8323, -0.1744],\n",
      "        [-1.1813, -0.3665],\n",
      "        [-1.8868, -0.1644],\n",
      "        [-1.2423, -0.3407],\n",
      "        [-1.9636, -0.1512],\n",
      "        [-1.3303, -0.3071],\n",
      "        [-0.8113, -0.5875],\n",
      "        [-1.4344, -0.2722],\n",
      "        [-0.8963, -0.5244],\n",
      "        [-1.5544, -0.2374],\n",
      "        [-0.9991, -0.4592],\n",
      "        [-1.6891, -0.2042],\n",
      "        [-1.1263, -0.3919],\n",
      "        [-0.6665, -0.7205],\n",
      "        [-1.2669, -0.3309],\n",
      "        [-0.7830, -0.6107],\n",
      "        [-1.4185, -0.2772],\n",
      "        [-2.1331, -0.1261],\n",
      "        [-1.6027, -0.2248],\n",
      "        [-1.0967, -0.4064],\n",
      "        [-1.7938, -0.1819],\n",
      "        [-1.2979, -0.3190],\n",
      "        [-0.8428, -0.5630],\n",
      "        [-0.4890, -0.9499],\n",
      "        [-1.0265, -0.4436],\n",
      "        [-0.6189, -0.7734],\n",
      "        [-0.3809, -1.1497],\n",
      "        [-0.2305, -1.5805],\n",
      "        [-0.4655, -0.9884],\n",
      "        [-0.9162, -0.5109],\n",
      "        [-1.5447, -0.2400],\n",
      "        [-1.0926, -0.4085],\n",
      "        [-1.7364, -0.1938],\n",
      "        [-1.2912, -0.3215],\n",
      "        [-0.8593, -0.5507],\n",
      "        [-0.5683, -0.8359],\n",
      "        [-0.3695, -1.1748],\n",
      "        [-0.6891, -0.6972],\n",
      "        [-0.4512, -1.0130],\n",
      "        [-0.8244, -0.5771],\n",
      "        [-0.5478, -0.8633],\n",
      "        [-0.3533, -1.2119],\n",
      "        [-0.6497, -0.7385],\n",
      "        [-1.1221, -0.3939],\n",
      "        [-0.7701, -0.6217],\n",
      "        [-0.5111, -0.9159],\n",
      "        [-0.9034, -0.5195],\n",
      "        [-1.4865, -0.2564],\n",
      "        [-1.0670, -0.4216],\n",
      "        [-0.7392, -0.6491],\n",
      "        [-1.2487, -0.3381],\n",
      "        [-1.9278, -0.1572],\n",
      "        [-0.5701, -0.8335],\n",
      "        [-0.2614, -1.4694],\n",
      "        [-0.1229, -2.1573],\n",
      "        [-0.2393, -1.5473],\n",
      "        [-0.4764, -0.9703],\n",
      "        [-0.8809, -0.5352],\n",
      "        [-1.4733, -0.2603],\n",
      "        [-0.8249, -0.5767],\n",
      "        [-1.4089, -0.2803],\n",
      "        [-0.7812, -0.6123],\n",
      "        [-0.3719, -1.1692],\n",
      "        [-0.7363, -0.6518],\n",
      "        [-0.3475, -1.2257],\n",
      "        [-0.1573, -1.9271],\n",
      "        [-0.0774, -2.5971],\n",
      "        [-0.1404, -2.0326],\n",
      "        [-0.2759, -1.4225],\n",
      "        [-0.5419, -0.8714],\n",
      "        [-0.2362, -1.5587],\n",
      "        [-0.4664, -0.9868],\n",
      "        [-0.8637, -0.5475],\n",
      "        [-0.3963, -1.1172],\n",
      "        [-0.1713, -1.8489],\n",
      "        [-0.3301, -1.2688],\n",
      "        [-0.6434, -0.7455],\n",
      "        [-1.1383, -0.3862],\n",
      "        [-0.5485, -0.8623],\n",
      "        [-1.0045, -0.4560],\n",
      "        [-0.4667, -0.9864],\n",
      "        [-0.8820, -0.5343],\n",
      "        [-0.3964, -1.1170],\n",
      "        [-0.1708, -1.8516],\n",
      "        [-0.3313, -1.2658],\n",
      "        [-0.6399, -0.7494],\n",
      "        [-1.1447, -0.3832],\n",
      "        [-1.7482, -0.1913],\n",
      "        [-1.0213, -0.4465],\n",
      "        [-0.4666, -0.9865],\n",
      "        [-0.9037, -0.5193],\n",
      "        [-0.4039, -1.1018],\n",
      "        [-0.1713, -1.8489],\n",
      "        [-0.3426, -1.2377],\n",
      "        [-0.6684, -0.7186],\n",
      "        [-1.1653, -0.3737],\n",
      "        [-0.5719, -0.8312],\n",
      "        [-0.2428, -1.5346],\n",
      "        [-0.1035, -2.3192],\n",
      "        [-0.1967, -1.7226],\n",
      "        [-0.3815, -1.1483],\n",
      "        [-0.7186, -0.6683],\n",
      "        [-1.1852, -0.3648],\n",
      "        [-1.7599, -0.1888],\n",
      "        [-1.0227, -0.4457],\n",
      "        [-1.5750, -0.2319],\n",
      "        [-0.8881, -0.5301],\n",
      "        [-0.4250, -1.0607],\n",
      "        [-0.7663, -0.6250],\n",
      "        [-0.3570, -1.2031],\n",
      "        [-0.6567, -0.7310],\n",
      "        [-1.1060, -0.4018],\n",
      "        [-0.5628, -0.8430],\n",
      "        [-0.2479, -1.5162],\n",
      "        [-0.4749, -0.9727],\n",
      "        [-0.1998, -1.7086],\n",
      "        [-0.3882, -1.1340],\n",
      "        [-0.6685, -0.7184],\n",
      "        [-1.0993, -0.4051],\n",
      "        [-0.5551, -0.8533],\n",
      "        [-0.2503, -1.5076],\n",
      "        [-0.0977, -2.3739],\n",
      "        [-0.1895, -1.7568],\n",
      "        [-0.3440, -1.2342],\n",
      "        [-0.5731, -0.8296],\n",
      "        [-0.9218, -0.5071],\n",
      "        [-1.3812, -0.2894],\n",
      "        [-0.7338, -0.6541],\n",
      "        [-1.1456, -0.3828],\n",
      "        [-0.5813, -0.8191],\n",
      "        [-0.9350, -0.4985],\n",
      "        [-1.3928, -0.2855],\n",
      "        [-0.7533, -0.6364],\n",
      "        [-0.3654, -1.1840],\n",
      "        [-0.6012, -0.7945],\n",
      "        [-0.2848, -1.3951],\n",
      "        [-0.4688, -0.9828],\n",
      "        [-0.6858, -0.7006],\n",
      "        [-1.2353, -0.3435],\n",
      "        [-1.9278, -0.1572],\n",
      "        [-1.2936, -0.3206],\n",
      "        [-0.7732, -0.6191],\n",
      "        [-0.3852, -1.1405],\n",
      "        [-0.8243, -0.5772],\n",
      "        [-1.4271, -0.2745],\n",
      "        [-0.8858, -0.5317],\n",
      "        [-1.5134, -0.2487],\n",
      "        [-0.9639, -0.4803],\n",
      "        [-1.6182, -0.2210],\n",
      "        [-2.3349, -0.1018],\n",
      "        [-3.0623, -0.0479],\n",
      "        [-2.5068, -0.0850],\n",
      "        [-1.9582, -0.1521],\n",
      "        [-1.4321, -0.2729],\n",
      "        [-2.1607, -0.1224],\n",
      "        [-1.6651, -0.2097],\n",
      "        [-1.1779, -0.3681],\n",
      "        [-1.8910, -0.1636],\n",
      "        [-1.4219, -0.2761],\n",
      "        [-0.9858, -0.4671],\n",
      "        [-1.6589, -0.2112],\n",
      "        [-0.6154, -0.7775],\n",
      "        [-1.1521, -0.3797],\n",
      "        [-0.6500, -0.7382],\n",
      "        [-0.3108, -1.3199],\n",
      "        [-0.6813, -0.7052],\n",
      "        [-1.2375, -0.3427],\n",
      "        [-0.7207, -0.6663],\n",
      "        [-1.2917, -0.3213],\n",
      "        [-0.7685, -0.6231],\n",
      "        [-0.3786, -1.1547],\n",
      "        [-0.1893, -1.7577],\n",
      "        [-0.4000, -1.1097],\n",
      "        [-0.8399, -0.5652],\n",
      "        [-0.4194, -1.0714],\n",
      "        [-0.8705, -0.5425],\n",
      "        [-0.4395, -1.0338],\n",
      "        [-0.2136, -1.6487],\n",
      "        [-0.4547, -1.0068],\n",
      "        [-0.2177, -1.6314],\n",
      "        [-0.4624, -0.9936],\n",
      "        [-0.9202, -0.5083],\n",
      "        [-0.4701, -0.9807],\n",
      "        [-0.2222, -1.6133],\n",
      "        [-0.4718, -0.9779],\n",
      "        [-0.9275, -0.5034],\n",
      "        [-1.5226, -0.2461],\n",
      "        [-0.9435, -0.4931],\n",
      "        [-0.4893, -0.9495],\n",
      "        [-0.2317, -1.5761],\n",
      "        [-0.4962, -0.9387],\n",
      "        [-0.2327, -1.5719],\n",
      "        [-0.4959, -0.9391],\n",
      "        [-0.2307, -1.5797],\n",
      "        [-0.4889, -0.9502],\n",
      "        [-0.2257, -1.5993],\n",
      "        [-0.4752, -0.9722],\n",
      "        [-0.2183, -1.6289],\n",
      "        [-0.4558, -1.0050],\n",
      "        [-0.2086, -1.6700],\n",
      "        [-0.4303, -1.0507],\n",
      "        [-0.8346, -0.5692],\n",
      "        [-0.4059, -1.0977],\n",
      "        [-0.7977, -0.5985],\n",
      "        [-0.3838, -1.1434],\n",
      "        [-0.7612, -0.6294],\n",
      "        [-0.3633, -1.1886],\n",
      "        [-0.7251, -0.6622],\n",
      "        [-0.3432, -1.2361],\n",
      "        [-0.6880, -0.6983],\n",
      "        [-1.1930, -0.3614],\n",
      "        [-0.6595, -0.7280],\n",
      "        [-0.3089, -1.3252],\n",
      "        [-0.6288, -0.7619],\n",
      "        [-1.1163, -0.3968],\n",
      "        [-0.6044, -0.7905],\n",
      "        [-1.0877, -0.4110],\n",
      "        [-0.5862, -0.8129],\n",
      "        [-1.0671, -0.4216],\n",
      "        [-0.5733, -0.8293],\n",
      "        [-1.0531, -0.4290],\n",
      "        [-0.5648, -0.8404],\n",
      "        [-1.0447, -0.4335],\n",
      "        [-1.6868, -0.2047],\n",
      "        [-1.0568, -0.4270],\n",
      "        [-1.7118, -0.1991],\n",
      "        [-1.0872, -0.4112],\n",
      "        [-0.6026, -0.7927],\n",
      "        [-1.1236, -0.3932],\n",
      "        [-1.8037, -0.1800],\n",
      "        [-1.1822, -0.3662],\n",
      "        [-0.6857, -0.7006],\n",
      "        [-0.3358, -1.2545],\n",
      "        [-0.1679, -1.8672],\n",
      "        [-0.3570, -1.2032],\n",
      "        [-0.7596, -0.6308],\n",
      "        [-0.3758, -1.1607],\n",
      "        [-0.7910, -0.6040],\n",
      "        [-1.3714, -0.2927],\n",
      "        [-0.8344, -0.5694],\n",
      "        [-0.4249, -1.0608],\n",
      "        [-0.2048, -1.6862],\n",
      "        [-0.1072, -2.2861],\n",
      "        [-0.2096, -1.6653],\n",
      "        [-0.1072, -2.2861],\n",
      "        [-0.2077, -1.6736],\n",
      "        [-0.4391, -1.0345],\n",
      "        [-0.8584, -0.5514],\n",
      "        [-0.4304, -1.0506],\n",
      "        [-0.1986, -1.7143],\n",
      "        [-0.0979, -2.3727],\n",
      "        [-0.1883, -1.7624],\n",
      "        [-0.3852, -1.1405],\n",
      "        [-0.7530, -0.6367],\n",
      "        [-1.2741, -0.3281],\n",
      "        [-1.9087, -0.1605],\n",
      "        [-1.2498, -0.3377],\n",
      "        [-0.7102, -0.6763],\n",
      "        [-1.2383, -0.3423],\n",
      "        [-0.7059, -0.6806],\n",
      "        [-1.2382, -0.3423],\n",
      "        [-0.7101, -0.6765],\n",
      "        [-1.2495, -0.3378],\n",
      "        [-0.7229, -0.6643],\n",
      "        [-1.2724, -0.3287],\n",
      "        [-0.7448, -0.6441],\n",
      "        [-1.3075, -0.3154],\n",
      "        [-0.7779, -0.6150],\n",
      "        [-0.3910, -1.1282],\n",
      "        [-0.8107, -0.5880],\n",
      "        [-1.3962, -0.2844],\n",
      "        [-2.1012, -0.1305],\n",
      "        [-1.4762, -0.2594],\n",
      "        [-2.2029, -0.1171],\n",
      "        [-2.9316, -0.0548],\n",
      "        [-2.3527, -0.0999],\n",
      "        [-3.1135, -0.0455],\n",
      "        [-2.5555, -0.0808],\n",
      "        [-2.0124, -0.1435],\n",
      "        [-1.4949, -0.2539],\n",
      "        [-1.0273, -0.4431],\n",
      "        [-0.6260, -0.7652],\n",
      "        [-0.3646, -1.1858],\n",
      "        [-0.7969, -0.5992],\n",
      "        [-1.4500, -0.2673],\n",
      "        [-0.9974, -0.4602],\n",
      "        [-0.6080, -0.7863],\n",
      "        [-0.3807, -1.1502],\n",
      "        [-0.7672, -0.6242],\n",
      "        [-0.4837, -0.9584],\n",
      "        [-0.9579, -0.4840],\n",
      "        [-0.6041, -0.7909],\n",
      "        [-1.1602, -0.3760],\n",
      "        [-0.7547, -0.6352],\n",
      "        [-0.4904, -0.9477],\n",
      "        [-0.9225, -0.5067],\n",
      "        [-0.6111, -0.7825],\n",
      "        [-1.1139, -0.3979],\n",
      "        [-1.8097, -0.1788],\n",
      "        [-1.3510, -0.2997]], grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([316])\n",
      "316\n",
      "torch.Size([316])\n",
      "3310: reward: 139.00, mean_100:  39.40, episodes: 84\n",
      "3408: reward:  98.00, mean_100:  40.09, episodes: 85\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m batch_states, batch_actions, batch_qvals \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m     59\u001b[0m cur_rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step_idx, exp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(exp_source):\n\u001b[1;32m     62\u001b[0m     batch_states\u001b[38;5;241m.\u001b[39mappend(exp\u001b[38;5;241m.\u001b[39mstate)\n\u001b[1;32m     63\u001b[0m     batch_actions\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mint\u001b[39m(exp\u001b[38;5;241m.\u001b[39maction))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/ptan/experience.py:176\u001b[0m, in \u001b[0;36mExperienceSourceFirstLast.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(ExperienceSourceFirstLast, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m():\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m exp[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(exp) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps:\n\u001b[1;32m    178\u001b[0m             last_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/ptan/experience.py:82\u001b[0m, in \u001b[0;36mExperienceSource.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m         states_indices\u001b[38;5;241m.\u001b[39mappend(idx)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m states_input:\n\u001b[0;32m---> 82\u001b[0m     states_actions, new_agent_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(states_actions):\n\u001b[1;32m     84\u001b[0m         g_idx \u001b[38;5;241m=\u001b[39m states_indices[idx]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/ptan/agent.py:126\u001b[0m, in \u001b[0;36mPolicyAgent.__call__\u001b[0;34m(self, states, agent_states)\u001b[0m\n\u001b[1;32m    124\u001b[0m     agent_states \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(states)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(states):\n\u001b[1;32m    128\u001b[0m         states \u001b[38;5;241m=\u001b[39m states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/ptan/agent.py:52\u001b[0m, in \u001b[0;36mfloat32_preprocessor\u001b[0;34m(states)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfloat32_preprocessor\u001b[39m(states):\n\u001b[1;32m     51\u001b[0m     np_states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(states, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp_states\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "EPISODES_TO_TRAIN = 4\n",
    "\n",
    "\n",
    "class PGN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def calc_qvals(rewards):\n",
    "    res = []\n",
    "    sum_r = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= GAMMA\n",
    "        sum_r += r\n",
    "        res.append(sum_r)\n",
    "    return list(reversed(res))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    writer = SummaryWriter(comment=\"-cartpole-reinforce\")\n",
    "\n",
    "    net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "    print(net)\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor,\n",
    "                                   apply_softmax=True)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    total_rewards = []\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "\n",
    "    batch_episodes = 0\n",
    "    batch_states, batch_actions, batch_qvals = [], [], []\n",
    "    cur_rewards = []\n",
    "\n",
    "    for step_idx, exp in enumerate(exp_source):\n",
    "        batch_states.append(exp.state)\n",
    "        batch_actions.append(int(exp.action))\n",
    "        cur_rewards.append(exp.reward)\n",
    "\n",
    "        if exp.last_state is None:\n",
    "            batch_qvals.extend(calc_qvals(cur_rewards))\n",
    "            cur_rewards.clear()\n",
    "            batch_episodes += 1\n",
    "\n",
    "        # handle new rewards\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            done_episodes += 1\n",
    "            reward = new_rewards[0]\n",
    "            total_rewards.append(reward)\n",
    "            mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "            print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\" % (\n",
    "                step_idx, reward, mean_rewards, done_episodes))\n",
    "            writer.add_scalar(\"reward\", reward, step_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "            writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "            if mean_rewards > 195:\n",
    "                print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "                break\n",
    "\n",
    "        if batch_episodes < EPISODES_TO_TRAIN:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        states_v = torch.FloatTensor(batch_states)\n",
    "        batch_actions_t = torch.LongTensor(batch_actions)\n",
    "        batch_qvals_v = torch.FloatTensor(batch_qvals)\n",
    "\n",
    "        logits_v = net(states_v)\n",
    "        \n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        print(log_prob_v.shape)\n",
    "        print(log_prob_v)\n",
    "        print(batch_qvals_v.shape)\n",
    "        print(len(batch_states))\n",
    "        print(batch_actions_t.shape)\n",
    "        log_prob_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]\n",
    "        loss_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_episodes = 0\n",
    "        batch_states.clear()\n",
    "        batch_actions.clear()\n",
    "        batch_qvals.clear()\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b3711e9-9b1b-426d-99d2-1ac4bc812731",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m log_prob_v \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(logits_v, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mlog_prob_v\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 1 with size 2"
     ]
    }
   ],
   "source": [
    "log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "a = log_prob_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30dc548a-a38a-4bd6-b0f1-0588e65d6102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range(0, 5), tensor([1, 2, 3, 4, 5])]\n"
     ]
    }
   ],
   "source": [
    "print([range(5), torch.LongTensor([1,2,3,4,5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5675e80a-e008-47d1-ac51-1b29497c78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.FloatTensor([[0.1, 0.8], [0.3, 0.7], [0.5, 0.4]])\n",
    "b = a[range(3), [1, 1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6843c1fd-f595-4f4a-bffd-7bc3959d38a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8000, 0.7000, 0.5000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8134b31-aba3-40cf-bc88-0c5d67d7bab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-gen_env]",
   "language": "python",
   "name": "conda-env-.conda-gen_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
