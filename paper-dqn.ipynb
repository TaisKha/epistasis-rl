{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8349b96-3e4a-4bbc-be58-b4888b6d78c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtaisikus\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/tskhakharova/.local/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:47: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tskhakharova/epistasis-rl/wandb/run-20220813_134257-16wffx6d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/taisikus/epistasis/runs/16wffx6d\" target=\"_blank\">scarlet-star-118</a></strong> to <a href=\"https://wandb.ai/taisikus/epistasis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease snps are found in 0:05:25, after 248 episodes and 149 iterations!\n",
      "Disease snps are found in 0:05:46, after 258 episodes and 159 iterations!\n",
      "Disease snps are found in 0:05:52, after 261 episodes and 162 iterations!\n",
      "Disease snps are found in 0:06:04, after 267 episodes and 168 iterations!\n",
      "Disease snps are found in 0:06:08, after 269 episodes and 170 iterations!\n",
      "Disease snps are found in 0:06:21, after 275 episodes and 176 iterations!\n",
      "Disease snps are found in 0:06:29, after 279 episodes and 180 iterations!\n",
      "Disease snps are found in 0:06:39, after 284 episodes and 185 iterations!\n",
      "Disease snps are found in 0:06:56, after 292 episodes and 193 iterations!\n",
      "Disease snps are found in 0:07:29, after 308 episodes and 209 iterations!\n",
      "Disease snps are found in 0:07:31, after 309 episodes and 210 iterations!\n",
      "Disease snps are found in 0:07:41, after 314 episodes and 215 iterations!\n",
      "Disease snps are found in 0:07:48, after 317 episodes and 218 iterations!\n",
      "Disease snps are found in 0:07:50, after 318 episodes and 219 iterations!\n",
      "Disease snps are found in 0:07:54, after 320 episodes and 221 iterations!\n",
      "Disease snps are found in 0:08:01, after 323 episodes and 224 iterations!\n",
      "Disease snps are found in 0:08:07, after 326 episodes and 227 iterations!\n",
      "Disease snps are found in 0:08:11, after 328 episodes and 229 iterations!\n",
      "Disease snps are found in 0:08:13, after 329 episodes and 230 iterations!\n",
      "Disease snps are found in 0:08:25, after 335 episodes and 236 iterations!\n",
      "Disease snps are found in 0:08:30, after 337 episodes and 238 iterations!\n",
      "Disease snps are found in 0:08:40, after 342 episodes and 243 iterations!\n",
      "Disease snps are found in 0:08:42, after 343 episodes and 244 iterations!\n",
      "Disease snps are found in 0:08:46, after 345 episodes and 246 iterations!\n",
      "Disease snps are found in 0:08:57, after 350 episodes and 251 iterations!\n",
      "Disease snps are found in 0:09:22, after 362 episodes and 263 iterations!\n",
      "Disease snps are found in 0:09:28, after 365 episodes and 266 iterations!\n",
      "Disease snps are found in 0:09:32, after 367 episodes and 268 iterations!\n",
      "Disease snps are found in 0:09:49, after 375 episodes and 276 iterations!\n",
      "Disease snps are found in 0:09:51, after 376 episodes and 277 iterations!\n",
      "Disease snps are found in 0:09:53, after 377 episodes and 278 iterations!\n",
      "Disease snps are found in 0:09:57, after 379 episodes and 280 iterations!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Engine run is terminating due to exception: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 660>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    710\u001b[0m setup_ignite(engine, params, exp_source, NAME, net)\n\u001b[1;32m    712\u001b[0m env\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m engine\n\u001b[0;32m--> 714\u001b[0m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_initial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/ignite/engine/engine.py:718\u001b[0m, in \u001b[0;36mEngine.run\u001b[0;34m(self, data, max_epochs, max_iters, epoch_length)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_length should be provided if data is None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdataloader \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m--> 718\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/ignite/engine/engine.py:797\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine run is terminating due to exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 797\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/ignite/engine/engine.py:467\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/ignite/engine/engine.py:767\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_engine()\n\u001b[0;32m--> 767\u001b[0m time_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once_on_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;66;03m# time is available for handlers but must be update after fire\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtimes[Events\u001b[38;5;241m.\u001b[39mEPOCH_COMPLETED\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m time_taken\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/ignite/engine/engine.py:859\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39miteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_STARTED)\n\u001b[0;32m--> 859\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_COMPLETED)\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_terminate \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_terminate_single_epoch:\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_batch\u001b[39m(engine, batch):\n\u001b[1;32m    695\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 696\u001b[0m     loss_v \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_dqn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m     loss_v\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    700\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mcalc_loss_dqn\u001b[0;34m(batch, net, tgt_net, gamma, device)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m#     actions_v = actions_v.unsqueeze(-1)\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m#     state_action_vals = net(states_v).gather(1, actions_v)\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m#     state_action_vals = state_action_vals.squeeze(-1)\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 241\u001b[0m         next_state_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtgt_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states_v\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    242\u001b[0m         next_state_vals[done_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    244\u001b[0m     bellman_vals \u001b[38;5;241m=\u001b[39m next_state_vals\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m*\u001b[39m gamma \u001b[38;5;241m+\u001b[39m rewards_v\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:150\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataParallel.forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids:\n\u001b[0;32m--> 150\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mbuffers()):\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;66;03m# fx = x.float() / 2\u001b[39;00m\n\u001b[1;32m    592\u001b[0m     fx \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 593\u001b[0m     conv_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(fx\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(conv_out)\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gen_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    441\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    442\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## dqn абсолютно такой же как в статье\n",
    "# без нормализации награды\n",
    "# все время один и тот же файл с данными\n",
    "# EPISODE_LENGTH=1\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import ptan\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from typing import Iterable, Tuple, List\n",
    "import warnings\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# import ptan_ignite\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage\n",
    "# from ignite.contrib.handlers import tensorboard_logger as tb_logger\n",
    "\n",
    "\n",
    "import enum\n",
    "import time\n",
    "from typing import Optional\n",
    "from ignite.engine import Engine, State\n",
    "from ignite.engine import Events, EventEnum\n",
    "from ignite.handlers.timing import Timer\n",
    "from ignite.contrib.handlers.base_logger import BaseOutputHandler\n",
    "from ignite.contrib.handlers.wandb_logger import WandBLogger\n",
    "from typing import Any, Callable, List, Optional, Union\n",
    "\n",
    "EPISODE_LENGTH = 1\n",
    "\n",
    "class OutputHandler(BaseOutputHandler):\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tag: str,\n",
    "        metric_names: Optional[List[str]] = None,\n",
    "        output_transform: Optional[Callable] = None,\n",
    "        global_step_transform: Optional[Callable] = None,\n",
    "        sync: Optional[bool] = None,\n",
    "        state_attributes: Optional[List[str]] = None,\n",
    "    ):\n",
    "        super().__init__(tag, metric_names, output_transform, global_step_transform, state_attributes)\n",
    "        self.sync = sync\n",
    "\n",
    "    def __call__(self, engine: Engine, logger: WandBLogger, event_name: Union[str, Events]) -> None:\n",
    "\n",
    "        if not isinstance(logger, WandBLogger):\n",
    "            raise RuntimeError(f\"Handler '{self.__class__.__name__}' works only with WandBLogger.\")\n",
    "\n",
    "        global_step = self.global_step_transform(engine, event_name)  # type: ignore[misc]\n",
    "        if not isinstance(global_step, int):\n",
    "            raise TypeError(\n",
    "                f\"global_step must be int, got {type(global_step)}.\"\n",
    "                \" Please check the output of global_step_transform.\"\n",
    "            )\n",
    "\n",
    "        metrics = self._setup_output_metrics_state_attrs(engine, log_text=True, key_tuple=False)\n",
    "        logger.log(metrics, sync=self.sync)\n",
    "\n",
    "\n",
    "class EpisodeEvents(EventEnum):\n",
    "    EPISODE_COMPLETED = \"episode_completed\"\n",
    "    BOUND_REWARD_REACHED = \"bound_reward_reached\"\n",
    "    BEST_REWARD_REACHED = \"best_reward_reached\"\n",
    "\n",
    "\n",
    "class EndOfEpisodeHandler:\n",
    "    def __init__(self, exp_source: ptan.experience.ExperienceSource, alpha: float = 0.98,\n",
    "                 bound_avg_reward: Optional[float] = None,\n",
    "                 subsample_end_of_episode: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Construct end-of-episode event handler\n",
    "        :param exp_source: experience source to use\n",
    "        :param alpha: smoothing alpha param\n",
    "        :param bound_avg_reward: optional boundary for average reward\n",
    "        :param subsample_end_of_episode: if given, end of episode event will be subsampled by this amount\n",
    "        \"\"\"\n",
    "        self._exp_source = exp_source\n",
    "        self._alpha = alpha\n",
    "        self._bound_avg_reward = bound_avg_reward\n",
    "        self._best_avg_reward = None\n",
    "        self._subsample_end_of_episode = subsample_end_of_episode\n",
    "\n",
    "    def attach(self, engine: Engine):\n",
    "        engine.add_event_handler(Events.ITERATION_COMPLETED, self)\n",
    "        engine.register_events(*EpisodeEvents)\n",
    "        State.event_to_attr[EpisodeEvents.EPISODE_COMPLETED] = \"episode\"\n",
    "        State.event_to_attr[EpisodeEvents.BOUND_REWARD_REACHED] = \"episode\"\n",
    "        State.event_to_attr[EpisodeEvents.BEST_REWARD_REACHED] = \"episode\"\n",
    "\n",
    "    def __call__(self, engine: Engine):\n",
    "        for reward, steps in self._exp_source.pop_rewards_steps():\n",
    "            engine.state.episode = getattr(engine.state, \"episode\", 0) + 1\n",
    "            engine.state.episode_reward = reward\n",
    "            engine.state.episode_steps = steps\n",
    "            engine.state.metrics['reward'] = reward\n",
    "            engine.state.metrics['steps'] = steps\n",
    "            engine.state.metrics['episode_number'] = engine.state.episode\n",
    "            self._update_smoothed_metrics(engine, reward, steps)\n",
    "            if self._subsample_end_of_episode is None or engine.state.episode % self._subsample_end_of_episode == 0:\n",
    "                engine.fire_event(EpisodeEvents.EPISODE_COMPLETED)\n",
    "            if self._bound_avg_reward is not None and engine.state.metrics['avg_reward'] >= self._bound_avg_reward:\n",
    "                engine.fire_event(EpisodeEvents.BOUND_REWARD_REACHED)\n",
    "            if self._best_avg_reward is None:\n",
    "                self._best_avg_reward = engine.state.metrics['avg_reward']\n",
    "            elif self._best_avg_reward < engine.state.metrics['avg_reward']:\n",
    "                engine.fire_event(EpisodeEvents.BEST_REWARD_REACHED)\n",
    "                self._best_avg_reward = engine.state.metrics['avg_reward']\n",
    "\n",
    "    def _update_smoothed_metrics(self, engine: Engine, reward: float, steps: int):\n",
    "        for attr_name, val in zip(('avg_reward', 'avg_steps'), (reward, steps)):\n",
    "            if attr_name not in engine.state.metrics:\n",
    "                engine.state.metrics[attr_name] = val\n",
    "            else:\n",
    "                engine.state.metrics[attr_name] *= self._alpha\n",
    "                engine.state.metrics[attr_name] += (1-self._alpha) * val\n",
    "\n",
    "\n",
    "class EpisodeFPSHandler:\n",
    "    FPS_METRIC = 'fps'\n",
    "    AVG_FPS_METRIC = 'avg_fps'\n",
    "    TIME_PASSED_METRIC = 'time_passed'\n",
    "\n",
    "    def __init__(self, fps_mul: float = 1.0, fps_smooth_alpha: float = 0.98):\n",
    "        self._timer = Timer(average=True)\n",
    "        self._fps_mul = fps_mul\n",
    "        self._started_ts = time.time()\n",
    "        self._fps_smooth_alpha = fps_smooth_alpha\n",
    "\n",
    "    def attach(self, engine: Engine, manual_step: bool = False):\n",
    "        self._timer.attach(engine, step=None if manual_step else Events.ITERATION_COMPLETED)\n",
    "        engine.add_event_handler(EpisodeEvents.EPISODE_COMPLETED, self)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        If manual_step=True on attach(), this method should be used every time we've communicated with environment\n",
    "        to get proper FPS\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._timer.step()\n",
    "\n",
    "    def __call__(self, engine: Engine):\n",
    "        t_val = self._timer.value()\n",
    "        if engine.state.iteration > 1:\n",
    "            fps = self._fps_mul / t_val\n",
    "            avg_fps = engine.state.metrics.get(self.AVG_FPS_METRIC)\n",
    "            if avg_fps is None:\n",
    "                avg_fps = fps\n",
    "            else:\n",
    "                avg_fps *= self._fps_smooth_alpha\n",
    "                avg_fps += (1-self._fps_smooth_alpha) * fps\n",
    "            engine.state.metrics[self.AVG_FPS_METRIC] = avg_fps\n",
    "            engine.state.metrics[self.FPS_METRIC] = fps\n",
    "        engine.state.metrics[self.TIME_PASSED_METRIC] = time.time() - self._started_ts\n",
    "        self._timer.reset()\n",
    "\n",
    "\n",
    "class PeriodEvents(EventEnum):\n",
    "    ITERS_10_COMPLETED = \"iterations_10_completed\"\n",
    "    ITERS_100_COMPLETED = \"iterations_100_completed\"\n",
    "    ITERS_1000_COMPLETED = \"iterations_1000_completed\"\n",
    "    ITERS_10000_COMPLETED = \"iterations_10000_completed\"\n",
    "    ITERS_100000_COMPLETED = \"iterations_100000_completed\"\n",
    "\n",
    "\n",
    "class PeriodicEvents:\n",
    "    \"\"\"\n",
    "    The same as CustomPeriodicEvent from ignite.contrib, but use true amount of iterations,\n",
    "    which is good for TensorBoard\n",
    "    \"\"\"\n",
    "\n",
    "    INTERVAL_TO_EVENT = {\n",
    "        10: PeriodEvents.ITERS_10_COMPLETED,\n",
    "        100: PeriodEvents.ITERS_100_COMPLETED,\n",
    "        1000: PeriodEvents.ITERS_1000_COMPLETED,\n",
    "        10000: PeriodEvents.ITERS_10000_COMPLETED,\n",
    "        100000: PeriodEvents.ITERS_100000_COMPLETED,\n",
    "    }\n",
    "\n",
    "    def attach(self, engine: Engine):\n",
    "        engine.add_event_handler(Events.ITERATION_COMPLETED, self)\n",
    "        engine.register_events(*PeriodEvents)\n",
    "        for e in PeriodEvents:\n",
    "            State.event_to_attr[e] = \"iteration\"\n",
    "\n",
    "    def __call__(self, engine: Engine):\n",
    "        for period, event in self.INTERVAL_TO_EVENT.items():\n",
    "            if engine.state.iteration % period == 0:\n",
    "                engine.fire_event(event)\n",
    "\n",
    "\n",
    "\n",
    "def unpack_batch(batch: List[ptan.experience.ExperienceFirstLast]):\n",
    "    states, actions, rewards, dones, last_states = [],[],[],[],[]\n",
    "    for exp in batch:\n",
    "        state = np.array(exp.state)\n",
    "        states.append(state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        dones.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            lstate = state  # the result will be masked anyway\n",
    "        else:\n",
    "            lstate = np.array(exp.last_state)\n",
    "        last_states.append(lstate)\n",
    "    return np.array(states, copy=False), np.array(actions), \\\n",
    "           np.array(rewards, dtype=np.float32), \\\n",
    "           np.array(dones, dtype=np.uint8), \\\n",
    "           np.array(last_states, copy=False)\n",
    "\n",
    "                \n",
    "def calc_loss_dqn(batch, net, tgt_net, gamma, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = \\\n",
    "        unpack_batch(batch)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "    state_action_vals = torch.sum(actions_v * net(states_v), dim=1, dtype=torch.float32)\n",
    "#     actions_v = actions_v.unsqueeze(-1)\n",
    "    \n",
    "#     state_action_vals = net(states_v).gather(1, actions_v)\n",
    "#     state_action_vals = state_action_vals.squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_state_vals = tgt_net(next_states_v).max(1)[0]\n",
    "        next_state_vals[done_mask] = 0.0\n",
    "\n",
    "    bellman_vals = next_state_vals.detach() * gamma + rewards_v\n",
    "    return nn.MSELoss()(state_action_vals, bellman_vals)\n",
    "\n",
    "class EpistasisEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.SAMPLE_SIZE = 600 #t1 = t2 = SAMPLE_SIZE\n",
    "        self.reset()\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.N_SNPS,), dtype=np.uint8)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=\n",
    "                        (3, 2*self.SAMPLE_SIZE, self.N_SNPS), dtype=np.uint8)\n",
    "        \n",
    "        \n",
    "    def establish_phen_gen(self, file):\n",
    "        with open(file) as f:\n",
    "            data = json.load(f)\n",
    "            genotype = np.array(data[\"genotype\"])\n",
    "            self.phenotype = np.array(data[\"phenotype\"])\n",
    "            self.genotype = genotype.T\n",
    "            num_phenotypes = max(self.phenotype)+1\n",
    "            self.disease_snps = data[\"disease_snps\"]\n",
    "            self.phen_gen = [[] for _ in range(num_phenotypes)]\n",
    "            for i in range(len(self.genotype)):\n",
    "                self.phen_gen[self.phenotype[i]].append(i)  \n",
    "            return  self.genotype.shape[0], self.genotype.shape[1]\n",
    "        \n",
    "    def normalize_reward(self, current_reward):\n",
    "        maximum_env_reward = self._count_reward(self.disease_snps)\n",
    "        minimal_reward = 0.5\n",
    "        normalized_reward = (current_reward - minimal_reward) / (maximum_env_reward - minimal_reward)\n",
    "        if normalized_reward > 1:\n",
    "            print(\"normalized reward > 1: \\n normalized reward = \", normalized_reward, \"\\n current reward = \", current_reward, \"\\n maximum_env_reward = \", maximum_env_reward )\n",
    "            normalized_reward = 0.1\n",
    "        return normalized_reward\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        snp_ids = self._take_action(action)\n",
    "        reward = self._count_reward(snp_ids)\n",
    "#         без нормализации\n",
    "        # reward = self.normalize_reward(reward)\n",
    "        \n",
    "        self.current_step += 1\n",
    "        if self.current_step == EPISODE_LENGTH:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False  \n",
    "        # done = self.current_step == 1\n",
    "        obs = None if done else self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def _count_reward(self, snp_ids):\n",
    "        \n",
    "        all_existing_seq = defaultdict(lambda: {'control' : 0, 'case' : 0})\n",
    "        for i, idv in enumerate(self.obs):\n",
    "            snp_to_cmp = tuple(idv[snp_id] for snp_id in snp_ids) #tuple of SNP that \n",
    "            if self.obs_phenotypes[i] == 0:\n",
    "                all_existing_seq[snp_to_cmp]['control'] += 1\n",
    "            else:\n",
    "                all_existing_seq[snp_to_cmp]['case'] += 1\n",
    "\n",
    "        #count reward      \n",
    "        TP = 0 #HR case\n",
    "        FP = 0 #HR control\n",
    "        TN = 0 #LR control\n",
    "        FN = 0 #LR case\n",
    "\n",
    "        for case_control_count in all_existing_seq.values():\n",
    "          # if seq is in LR group\n",
    "            if case_control_count['case'] <= case_control_count['control']: #вопрос <= или <\n",
    "                FN += case_control_count['case']\n",
    "                TN += case_control_count['control']\n",
    "            else:\n",
    "          # if seq is in HR group\n",
    "                TP += case_control_count['case']\n",
    "                FP += case_control_count['control']\n",
    "        R = (FP + TN) / (TP + FN)\n",
    "        delta = FP / (TP+0.001)\n",
    "        gamma = (TP + FP + TN + FN) / (TP+0.001)\n",
    "        CCR = 0.5 * (TP / (TP + FN) + TN / (FP + TN))\n",
    "        U = (R - delta)**2 / ((1 + delta) * (gamma - delta - 1 + 0.001))\n",
    "        koef = 1\n",
    "        if len(snp_ids) > len(self.disease_snps):\n",
    "                print(\"len(snp_ids) > len(self.disease_snps)\")\n",
    "                koef = 1 / len(snp_ids)\n",
    "\n",
    "        return koef*(CCR + U)\n",
    "\n",
    "  \n",
    "    def reset(self):\n",
    "        \n",
    "        pops = [\"ASW\", \"CEU\", \"CEU+TSI\", \"CHD\", \"GIH\", \"JPT+CHB\", \"LWK\", \"MEX\", \"MKK\", \"TSI\"]\n",
    "        sim_idx = np.random.randint(2500)\n",
    "        corp_idx = np.random.randint(1, 23)\n",
    "        pop_idx = np.random.choice(pops)\n",
    "        \n",
    "        self.filename = f\"/home/tskhakharova/epistasis-rl/epigen/sim/{sim_idx}_{corp_idx}_{pop_idx}.json\"\n",
    "        # filename = f\"/home/tskhakharova/epistasis-rl/epigen/sim/5_7_CEU.json\"\n",
    "        if not os.path.exists(self.filename):\n",
    "            os.system(f\"cd /home/tskhakharova/epistasis-rl/epigen/ && python3 simulate_data.py --sim-ids {sim_idx} --corpus-id {corp_idx} --pop {pop_idx} --inds 5000 --snps 100 --model models/ext_model.ini\")\n",
    "\n",
    "        self.N_IDV, self.N_SNPS = self.establish_phen_gen(self.filename)\n",
    "        \n",
    "        self.obs_phenotypes = None\n",
    "        one_hot_obs = self._next_observation()\n",
    "        self.current_step = 0\n",
    "        \n",
    "        return one_hot_obs\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        chosen_snp_ids = []\n",
    "        for i, choice in enumerate(action):\n",
    "            if choice == 1:\n",
    "                chosen_snp_ids.append(i)\n",
    "        return chosen_snp_ids    \n",
    "    \n",
    "    def _next_observation(self):\n",
    "        id_0 = np.random.choice(self.phen_gen[0], self.SAMPLE_SIZE)\n",
    "        id_1 = np.random.choice(self.phen_gen[1], self.SAMPLE_SIZE)\n",
    "        sample_ids = np.array(list(zip(id_0,id_1))).flatten()\n",
    "        self.obs = np.array([self.genotype[idv] for idv in sample_ids])\n",
    "        self.obs_phenotypes = [self.phenotype[idv] for idv in sample_ids]\n",
    "        \n",
    "        #one_hot\n",
    "        one_hot_obs = F.one_hot(torch.tensor(self.obs), 3)\n",
    "        one_hot_obs = one_hot_obs.movedim(2, 0)\n",
    "\n",
    "        return one_hot_obs\n",
    "    \n",
    "class FixedEpistasisEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, sample_size, n_snps, observation_onehot, filename, observation, obs_phenotypes, disease_snps):\n",
    "        self.one_hot_obs = observation_onehot\n",
    "        self.filename = filename\n",
    "        self.obs = observation\n",
    "        self.obs_phenotypes = obs_phenotypes\n",
    "        self.disease_snps = disease_snps\n",
    "        \n",
    "        self.SAMPLE_SIZE = sample_size #t1 = t2 = SAMPLE_SIZE\n",
    "        self.N_SNPS = n_snps\n",
    "        \n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.N_SNPS,), dtype=np.uint8)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=\n",
    "                        (3, 2*self.SAMPLE_SIZE, self.N_SNPS), dtype=np.uint8)\n",
    "        self.engine = None\n",
    "        \n",
    "        \n",
    "    def normalize_reward(self, current_reward):\n",
    "        maximum_env_reward = self._count_reward(self.disease_snps)\n",
    "        minimal_reward = 0.5\n",
    "        normalized_reward = (current_reward - minimal_reward) / (maximum_env_reward - minimal_reward)\n",
    "        if normalized_reward > 1:\n",
    "            print(\"normalized reward > 1: \\n normalized reward = \", normalized_reward, \"\\n current reward = \", current_reward, \"\\n maximum_env_reward = \", maximum_env_reward )\n",
    "            normalized_reward = 0.1\n",
    "        return normalized_reward\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        snp_ids = self._take_action(action)\n",
    "        reward = self._count_reward(snp_ids)\n",
    "#         без нормализации\n",
    "        # reward = self.normalize_reward(reward)\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = self.current_step == EPISODE_LENGTH\n",
    "        return self.one_hot_obs, reward, done, {}\n",
    "    \n",
    "    def _count_reward(self, snp_ids):\n",
    "        \n",
    "        if set(snp_ids) == set(self.disease_snps):\n",
    "            print(\"Disease snps are found\", end)\n",
    "            passed = engine.state.metrics['time_passed']\n",
    "            print(\"in %s, after %d episodes \"\n",
    "                  \"and %d iterations!\" % (\n",
    "                timedelta(seconds=int(passed)),\n",
    "                engine.state.episode, engine.state.iteration))\n",
    "        \n",
    "        all_existing_seq = defaultdict(lambda: {'control' : 0, 'case' : 0})\n",
    "        for i, idv in enumerate(self.obs):\n",
    "            snp_to_cmp = tuple(idv[snp_id] for snp_id in snp_ids) #tuple of SNP that \n",
    "            if self.obs_phenotypes[i] == 0:\n",
    "                all_existing_seq[snp_to_cmp]['control'] += 1\n",
    "            else:\n",
    "                all_existing_seq[snp_to_cmp]['case'] += 1\n",
    "\n",
    "        #count reward      \n",
    "        TP = 0 #HR case\n",
    "        FP = 0 #HR control\n",
    "        TN = 0 #LR control\n",
    "        FN = 0 #LR case\n",
    "\n",
    "        for case_control_count in all_existing_seq.values():\n",
    "          # if seq is in LR group\n",
    "            if case_control_count['case'] <= case_control_count['control']: #вопрос <= или <\n",
    "                FN += case_control_count['case']\n",
    "                TN += case_control_count['control']\n",
    "            else:\n",
    "          # if seq is in HR group\n",
    "                TP += case_control_count['case']\n",
    "                FP += case_control_count['control']\n",
    "        R = (FP + TN) / (TP + FN)\n",
    "        delta = FP / (TP+0.001)\n",
    "        gamma = (TP + FP + TN + FN) / (TP+0.001)\n",
    "        CCR = 0.5 * (TP / (TP + FN) + TN / (FP + TN))\n",
    "        U = (R - delta)**2 / ((1 + delta) * (gamma - delta - 1 + 0.001))\n",
    "        koef = 1\n",
    "        if len(snp_ids) > len(self.disease_snps):\n",
    "                print(\"len(snp_ids) > len(self.disease_snps)\")\n",
    "                koef = 1 / len(snp_ids)\n",
    "\n",
    "        return koef*(CCR + U)\n",
    "\n",
    "  \n",
    "    def reset(self):\n",
    "\n",
    "        self.current_step = 0\n",
    "        \n",
    "        return self.one_hot_obs\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        chosen_snp_ids = []\n",
    "        for i, choice in enumerate(action):\n",
    "            if choice == 1:\n",
    "                chosen_snp_ids.append(i)\n",
    "        return chosen_snp_ids       \n",
    "    \n",
    "class EpsilonTracker:\n",
    "    def __init__(self, selector: ptan.actions.EpsilonGreedyActionSelector,\n",
    "                 params: SimpleNamespace):\n",
    "        self.selector = selector\n",
    "        self.params = params\n",
    "        self.frame(0)\n",
    "\n",
    "    def frame(self, frame_idx: int):\n",
    "        eps = self.params.epsilon_start - \\\n",
    "              frame_idx / self.params.epsilon_frames\n",
    "        self.selector.epsilon = max(self.params.epsilon_final, eps)\n",
    "\n",
    "\n",
    "def batch_generator(buffer: ptan.experience.ExperienceReplayBuffer,\n",
    "                    initial: int, batch_size: int):\n",
    "    buffer.populate(initial)\n",
    "    while True:\n",
    "        buffer.populate(1)\n",
    "        yield buffer.sample(batch_size)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_values_of_states(states, net, device=\"cpu\"):\n",
    "    mean_vals = []\n",
    "    for batch in np.array_split(states, 64):\n",
    "        states_v = torch.tensor(batch).to(device)\n",
    "        action_values_v = net(states_v)\n",
    "        best_action_values_v = action_values_v.max(1)[0]\n",
    "        mean_vals.append(best_action_values_v.mean().item())\n",
    "    return np.mean(mean_vals)\n",
    "\n",
    "\n",
    "def setup_ignite(engine: Engine, params: SimpleNamespace,\n",
    "                 exp_source, run_name: str, net,\n",
    "                 extra_metrics: Iterable[str] = ()):\n",
    "    # get rid of missing metrics warning\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "    handler = EndOfEpisodeHandler(\n",
    "        exp_source, bound_avg_reward=params.stop_reward)\n",
    "    handler.attach(engine)\n",
    "    EpisodeFPSHandler().attach(engine)\n",
    "\n",
    "    # @engine.on(EpisodeEvents.EPISODE_COMPLETED)\n",
    "    # def episode_completed(trainer: Engine):\n",
    "    #     passed = trainer.state.metrics.get('time_passed', 0)\n",
    "    #     print(\"Episode %d: reward=%.5f, steps=%s, \"\n",
    "    #           \"speed=%.1f f/s, elapsed=%s\" % (\n",
    "    #         trainer.state.episode, trainer.state.episode_reward,\n",
    "    #         trainer.state.episode_steps,\n",
    "    #         trainer.state.metrics.get('avg_fps', 0),\n",
    "    #         timedelta(seconds=int(passed))))\n",
    "\n",
    "    @engine.on(EpisodeEvents.BOUND_REWARD_REACHED)\n",
    "    def game_solved(trainer: Engine):\n",
    "        passed = trainer.state.metrics['time_passed']\n",
    "        print(\"Game solved in %s, after %d episodes \"\n",
    "              \"and %d iterations!\" % (\n",
    "            timedelta(seconds=int(passed)),\n",
    "            trainer.state.episode, trainer.state.iteration))\n",
    "        trainer.should_terminate = True\n",
    "\n",
    "    # now = datetime.now().isoformat(timespec='minutes').replace(':', '')\n",
    "    # wandb.tensorboard.patch(root_logdir=\"./logs/debug\")\n",
    "    # logdir = f\"runs/{now}-{params.run_name}-{run_name}\"\n",
    "    # tb = tb_logger.TensorboardLogger(log_dir=logdir)\n",
    "\n",
    "    wandb_logger = WandBLogger(\n",
    "        project=\"epistasis\",\n",
    "        entity=\"taisikus\",\n",
    "    )\n",
    "    \n",
    "    run_avg = RunningAverage(output_transform=lambda v: v['loss'])\n",
    "    run_avg.attach(engine, \"avg_loss\")\n",
    "\n",
    "    metrics = ['reward', 'steps', 'avg_reward', 'episode_number']\n",
    "    event = EpisodeEvents.EPISODE_COMPLETED\n",
    "    handler = OutputHandler(tag=\"episodes\", metric_names=metrics)\n",
    "    wandb_logger.attach(engine, log_handler=handler, event_name=event)\n",
    "    \n",
    "    # write to tensorboard every 100 iterations\n",
    "    PeriodicEvents().attach(engine)\n",
    "    metrics = ['avg_loss', 'avg_fps']\n",
    "    metrics.extend(extra_metrics)\n",
    "    handler = OutputHandler(tag=\"train\", metric_names=metrics, output_transform=lambda a: a)\n",
    "    event = PeriodEvents.ITERS_100_COMPLETED\n",
    "    wandb_logger.attach(engine, log_handler=handler, event_name=event)\n",
    "    \n",
    "    wandb_logger.watch(net)\n",
    "    \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 64, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # fx = x.float() / 2\n",
    "        fx = x.float() / 2\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "    \n",
    "class EpsilonGreedyActionSelector(ptan.actions.ActionSelector):\n",
    "    def __init__(self, epsilon=0.05, selector=None):\n",
    "        self.epsilon = epsilon\n",
    "        self.selector = selector if selector is not None else ptan.actions.ArgmaxActionSelector()\n",
    "\n",
    "    def __call__(self, scores):\n",
    "        assert isinstance(scores, np.ndarray)\n",
    "        actions = []\n",
    "        for batch in scores:\n",
    "            num_selected_snps = 2\n",
    "            snps_idx = []\n",
    "            if np.random.random() < self.epsilon:\n",
    "                snps_idx = np.random.choice(len(batch), num_selected_snps, replace=False)\n",
    "            else:    \n",
    "                for i in range(num_selected_snps):\n",
    "                    largest_score_idx = np.argmax(batch)\n",
    "                    snps_idx.append(largest_score_idx)\n",
    "                    batch[largest_score_idx] = -1\n",
    "            action = np.zeros(len(batch))        \n",
    "            for snp in snps_idx:\n",
    "                action[snp] = 1\n",
    "            actions.append(action)\n",
    "            \n",
    "        return np.array(actions)    \n",
    "        # batch_size, n_actions = scores.shape\n",
    "        # actions = self.selector(scores)\n",
    "        # mask = np.random.random(size=batch_size) < self.epsilon\n",
    "        # rand_actions = np.random.choice(n_actions, sum(mask))\n",
    "        # actions[mask] = rand_actions\n",
    "        \n",
    "\n",
    "# params = SimpleNamespace(**{\n",
    "#         'env_name': \"EpistasisEnv\",\n",
    "#         'stop_reward': 10000,\n",
    "#         'run_name': 'dqn-basic',\n",
    "#         'replay_size': 10 ** 6,\n",
    "#         'replay_initial': 25000,\n",
    "#         'target_net_sync': 10000,\n",
    "#         'epsilon_frames': 100000,\n",
    "#         'epsilon_start': 1.0,\n",
    "#         'epsilon_final': 0.1,\n",
    "#         'learning_rate': 0.00025,\n",
    "#         'gamma': 0.99,\n",
    "#         'batch_size': 32\n",
    "#     })\n",
    "\n",
    "# для prelimitary\n",
    "params = SimpleNamespace(**{\n",
    "        'env_name': \"EpistasisEnv\",\n",
    "        'stop_reward': 1.2,\n",
    "        'run_name': 'dqn-basic-5000',\n",
    "        'replay_size': 5000,\n",
    "        'replay_initial': 100,\n",
    "        'target_net_sync': 50,\n",
    "        'epsilon_frames': 1000,\n",
    "        'epsilon_start': 1.0,\n",
    "        'epsilon_final': 0.1,\n",
    "        'learning_rate': 0.001,\n",
    "        'gamma': 0.99,\n",
    "        'batch_size': 32\n",
    "    })\n",
    "NAME = \"dqn_baseline\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    temp_env = EpistasisEnv()\n",
    "    \n",
    "    fixed_observation_onehot = temp_env.reset()\n",
    "    fixed_filename = temp_env.filename\n",
    "    fixed_observation = temp_env.obs\n",
    "    fixed_obs_phenotypes = temp_env.obs_phenotypes\n",
    "    fixed_disease_snps = temp_env.disease_snps\n",
    "    fixed_sample_size = temp_env.SAMPLE_SIZE\n",
    "    fixed_n_snps = temp_env.N_SNPS\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = FixedEpistasisEnv(fixed_sample_size, fixed_n_snps, fixed_observation_onehot, fixed_filename, fixed_observation, fixed_obs_phenotypes, fixed_disease_snps)\n",
    "    net = DQN(env.observation_space.shape, env.N_SNPS)\n",
    "    net = nn.DataParallel(net)\n",
    "    net = net.to(device)\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    selector = EpsilonGreedyActionSelector(\n",
    "        epsilon=params.epsilon_start)\n",
    "    \n",
    "    epsilon_tracker = EpsilonTracker(selector, params)\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        env, agent, gamma=params.gamma)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "        exp_source, buffer_size=params.replay_size)\n",
    "    optimizer = optim.Adam(net.parameters(),\n",
    "                           lr=params.learning_rate)\n",
    "\n",
    "    def process_batch(engine, batch):\n",
    "        optimizer.zero_grad()\n",
    "        loss_v = calc_loss_dqn(\n",
    "            batch, net, tgt_net.target_model,\n",
    "            gamma=params.gamma, device=device)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        epsilon_tracker.frame(engine.state.iteration)\n",
    "        if engine.state.iteration % params.target_net_sync == 0:\n",
    "            tgt_net.sync()\n",
    "        return {\n",
    "            \"loss\": loss_v.item(),\n",
    "            \"epsilon\": selector.epsilon,\n",
    "        }\n",
    "\n",
    "    engine = Engine(process_batch)\n",
    "    setup_ignite(engine, params, exp_source, NAME, net)\n",
    "    \n",
    "    env.engine = engine\n",
    "    \n",
    "    engine.run(batch_generator(buffer, params.replay_initial,\n",
    "                                      params.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0bd08-d34e-4846-863f-45bcd9c2a335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-gen_env]",
   "language": "python",
   "name": "conda-env-.conda-gen_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
