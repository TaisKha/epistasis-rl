{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df496099-0e25-4dc9-acbf-462e967888a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from lib import common\n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENVS = 1\n",
    "\n",
    "REWARD_STEPS = 4\n",
    "CLIP_GRAD = 0.1\n",
    "\n",
    "\n",
    "class SnpA2C(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(AtariA2C, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float()\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.policy(conv_out), self.value(conv_out)\n",
    "\n",
    "\n",
    "def unpack_batch(batch, net, device='cpu'):\n",
    "    \"\"\"\n",
    "    Convert batch into training tensors\n",
    "    :param batch:\n",
    "    :param net:\n",
    "    :return: states variable, actions tensor, reference values variable\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    not_done_idx = []\n",
    "    last_states = []\n",
    "    for idx, exp in enumerate(batch):\n",
    "        states.append(np.array(exp.state, copy=False))\n",
    "        actions.append(int(exp.action))\n",
    "        rewards.append(exp.reward)\n",
    "        if exp.last_state is not None:\n",
    "            not_done_idx.append(idx)\n",
    "            last_states.append(np.array(exp.last_state, copy=False))\n",
    "\n",
    "    states_v = torch.FloatTensor(\n",
    "        np.array(states, copy=False)).to(device)\n",
    "    actions_t = torch.LongTensor(actions).to(device)\n",
    "\n",
    "    # handle rewards\n",
    "    rewards_np = np.array(rewards, dtype=np.float32)\n",
    "    if not_done_idx:\n",
    "        last_states_v = torch.FloatTensor(np.array(last_states, copy=False)).to(device)\n",
    "        last_vals_v = net(last_states_v)[1]\n",
    "        last_vals_np = last_vals_v.data.cpu().numpy()[:, 0]\n",
    "        last_vals_np *= GAMMA ** REWARD_STEPS\n",
    "        rewards_np[not_done_idx] += last_vals_np\n",
    "\n",
    "    ref_vals_v = torch.FloatTensor(rewards_np).to(device)\n",
    "\n",
    "    return states_v, actions_t, ref_vals_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef48615a-58f4-48f5-85f5-c0c5d6c786e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpistasisEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.SAMPLE_SIZE = 600 #t1 = t2 = SAMPLE_SIZE\n",
    "        self.reset()\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.N_SNPS,), dtype=np.uint8)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=\n",
    "                        (3, 2*self.SAMPLE_SIZE, self.N_SNPS), dtype=np.uint8)\n",
    "        \n",
    "        \n",
    "    def establish_phen_gen(self, file):\n",
    "        with open(file) as f:\n",
    "            data = json.load(f)\n",
    "            genotype = np.array(data[\"genotype\"])\n",
    "            self.phenotype = np.array(data[\"phenotype\"])\n",
    "            self.genotype = genotype.T\n",
    "            num_phenotypes = max(self.phenotype)+1\n",
    "            self.disease_snps = data[\"disease_snps\"]\n",
    "            self.phen_gen = [[] for _ in range(num_phenotypes)]\n",
    "            for i in range(len(self.genotype)):\n",
    "                self.phen_gen[self.phenotype[i]].append(i)  \n",
    "            return  self.genotype.shape[0], self.genotype.shape[1]\n",
    "        \n",
    "    def normalize_reward(self, current_reward):\n",
    "        maximum_env_reward = self._count_reward(self.disease_snps)\n",
    "        minimal_reward = 0.5\n",
    "        normalized_reward = (current_reward - minimal_reward) / (maximum_env_reward - minimal_reward)\n",
    "        if normalized_reward > 1:\n",
    "            print(\"normalized reward > 1: \\n normalized reward = \", normalized_reward, \"\\n current reward = \", current_reward, \"\\n maximum_env_reward = \", maximum_env_reward )\n",
    "            normalized_reward = 0.1\n",
    "        return normalized_reward\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        snp_ids = self._take_action(action)\n",
    "        reward = self._count_reward(snp_ids)\n",
    "#         без нормализации\n",
    "        # reward = self.normalize_reward(reward)\n",
    "        \n",
    "        self.current_step += 1\n",
    "        if self.current_step == EPISODE_LENGTH:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False  \n",
    "        # done = self.current_step == 1\n",
    "        obs = None if done else self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def _count_reward(self, snp_ids):\n",
    "        \n",
    "        all_existing_seq = defaultdict(lambda: {'control' : 0, 'case' : 0})\n",
    "        for i, idv in enumerate(self.obs):\n",
    "            snp_to_cmp = tuple(idv[snp_id] for snp_id in snp_ids) #tuple of SNP that \n",
    "            if self.obs_phenotypes[i] == 0:\n",
    "                all_existing_seq[snp_to_cmp]['control'] += 1\n",
    "            else:\n",
    "                all_existing_seq[snp_to_cmp]['case'] += 1\n",
    "\n",
    "        #count reward      \n",
    "        TP = 0 #HR case\n",
    "        FP = 0 #HR control\n",
    "        TN = 0 #LR control\n",
    "        FN = 0 #LR case\n",
    "\n",
    "        for case_control_count in all_existing_seq.values():\n",
    "          # if seq is in LR group\n",
    "            if case_control_count['case'] <= case_control_count['control']: #вопрос <= или <\n",
    "                FN += case_control_count['case']\n",
    "                TN += case_control_count['control']\n",
    "            else:\n",
    "          # if seq is in HR group\n",
    "                TP += case_control_count['case']\n",
    "                FP += case_control_count['control']\n",
    "        R = (FP + TN) / (TP + FN)\n",
    "        delta = FP / (TP+0.001)\n",
    "        gamma = (TP + FP + TN + FN) / (TP+0.001)\n",
    "        CCR = 0.5 * (TP / (TP + FN) + TN / (FP + TN))\n",
    "        U = (R - delta)**2 / ((1 + delta) * (gamma - delta - 1 + 0.001))\n",
    "        koef = 1\n",
    "        # if len(snp_ids) > len(self.disease_snps):\n",
    "        #         print(\"len(snp_ids) > len(self.disease_snps)\")\n",
    "        #         koef = 1 / len(snp_ids)\n",
    "\n",
    "        return koef*(CCR + U)\n",
    "\n",
    "  \n",
    "    def reset(self):\n",
    "        \n",
    "        pops = [\"ASW\", \"CEU\", \"CEU+TSI\", \"CHD\", \"GIH\", \"JPT+CHB\", \"LWK\", \"MEX\", \"MKK\", \"TSI\"]\n",
    "        sim_idx = np.random.randint(2500)\n",
    "        corp_idx = np.random.randint(1, 23)\n",
    "        pop_idx = np.random.choice(pops)\n",
    "        \n",
    "        self.filename = f\"/home/tskhakharova/epistasis-rl/epigen/sim/{sim_idx}_{corp_idx}_{pop_idx}.json\"\n",
    "        # filename = f\"/home/tskhakharova/epistasis-rl/epigen/sim/5_7_CEU.json\"\n",
    "        if not os.path.exists(self.filename):\n",
    "            os.system(f\"cd /home/tskhakharova/epistasis-rl/epigen/ && python3 simulate_data.py --sim-ids {sim_idx} --corpus-id {corp_idx} --pop {pop_idx} --inds 5000 --snps 100 --model models/ext_model.ini\")\n",
    "\n",
    "        self.N_IDV, self.N_SNPS = self.establish_phen_gen(self.filename)\n",
    "        \n",
    "        self.obs_phenotypes = None\n",
    "        one_hot_obs = self._next_observation()\n",
    "        self.current_step = 0\n",
    "        \n",
    "        return one_hot_obs\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        chosen_snp_ids = []\n",
    "        for i, choice in enumerate(action):\n",
    "            if choice == 1:\n",
    "                chosen_snp_ids.append(i)\n",
    "        return chosen_snp_ids    \n",
    "    \n",
    "    def _next_observation(self):\n",
    "        id_0 = np.random.choice(self.phen_gen[0], self.SAMPLE_SIZE)\n",
    "        id_1 = np.random.choice(self.phen_gen[1], self.SAMPLE_SIZE)\n",
    "        sample_ids = np.array(list(zip(id_0,id_1))).flatten()\n",
    "        self.obs = np.array([self.genotype[idv] for idv in sample_ids])\n",
    "        self.obs_phenotypes = [self.phenotype[idv] for idv in sample_ids]\n",
    "        \n",
    "        #one_hot\n",
    "        one_hot_obs = F.one_hot(torch.tensor(self.obs), 3)\n",
    "        one_hot_obs = one_hot_obs.movedim(2, 0)\n",
    "\n",
    "        return one_hot_obs\n",
    "    \n",
    "class FixedEpistasisEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, sample_size, n_snps, observation_onehot, filename, observation, obs_phenotypes, disease_snps):\n",
    "        self.one_hot_obs = observation_onehot\n",
    "        self.filename = filename\n",
    "        self.obs = observation\n",
    "        self.obs_phenotypes = obs_phenotypes\n",
    "        self.disease_snps = disease_snps\n",
    "        \n",
    "        self.SAMPLE_SIZE = sample_size #t1 = t2 = SAMPLE_SIZE\n",
    "        self.N_SNPS = n_snps\n",
    "        \n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.N_SNPS,), dtype=np.uint8)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=\n",
    "                        (3, 2*self.SAMPLE_SIZE, self.N_SNPS), dtype=np.uint8)\n",
    "        self.engine = None\n",
    "        \n",
    "        \n",
    "    def normalize_reward(self, current_reward):\n",
    "        maximum_env_reward = self._count_reward(self.disease_snps)\n",
    "        minimal_reward = 0.5\n",
    "        normalized_reward = (current_reward - minimal_reward) / (maximum_env_reward - minimal_reward)\n",
    "        if normalized_reward > 1:\n",
    "            print(\"normalized reward > 1: \\n normalized reward = \", normalized_reward, \"\\n current reward = \", current_reward, \"\\n maximum_env_reward = \", maximum_env_reward )\n",
    "            normalized_reward = 0.1\n",
    "        return normalized_reward\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        snp_ids = self._take_action(action)\n",
    "        reward = self._count_reward(snp_ids)\n",
    "#         без нормализации\n",
    "        # reward = self.normalize_reward(reward)\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = self.current_step == EPISODE_LENGTH\n",
    "        return self.one_hot_obs, reward, done, {}\n",
    "    \n",
    "    def _count_reward(self, snp_ids):\n",
    "        \n",
    "        if set(snp_ids) == set(self.disease_snps):\n",
    "            print(\"Disease snps are found\")\n",
    "            \n",
    "        \n",
    "        all_existing_seq = defaultdict(lambda: {'control' : 0, 'case' : 0})\n",
    "        for i, idv in enumerate(self.obs):\n",
    "            snp_to_cmp = tuple(idv[snp_id] for snp_id in snp_ids) #tuple of SNP that \n",
    "            if self.obs_phenotypes[i] == 0:\n",
    "                all_existing_seq[snp_to_cmp]['control'] += 1\n",
    "            else:\n",
    "                all_existing_seq[snp_to_cmp]['case'] += 1\n",
    "\n",
    "        #count reward      \n",
    "        TP = 0 #HR case\n",
    "        FP = 0 #HR control\n",
    "        TN = 0 #LR control\n",
    "        FN = 0 #LR case\n",
    "\n",
    "        for case_control_count in all_existing_seq.values():\n",
    "          # if seq is in LR group\n",
    "            if case_control_count['case'] <= case_control_count['control']: #вопрос <= или <\n",
    "                FN += case_control_count['case']\n",
    "                TN += case_control_count['control']\n",
    "            else:\n",
    "          # if seq is in HR group\n",
    "                TP += case_control_count['case']\n",
    "                FP += case_control_count['control']\n",
    "        R = (FP + TN) / (TP + FN)\n",
    "        delta = FP / (TP+0.001)\n",
    "        gamma = (TP + FP + TN + FN) / (TP+0.001)\n",
    "        CCR = 0.5 * (TP / (TP + FN) + TN / (FP + TN))\n",
    "        U = (R - delta)**2 / ((1 + delta) * (gamma - delta - 1 + 0.001))\n",
    "        koef = 1\n",
    "        #добавила коэффициент\n",
    "        if len(snp_ids) > len(self.disease_snps):\n",
    "                print(\"len(snp_ids) > len(self.disease_snps)\")\n",
    "                koef = 1 / len(snp_ids)\n",
    "# отнимаю 0.5\n",
    "        return koef*(CCR + U - 0.5)\n",
    "\n",
    "  \n",
    "    def reset(self):\n",
    "\n",
    "        self.current_step = 0\n",
    "        \n",
    "        return self.one_hot_obs\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        chosen_snp_ids = []\n",
    "        for i, choice in enumerate(action):\n",
    "            if choice == 1:\n",
    "                chosen_snp_ids.append(i)\n",
    "        return chosen_snp_ids  \n",
    "\n",
    "class EpiProbabilityActionSelector(ptan.actions.ActionSelector):\n",
    "    \"\"\"\n",
    "    Converts probabilities of actions into action by sampling them\n",
    "    \"\"\"\n",
    "    def __call__(self, probs):\n",
    "        assert isinstance(probs, np.ndarray)\n",
    "        assert isinstance(probs[0], np.ndarray)\n",
    "        actions = []\n",
    "#         print(\"EpiProbabilityActionSelector - probs shape:\", probs.shape)\n",
    "        for prob in probs:\n",
    "            # print(prob, end=' ')\n",
    "            num_selected_snps = 0\n",
    "            for oneprob in prob:\n",
    "                if oneprob > 1/len(prob):\n",
    "                    num_selected_snps += 1\n",
    "            wandb.log({\"num_selected_snps\":num_selected_snps}, commit=False)        \n",
    "            # print(f'{num_selected_snps=}')        \n",
    "            if num_selected_snps < 2:\n",
    "                print(\"num_selected_snps < 2\")\n",
    "                num_selected_snps = 2\n",
    "                    \n",
    "        # for prob in probs:\n",
    "        #     num_selected_snps = 2\n",
    "            # num_selected_snps = 0\n",
    "            # amount_of_oneprob_more_than_1_div_n = 0\n",
    "            # while amount_of_oneprob_more_than_1_div_n < 2:\n",
    "            #     amount_of_oneprob_more_than_1_div_n = 0\n",
    "            #     if num_selected_snps > len(prob)/10:\n",
    "            #         num_selected_snps = int(len(prob)/10)\n",
    "            #         break\n",
    "            #     num_selected_snps += 1\n",
    "            #     for oneprob in prob:\n",
    "            #         if oneprob > 1 / num_selected_snps:\n",
    "            #             amount_of_oneprob_more_than_1_div_n += 1\n",
    "            \n",
    "            chosen_snp = np.random.choice(len(prob), size=num_selected_snps, replace=False, p=prob)\n",
    "            action = np.zeros(len(prob))\n",
    "            for snp in chosen_snp:\n",
    "                action[snp] = 1\n",
    "            actions.append(action)\n",
    "        return np.array(actions)\n",
    "    \n",
    "\n",
    "class SnpPGN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(SnpPGN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 64, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float()\n",
    "#         fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc(conv_out)  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51f37d1-cad4-47bd-a661-090f29f870f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
    "    # parser.add_argument(\"-n\", \"--name\", required=True, help=\"Name of the run\")\n",
    "    # args = parser.parse_args()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    temp_env = EpistasisEnv()\n",
    "    fixed_observation_onehot = temp_env.reset()\n",
    "    fixed_filename = temp_env.filename\n",
    "    fixed_observation = temp_env.obs\n",
    "    fixed_obs_phenotypes = temp_env.obs_phenotypes\n",
    "    fixed_disease_snps = temp_env.disease_snps\n",
    "    fixed_sample_size = temp_env.SAMPLE_SIZE\n",
    "    fixed_n_snps = temp_env.N_SNPS\n",
    "    \n",
    "\n",
    "    # make_env = lambda: ptan.common.wrappers.wrap_dqn(gym.make(\"PongNoFrameskip-v4\"))\n",
    "    make_env = lambda: FixedEpistasisEnv(fixed_sample_size, fixed_n_snps, fixed_observation_onehot, fixed_filename, fixed_observation, fixed_obs_phenotypes, fixed_disease_snps)\n",
    "\n",
    "    envs = [make_env() for _ in range(NUM_ENVS)]\n",
    "    # writer = SummaryWriter(comment=\"-pong-a2c_\" + args.name)\n",
    "\n",
    "    net = AtariA2C(envs[0].observation_space.shape, envs[0].action_space.n).to(device)\n",
    "    print(net)\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(lambda x: net(x)[0], apply_softmax=True, device=device)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(envs, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    with common.RewardTracker(writer, stop_reward=1) as tracker:\n",
    "        with ptan.common.utils.TBMeanTracker(writer, batch_size=10) as tb_tracker:\n",
    "            for step_idx, exp in enumerate(exp_source):\n",
    "                batch.append(exp)\n",
    "\n",
    "                # handle new rewards\n",
    "                new_rewards = exp_source.pop_total_rewards()\n",
    "                if new_rewards:\n",
    "                    if tracker.reward(new_rewards[0], step_idx):\n",
    "                        break\n",
    "\n",
    "                if len(batch) < BATCH_SIZE:\n",
    "                    continue\n",
    "\n",
    "                states_v, actions_t, vals_ref_v = unpack_batch(batch, net, device=device)\n",
    "                batch.clear()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits_v, value_v = net(states_v)\n",
    "                loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)\n",
    "\n",
    "                log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "                adv_v = vals_ref_v - value_v.detach()\n",
    "                log_prob_actions_v = adv_v * log_prob_v[range(BATCH_SIZE), actions_t]\n",
    "                loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "                prob_v = F.softmax(logits_v, dim=1)\n",
    "                entropy_loss_v = ENTROPY_BETA * (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "\n",
    "                # calculate policy gradients only\n",
    "                loss_policy_v.backward(retain_graph=True)\n",
    "                grads = np.concatenate([p.grad.data.cpu().numpy().flatten()\n",
    "                                        for p in net.parameters()\n",
    "                                        if p.grad is not None])\n",
    "\n",
    "                # apply entropy and value gradients\n",
    "                loss_v = entropy_loss_v + loss_value_v\n",
    "                loss_v.backward()\n",
    "                nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n",
    "                optimizer.step()\n",
    "                # get full loss\n",
    "                loss_v += loss_policy_v\n",
    "\n",
    "                tb_tracker.track(\"advantage\",       adv_v, step_idx)\n",
    "                tb_tracker.track(\"values\",          value_v, step_idx)\n",
    "                tb_tracker.track(\"batch_rewards\",   vals_ref_v, step_idx)\n",
    "                tb_tracker.track(\"loss_entropy\",    entropy_loss_v, step_idx)\n",
    "                tb_tracker.track(\"loss_policy\",     loss_policy_v, step_idx)\n",
    "                tb_tracker.track(\"loss_value\",      loss_value_v, step_idx)\n",
    "                tb_tracker.track(\"loss_total\",      loss_v, step_idx)\n",
    "                tb_tracker.track(\"grad_l2\",         np.sqrt(np.mean(np.square(grads))), step_idx)\n",
    "                tb_tracker.track(\"grad_max\",        np.max(np.abs(grads)), step_idx)\n",
    "                tb_tracker.track(\"grad_var\",        np.var(grads), step_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-gen_env]",
   "language": "python",
   "name": "conda-env-.conda-gen_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
